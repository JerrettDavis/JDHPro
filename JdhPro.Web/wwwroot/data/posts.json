[
  {
    "id": "jd.efcpt.build",
    "title": "JD.Efcpt.Build",
    "date": "2025-12-21T00:00:00",
    "description": "JD.Efcpt.Build automates EF Core Power Tools scaffolding during the build, keeping database-first models in sync with schema changes without manual regeneration.",
    "featured": null,
    "tags": [
      "jd-efcpt-build",
      "dotnet",
      "ef-core",
      "database-first",
      "msbuild",
      "code-generation",
      "tooling",
      "automation"
    ],
    "categories": [
      "Programming",
      "Programming/Tooling",
      "Programming/Databases"
    ],
    "series": null,
    "seriesOrder": null,
    "content": "# \u0022Where Did Database First Go?\u0022\n\nIf you were using Entity Framework when EF Core first dropped, you probably remember the moment you went looking for database-first support and found... nothing.\n\nEF Core launched as a code-first framework. The Reverse Engineer tooling that EF6 developers relied on (the right-click, point at a database, generate your models workflow) wasn\u0027t there. Microsoft\u0027s position was essentially \u0022migrations are the future, figure it out.\u0022 And if your team had an existing database, or a DBA who actually owned the schema, or compliance requirements that meant the database was the source of truth... well, good luck with that.\n\nThe community\u0027s response was immediate and loud. \u0022Where did database first go?\u0022 became a recurring theme in GitHub issues, Stack Overflow questions, and the quiet frustration of developers who just wanted to talk to their database without hand-writing a hundred entity classes.\n\nEventually, tooling caught up. EF Core Power Tools emerged as the community answer: a Visual Studio extension that brought back the reverse engineering workflow. You could point it at a database or a DACPAC, configure some options, and generate your models. Problem solved, mostly.\n\nBut here\u0027s the thing about manual processes: they work fine right up until they don\u0027t.\n\n---\n\n## The Problem That Keeps Happening\n\nI\u0027ve spent enough time in codebases with legacy data layers to recognize a pattern. It goes something like this:\n\nA project starts with good intentions. Someone sets up EF Core Power Tools, generates the initial models, commits everything, and documents the process. \u0022When the schema changes, regenerate the models using this tool with these settings.\u0022 Clear enough.\n\nThen time passes.\n\nThe developer who set it up leaves. The documentation gets stale. Someone regenerates with slightly different settings and commits the result. Someone else forgets to regenerate entirely after a schema change. The models drift. The configuration drifts. Nobody\u0027s quite sure what the \u0022correct\u0022 regeneration process is anymore, so people just... stop doing it consistently.\n\nThis isn\u0027t a dramatic failure. It\u0027s a slow erosion. The kind of problem that doesn\u0027t announce itself until you\u0027re debugging a production issue and realize the entity class doesn\u0027t have a column that\u0027s been in the database for six months.\n\nIf you\u0027ve worked in a codebase long enough, you\u0027ve probably seen some version of this. Maybe you\u0027ve been the person who discovered the drift. Maybe you\u0027ve been the person who caused it. (No judgment. We\u0027ve all been there.)\n\nThe frustrating part is that the fix is always the same: regenerate the models, commit the changes, remind everyone to regenerate after schema changes. And then six months later, you\u0027re having the same conversation again.\n\n---\n\n## Why Manual Regeneration Fails\n\nLet\u0027s be specific about what goes wrong, because understanding the failure modes is the first step toward fixing them.\n\n**The ownership problem.** Whose job is it to regenerate after a schema change? The person who changed the schema? The person who owns the data layer? The tech lead? Nobody has a clear answer, which means sometimes everyone does it (chaos) and sometimes nobody does it (drift).\n\n**The configuration problem.** EF Core Power Tools stores settings in JSON files. Namespaces, nullable reference types, navigation property generation, renaming rules. There are dozens of options. If developers regenerate with different configurations, you get inconsistent output. Same database, different generated code.\n\n**The tooling problem.** Regeneration requires Visual Studio with the extension installed. CI servers don\u0027t have Visual Studio. New developers might not have the extension. Remote development setups might not support it. The process that works on one machine doesn\u0027t necessarily work on another.\n\n**The noise problem.** Regeneration often produces massive diffs. Property reordering, whitespace changes, attribute additions. Stuff that doesn\u0027t represent actual schema changes but clutters up the commit. Developers learn to distrust regeneration diffs, which makes them reluctant to regenerate, which makes the problem worse.\n\n**The timing problem.** Even when everyone knows the process, there\u0027s no enforcement. You can commit code that references a column the models don\u0027t have, and the build might still pass if nothing actually uses that code path yet. The error surfaces later, in a different context, when the connection to the original schema change is long forgotten.\n\nNone of these are individually catastrophic. Together, they add up to a process that works in theory but fails in practice.\n\n---\n\n## The Idea\n\nHere\u0027s the thought that eventually became this project: if model generation can be invoked from the command line (and it can, via EF Core Power Tools CLI), then model generation can be part of the build.\n\nNot a separate step you remember to run. Not a manual process with unclear ownership. Just part of what happens when you run \u0060dotnet build\u0060.\n\nThe build already knows how to compile your code. It already knows how to restore packages, run analyzers, produce artifacts. Adding \u0022generate EF Core models from the schema\u0022 to that list isn\u0027t conceptually different from any other build-time code generation.\n\nIf the build handles it, the ownership question disappears. The build owns it. If the build handles it with consistent configuration, the drift disappears. Everyone gets the same output. If the build handles it on every machine, the tooling problem disappears. No special extensions required.\n\nThis is JD.Efcpt.Build: an MSBuild integration that makes EF Core model generation automatic.\n\n---\n\n## How It Actually Works\n\nThe package hooks into your build through MSBuild targets that run before compilation. When you build, it:\n\n1. **Finds your schema source.** Either a SQL Server Database Project (\u0060.sqlproj\u0060) that gets compiled to a DACPAC, or a connection string pointing to a live database.\n\n2. **Computes a fingerprint.** A hash of all the inputs: the DACPAC or schema metadata, the configuration file, the renaming rules, any custom templates. This fingerprint represents \u0022the current state of everything that affects generation.\u0022\n\n3. **Compares to the previous fingerprint.** If they match, nothing changed, and generation is skipped. If they differ, something changed, and generation runs.\n\n4. **Generates models.** Using EF Core Power Tools CLI, same as you\u0027d run manually, but automated. Output goes to \u0060obj/efcpt/Generated/\u0060 with a \u0060.g.cs\u0060 extension.\n\n5. **Adds generated files to compilation.** Automatically. You don\u0027t edit your project file or manage includes.\n\nThe fingerprinting is what makes this practical. You don\u0027t want generation running on every build. That would be slow and developers would hate it. The fingerprint check is fast (XxHash64, designed for exactly this kind of content comparison), so incremental builds have essentially zero overhead. Generation only runs when inputs actually change.\n\n---\n\n## Two Ways to Get Your Schema\n\nDifferent teams manage database schemas differently, so the package supports two modes.\n\n**DACPAC Mode** is for teams with SQL Server Database Projects. You have a \u0060.sqlproj\u0060 that defines your schema in version-controlled SQL files. The package builds this project to produce a DACPAC, then generates models from that DACPAC.\n\n\u0060\u0060\u0060xml\n\u003CPropertyGroup\u003E\n  \u003CEfcptSqlProj\u003E..\\Database\\MyDatabase.sqlproj\u003C/EfcptSqlProj\u003E\n\u003C/PropertyGroup\u003E\n\u0060\u0060\u0060\n\nThis is nice because your schema is code. It lives in source control. Changes go through pull requests. The DACPAC is a build artifact, and models are derived from that artifact deterministically.\n\n**Connection String Mode** is for teams without database projects. Maybe you apply migrations to a dev database and want to scaffold from that. Maybe you\u0027re working against a cloud database. Maybe you just don\u0027t want to deal with DACPACs.\n\n\u0060\u0060\u0060xml\n\u003CPropertyGroup\u003E\n  \u003CEfcptConnectionString\u003E$(DB_CONNECTION_STRING)\u003C/EfcptConnectionString\u003E\n\u003C/PropertyGroup\u003E\n\u0060\u0060\u0060\n\nThe package connects, queries system tables to understand the schema, and generates from that. The fingerprint is computed from the schema metadata, so incremental builds still work. If the schema hasn\u0027t changed, generation is skipped.\n\nBoth modes use the same configuration files and produce the same kind of output. They just differ in where the schema comes from.\n\n---\n\n## Setting It Up\n\nThe minimum setup is almost trivial:\n\n\u0060\u0060\u0060xml\n\u003CItemGroup\u003E\n  \u003CPackageReference Include=\u0022JD.Efcpt.Build\u0022 Version=\u00221.0.0\u0022 /\u003E\n\u003C/ItemGroup\u003E\n\u0060\u0060\u0060\n\nIf you have a \u0060.sqlproj\u0060 in your solution and an \u0060efcpt-config.json\u0060 in your project directory, that\u0027s it. Run \u0060dotnet build\u0060 and models appear.\n\nFor more control, you add configuration. The \u0060efcpt-config.json\u0060 controls generation behavior:\n\n\u0060\u0060\u0060json\n{\n  \u0022names\u0022: {\n    \u0022root-namespace\u0022: \u0022MyApp.Data\u0022,\n    \u0022dbcontext-name\u0022: \u0022ApplicationDbContext\u0022\n  },\n  \u0022code-generation\u0022: {\n    \u0022use-nullable-reference-types\u0022: true,\n    \u0022enable-on-configuring\u0022: false\n  }\n}\n\u0060\u0060\u0060\n\nThe \u0060enable-on-configuring: false\u0060 means your DbContext won\u0027t have a hardcoded connection string. You configure that in your DI container, where it belongs.\n\nIf your database uses naming conventions that don\u0027t map cleanly to C#, you add renaming rules:\n\n\u0060\u0060\u0060json\n[\n  {\n    \u0022SchemaName\u0022: \u0022dbo\u0022,\n    \u0022Tables\u0022: [\n      {\n        \u0022Name\u0022: \u0022tbl_Users\u0022,\n        \u0022NewName\u0022: \u0022User\u0022,\n        \u0022Columns\u0022: [\n          { \u0022Name\u0022: \u0022user_id\u0022, \u0022NewName\u0022: \u0022Id\u0022 }\n        ]\n      }\n    ]\n  }\n]\n\u0060\u0060\u0060\n\nNow \u0060tbl_Users.user_id\u0060 becomes \u0060User.Id\u0060. The database can keep its conventions, your C# code can have its conventions, and the mapping is explicit and version-controlled.\n\n---\n\n## What About Custom Code?\n\nA reasonable concern: \u0022I have computed properties and validation methods on my entities. Won\u0027t regeneration overwrite those?\u0022\n\nThis is what partial classes are for.\n\nThe generated entity is one half:\n\n\u0060\u0060\u0060csharp\n// obj/efcpt/Generated/User.g.cs\npublic partial class User\n{\n    public int Id { get; set; }\n    public string Email { get; set; }\n    public string FirstName { get; set; }\n    public string LastName { get; set; }\n}\n\u0060\u0060\u0060\n\nYour custom logic is the other half:\n\n\u0060\u0060\u0060csharp\n// Models/User.cs\npublic partial class User\n{\n    public string FullName =\u003E $\u0022{FirstName} {LastName}\u0022;\n\n    public bool HasValidEmail =\u003E Email?.Contains(\u0022@\u0022) ?? false;\n}\n\u0060\u0060\u0060\n\nBoth compile into a single class. The generated half gets regenerated every build. Your custom half stays exactly as you wrote it.\n\nThis separation is actually cleaner than mixing generated and custom code in the same file. You know at a glance what\u0027s generated (\u0060.g.cs\u0060 in \u0060obj/\u0060) and what\u0027s yours (everything else).\n\n---\n\n## CI/CD Without Special Steps\n\nOne of the pain points I mentioned earlier was CI/CD. Manual regeneration doesn\u0027t work in automated pipelines. You\u0027re stuck either committing generated code (merge conflicts) or maintaining custom regeneration scripts (fragile).\n\nWith the build handling generation, CI just works:\n\n\u0060\u0060\u0060yaml\nsteps:\n  - uses: actions/checkout@v4\n\n  - name: Setup .NET\n    uses: actions/setup-dotnet@v4\n    with:\n      dotnet-version: \u002710.0.x\u0027\n\n  - name: Build\n    run: dotnet build --configuration Release\n    env:\n      DB_CONNECTION_STRING: ${{ secrets.DB_CONNECTION_STRING }}\n\u0060\u0060\u0060\n\nNo special steps for EF Core generation. The build handles it. On .NET 10\u002B, the package uses \u0060dotnet dnx\u0060 to execute the tool directly from the package feed without requiring installation. On older versions, it uses tool manifests or global tools.\n\nPull requests that include schema changes automatically include the corresponding model changes, because both happen during the build. Schema and code are validated together.\n\n---\n\n## When Things Go Wrong\n\nThings will go wrong. Here\u0027s how you figure out what happened.\n\n**Enable verbose logging:**\n\n\u0060\u0060\u0060xml\n\u003CPropertyGroup\u003E\n  \u003CEfcptLogVerbosity\u003Edetailed\u003C/EfcptLogVerbosity\u003E\n\u003C/PropertyGroup\u003E\n\u0060\u0060\u0060\n\nBuild output now includes exactly what\u0027s happening: which inputs were found, what fingerprint was computed, whether generation ran or was skipped.\n\n**Check the resolved inputs:**\n\nAfter a build, look at \u0060obj/efcpt/resolved-inputs.json\u0060. This shows exactly what the package found for each input. If something\u0027s wrong, you\u0027ll see it here.\n\n**Inspect the fingerprint:**\n\nThe fingerprint is stored at \u0060obj/efcpt/fingerprint.txt\u0060. If generation is running unexpectedly (or not running when it should), the fingerprint tells you whether inputs changed from the package\u0027s perspective.\n\n---\n\n## Who This Is For\n\nI want to be honest about fit.\n\n**This is probably for you if:**\n\nYou\u0027re doing database-first development and you\u0027ve experienced the regeneration coordination problem. The \u0022did someone regenerate?\u0022 question has come up, and the answer wasn\u0027t always clear.\n\nYour schema changes regularly. If you\u0027re shipping schema changes weekly, manual regeneration becomes friction.\n\nYou want builds that work identically everywhere. Local machines, CI servers, new developer laptops. Everyone should get the same generated code from the same inputs.\n\n**This probably isn\u0027t for you if:**\n\nYour schema is essentially static. If schema changes are rare, manual regeneration isn\u0027t that painful.\n\nYou\u0027re using code-first migrations. If migrations are your source of truth, you\u0027re solving a different problem.\n\nYou\u0027re not using EF Core Power Tools already. This package automates EF Core Power Tools; if you\u0027re using a different generation approach, this doesn\u0027t apply.\n\n---\n\n## The Groan, Addressed\n\n\u0022Where did database first go?\u0022\n\nIt\u0027s been years since EF Core launched without reverse engineering, and the tooling has caught up. EF Core Power Tools exists. The CLI exists. The capability is there.\n\nBut capability isn\u0027t the same as workflow. Having the tools isn\u0027t the same as having a process that works reliably across a team, across time, across environments.\n\nJD.Efcpt.Build is an attempt to close that gap. To take the capability that exists and make it automatic. To make the build the owner of model generation, so humans don\u0027t have to remember to do it.\n\nYour database schema is the source of truth. This package just makes sure your code reflects that truth, every time you build, without manual intervention.\n\nOne less thing to coordinate. One less thing to forget. One less thing to go wrong in production because a manual step got skipped.\n\nThat\u0027s the pitch. Give it a try if it fits your situation.\n\n---\n\n*JD.Efcpt.Build is [open source](https://github.com/JerrettDavis/JD.Efcpt.Build) and available on [NuGet](https://www.nuget.org/packages/JD.Efcpt.Build).*",
    "contentHtml": "\u003Ch1 id=\u0022where-did-database-first-go\u0022\u003E\u0026quot;Where Did Database First Go?\u0026quot;\u003C/h1\u003E\n\u003Cp\u003EIf you were using Entity Framework when EF Core first dropped, you probably remember the moment you went looking for database-first support and found... nothing.\u003C/p\u003E\n\u003Cp\u003EEF Core launched as a code-first framework. The Reverse Engineer tooling that EF6 developers relied on (the right-click, point at a database, generate your models workflow) wasn\u0027t there. Microsoft\u0027s position was essentially \u0026quot;migrations are the future, figure it out.\u0026quot; And if your team had an existing database, or a DBA who actually owned the schema, or compliance requirements that meant the database was the source of truth... well, good luck with that.\u003C/p\u003E\n\u003Cp\u003EThe community\u0027s response was immediate and loud. \u0026quot;Where did database first go?\u0026quot; became a recurring theme in GitHub issues, Stack Overflow questions, and the quiet frustration of developers who just wanted to talk to their database without hand-writing a hundred entity classes.\u003C/p\u003E\n\u003Cp\u003EEventually, tooling caught up. EF Core Power Tools emerged as the community answer: a Visual Studio extension that brought back the reverse engineering workflow. You could point it at a database or a DACPAC, configure some options, and generate your models. Problem solved, mostly.\u003C/p\u003E\n\u003Cp\u003EBut here\u0027s the thing about manual processes: they work fine right up until they don\u0027t.\u003C/p\u003E\n\u003Chr /\u003E\n\u003Ch2 id=\u0022the-problem-that-keeps-happening\u0022\u003EThe Problem That Keeps Happening\u003C/h2\u003E\n\u003Cp\u003EI\u0027ve spent enough time in codebases with legacy data layers to recognize a pattern. It goes something like this:\u003C/p\u003E\n\u003Cp\u003EA project starts with good intentions. Someone sets up EF Core Power Tools, generates the initial models, commits everything, and documents the process. \u0026quot;When the schema changes, regenerate the models using this tool with these settings.\u0026quot; Clear enough.\u003C/p\u003E\n\u003Cp\u003EThen time passes.\u003C/p\u003E\n\u003Cp\u003EThe developer who set it up leaves. The documentation gets stale. Someone regenerates with slightly different settings and commits the result. Someone else forgets to regenerate entirely after a schema change. The models drift. The configuration drifts. Nobody\u0027s quite sure what the \u0026quot;correct\u0026quot; regeneration process is anymore, so people just... stop doing it consistently.\u003C/p\u003E\n\u003Cp\u003EThis isn\u0027t a dramatic failure. It\u0027s a slow erosion. The kind of problem that doesn\u0027t announce itself until you\u0027re debugging a production issue and realize the entity class doesn\u0027t have a column that\u0027s been in the database for six months.\u003C/p\u003E\n\u003Cp\u003EIf you\u0027ve worked in a codebase long enough, you\u0027ve probably seen some version of this. Maybe you\u0027ve been the person who discovered the drift. Maybe you\u0027ve been the person who caused it. (No judgment. We\u0027ve all been there.)\u003C/p\u003E\n\u003Cp\u003EThe frustrating part is that the fix is always the same: regenerate the models, commit the changes, remind everyone to regenerate after schema changes. And then six months later, you\u0027re having the same conversation again.\u003C/p\u003E\n\u003Chr /\u003E\n\u003Ch2 id=\u0022why-manual-regeneration-fails\u0022\u003EWhy Manual Regeneration Fails\u003C/h2\u003E\n\u003Cp\u003ELet\u0027s be specific about what goes wrong, because understanding the failure modes is the first step toward fixing them.\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003EThe ownership problem.\u003C/strong\u003E Whose job is it to regenerate after a schema change? The person who changed the schema? The person who owns the data layer? The tech lead? Nobody has a clear answer, which means sometimes everyone does it (chaos) and sometimes nobody does it (drift).\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003EThe configuration problem.\u003C/strong\u003E EF Core Power Tools stores settings in JSON files. Namespaces, nullable reference types, navigation property generation, renaming rules. There are dozens of options. If developers regenerate with different configurations, you get inconsistent output. Same database, different generated code.\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003EThe tooling problem.\u003C/strong\u003E Regeneration requires Visual Studio with the extension installed. CI servers don\u0027t have Visual Studio. New developers might not have the extension. Remote development setups might not support it. The process that works on one machine doesn\u0027t necessarily work on another.\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003EThe noise problem.\u003C/strong\u003E Regeneration often produces massive diffs. Property reordering, whitespace changes, attribute additions. Stuff that doesn\u0027t represent actual schema changes but clutters up the commit. Developers learn to distrust regeneration diffs, which makes them reluctant to regenerate, which makes the problem worse.\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003EThe timing problem.\u003C/strong\u003E Even when everyone knows the process, there\u0027s no enforcement. You can commit code that references a column the models don\u0027t have, and the build might still pass if nothing actually uses that code path yet. The error surfaces later, in a different context, when the connection to the original schema change is long forgotten.\u003C/p\u003E\n\u003Cp\u003ENone of these are individually catastrophic. Together, they add up to a process that works in theory but fails in practice.\u003C/p\u003E\n\u003Chr /\u003E\n\u003Ch2 id=\u0022the-idea\u0022\u003EThe Idea\u003C/h2\u003E\n\u003Cp\u003EHere\u0027s the thought that eventually became this project: if model generation can be invoked from the command line (and it can, via EF Core Power Tools CLI), then model generation can be part of the build.\u003C/p\u003E\n\u003Cp\u003ENot a separate step you remember to run. Not a manual process with unclear ownership. Just part of what happens when you run \u003Ccode\u003Edotnet build\u003C/code\u003E.\u003C/p\u003E\n\u003Cp\u003EThe build already knows how to compile your code. It already knows how to restore packages, run analyzers, produce artifacts. Adding \u0026quot;generate EF Core models from the schema\u0026quot; to that list isn\u0027t conceptually different from any other build-time code generation.\u003C/p\u003E\n\u003Cp\u003EIf the build handles it, the ownership question disappears. The build owns it. If the build handles it with consistent configuration, the drift disappears. Everyone gets the same output. If the build handles it on every machine, the tooling problem disappears. No special extensions required.\u003C/p\u003E\n\u003Cp\u003EThis is JD.Efcpt.Build: an MSBuild integration that makes EF Core model generation automatic.\u003C/p\u003E\n\u003Chr /\u003E\n\u003Ch2 id=\u0022how-it-actually-works\u0022\u003EHow It Actually Works\u003C/h2\u003E\n\u003Cp\u003EThe package hooks into your build through MSBuild targets that run before compilation. When you build, it:\u003C/p\u003E\n\u003Col\u003E\n\u003Cli\u003E\u003Cp\u003E\u003Cstrong\u003EFinds your schema source.\u003C/strong\u003E Either a SQL Server Database Project (\u003Ccode\u003E.sqlproj\u003C/code\u003E) that gets compiled to a DACPAC, or a connection string pointing to a live database.\u003C/p\u003E\n\u003C/li\u003E\n\u003Cli\u003E\u003Cp\u003E\u003Cstrong\u003EComputes a fingerprint.\u003C/strong\u003E A hash of all the inputs: the DACPAC or schema metadata, the configuration file, the renaming rules, any custom templates. This fingerprint represents \u0026quot;the current state of everything that affects generation.\u0026quot;\u003C/p\u003E\n\u003C/li\u003E\n\u003Cli\u003E\u003Cp\u003E\u003Cstrong\u003ECompares to the previous fingerprint.\u003C/strong\u003E If they match, nothing changed, and generation is skipped. If they differ, something changed, and generation runs.\u003C/p\u003E\n\u003C/li\u003E\n\u003Cli\u003E\u003Cp\u003E\u003Cstrong\u003EGenerates models.\u003C/strong\u003E Using EF Core Power Tools CLI, same as you\u0027d run manually, but automated. Output goes to \u003Ccode\u003Eobj/efcpt/Generated/\u003C/code\u003E with a \u003Ccode\u003E.g.cs\u003C/code\u003E extension.\u003C/p\u003E\n\u003C/li\u003E\n\u003Cli\u003E\u003Cp\u003E\u003Cstrong\u003EAdds generated files to compilation.\u003C/strong\u003E Automatically. You don\u0027t edit your project file or manage includes.\u003C/p\u003E\n\u003C/li\u003E\n\u003C/ol\u003E\n\u003Cp\u003EThe fingerprinting is what makes this practical. You don\u0027t want generation running on every build. That would be slow and developers would hate it. The fingerprint check is fast (XxHash64, designed for exactly this kind of content comparison), so incremental builds have essentially zero overhead. Generation only runs when inputs actually change.\u003C/p\u003E\n\u003Chr /\u003E\n\u003Ch2 id=\u0022two-ways-to-get-your-schema\u0022\u003ETwo Ways to Get Your Schema\u003C/h2\u003E\n\u003Cp\u003EDifferent teams manage database schemas differently, so the package supports two modes.\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003EDACPAC Mode\u003C/strong\u003E is for teams with SQL Server Database Projects. You have a \u003Ccode\u003E.sqlproj\u003C/code\u003E that defines your schema in version-controlled SQL files. The package builds this project to produce a DACPAC, then generates models from that DACPAC.\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-xml\u0022\u003E\n\u0026lt;/PropertyGroup\u0026gt;\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EThis is nice because your schema is code. It lives in source control. Changes go through pull requests. The DACPAC is a build artifact, and models are derived from that artifact deterministically.\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003EConnection String Mode\u003C/strong\u003E is for teams without database projects. Maybe you apply migrations to a dev database and want to scaffold from that. Maybe you\u0027re working against a cloud database. Maybe you just don\u0027t want to deal with DACPACs.\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-xml\u0022\u003E\n\u0026lt;/PropertyGroup\u0026gt;\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EThe package connects, queries system tables to understand the schema, and generates from that. The fingerprint is computed from the schema metadata, so incremental builds still work. If the schema hasn\u0027t changed, generation is skipped.\u003C/p\u003E\n\u003Cp\u003EBoth modes use the same configuration files and produce the same kind of output. They just differ in where the schema comes from.\u003C/p\u003E\n\u003Chr /\u003E\n\u003Ch2 id=\u0022setting-it-up\u0022\u003ESetting It Up\u003C/h2\u003E\n\u003Cp\u003EThe minimum setup is almost trivial:\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-xml\u0022\u003E\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EIf you have a \u003Ccode\u003E.sqlproj\u003C/code\u003E in your solution and an \u003Ccode\u003Eefcpt-config.json\u003C/code\u003E in your project directory, that\u0027s it. Run \u003Ccode\u003Edotnet build\u003C/code\u003E and models appear.\u003C/p\u003E\n\u003Cp\u003EFor more control, you add configuration. The \u003Ccode\u003Eefcpt-config.json\u003C/code\u003E controls generation behavior:\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-json\u0022\u003E{\n  \u0026quot;names\u0026quot;: {\n    \u0026quot;root-namespace\u0026quot;: \u0026quot;MyApp.Data\u0026quot;,\n    \u0026quot;dbcontext-name\u0026quot;: \u0026quot;ApplicationDbContext\u0026quot;\n  },\n  \u0026quot;code-generation\u0026quot;: {\n    \u0026quot;use-nullable-reference-types\u0026quot;: true,\n    \u0026quot;enable-on-configuring\u0026quot;: false\n  }\n}\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EThe \u003Ccode\u003Eenable-on-configuring: false\u003C/code\u003E means your DbContext won\u0027t have a hardcoded connection string. You configure that in your DI container, where it belongs.\u003C/p\u003E\n\u003Cp\u003EIf your database uses naming conventions that don\u0027t map cleanly to C#, you add renaming rules:\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-json\u0022\u003E[\n  {\n    \u0026quot;SchemaName\u0026quot;: \u0026quot;dbo\u0026quot;,\n    \u0026quot;Tables\u0026quot;: [\n      {\n        \u0026quot;Name\u0026quot;: \u0026quot;tbl_Users\u0026quot;,\n        \u0026quot;NewName\u0026quot;: \u0026quot;User\u0026quot;,\n        \u0026quot;Columns\u0026quot;: [\n          { \u0026quot;Name\u0026quot;: \u0026quot;user_id\u0026quot;, \u0026quot;NewName\u0026quot;: \u0026quot;Id\u0026quot; }\n        ]\n      }\n    ]\n  }\n]\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003ENow \u003Ccode\u003Etbl_Users.user_id\u003C/code\u003E becomes \u003Ccode\u003EUser.Id\u003C/code\u003E. The database can keep its conventions, your C# code can have its conventions, and the mapping is explicit and version-controlled.\u003C/p\u003E\n\u003Chr /\u003E\n\u003Ch2 id=\u0022what-about-custom-code\u0022\u003EWhat About Custom Code?\u003C/h2\u003E\n\u003Cp\u003EA reasonable concern: \u0026quot;I have computed properties and validation methods on my entities. Won\u0027t regeneration overwrite those?\u0026quot;\u003C/p\u003E\n\u003Cp\u003EThis is what partial classes are for.\u003C/p\u003E\n\u003Cp\u003EThe generated entity is one half:\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-csharp\u0022\u003E// obj/efcpt/Generated/User.g.cs\npublic partial class User\n{\n    public int Id { get; set; }\n    public string Email { get; set; }\n    public string FirstName { get; set; }\n    public string LastName { get; set; }\n}\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EYour custom logic is the other half:\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-csharp\u0022\u003E// Models/User.cs\npublic partial class User\n{\n    public string FullName =\u0026gt; $\u0026quot;{FirstName} {LastName}\u0026quot;;\n\n    public bool HasValidEmail =\u0026gt; Email?.Contains(\u0026quot;@\u0026quot;) ?? false;\n}\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EBoth compile into a single class. The generated half gets regenerated every build. Your custom half stays exactly as you wrote it.\u003C/p\u003E\n\u003Cp\u003EThis separation is actually cleaner than mixing generated and custom code in the same file. You know at a glance what\u0027s generated (\u003Ccode\u003E.g.cs\u003C/code\u003E in \u003Ccode\u003Eobj/\u003C/code\u003E) and what\u0027s yours (everything else).\u003C/p\u003E\n\u003Chr /\u003E\n\u003Ch2 id=\u0022cicd-without-special-steps\u0022\u003ECI/CD Without Special Steps\u003C/h2\u003E\n\u003Cp\u003EOne of the pain points I mentioned earlier was CI/CD. Manual regeneration doesn\u0027t work in automated pipelines. You\u0027re stuck either committing generated code (merge conflicts) or maintaining custom regeneration scripts (fragile).\u003C/p\u003E\n\u003Cp\u003EWith the build handling generation, CI just works:\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-yaml\u0022\u003Esteps:\n  - uses: actions/checkout@v4\n\n  - name: Setup .NET\n    uses: actions/setup-dotnet@v4\n    with:\n      dotnet-version: \u002710.0.x\u0027\n\n  - name: Build\n    run: dotnet build --configuration Release\n    env:\n      DB_CONNECTION_STRING: ${{ secrets.DB_CONNECTION_STRING }}\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003ENo special steps for EF Core generation. The build handles it. On .NET 10\u002B, the package uses \u003Ccode\u003Edotnet dnx\u003C/code\u003E to execute the tool directly from the package feed without requiring installation. On older versions, it uses tool manifests or global tools.\u003C/p\u003E\n\u003Cp\u003EPull requests that include schema changes automatically include the corresponding model changes, because both happen during the build. Schema and code are validated together.\u003C/p\u003E\n\u003Chr /\u003E\n\u003Ch2 id=\u0022when-things-go-wrong\u0022\u003EWhen Things Go Wrong\u003C/h2\u003E\n\u003Cp\u003EThings will go wrong. Here\u0027s how you figure out what happened.\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003EEnable verbose logging:\u003C/strong\u003E\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-xml\u0022\u003E\n\u0026lt;/PropertyGroup\u0026gt;\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EBuild output now includes exactly what\u0027s happening: which inputs were found, what fingerprint was computed, whether generation ran or was skipped.\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003ECheck the resolved inputs:\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003EAfter a build, look at \u003Ccode\u003Eobj/efcpt/resolved-inputs.json\u003C/code\u003E. This shows exactly what the package found for each input. If something\u0027s wrong, you\u0027ll see it here.\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003EInspect the fingerprint:\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003EThe fingerprint is stored at \u003Ccode\u003Eobj/efcpt/fingerprint.txt\u003C/code\u003E. If generation is running unexpectedly (or not running when it should), the fingerprint tells you whether inputs changed from the package\u0027s perspective.\u003C/p\u003E\n\u003Chr /\u003E\n\u003Ch2 id=\u0022who-this-is-for\u0022\u003EWho This Is For\u003C/h2\u003E\n\u003Cp\u003EI want to be honest about fit.\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003EThis is probably for you if:\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003EYou\u0027re doing database-first development and you\u0027ve experienced the regeneration coordination problem. The \u0026quot;did someone regenerate?\u0026quot; question has come up, and the answer wasn\u0027t always clear.\u003C/p\u003E\n\u003Cp\u003EYour schema changes regularly. If you\u0027re shipping schema changes weekly, manual regeneration becomes friction.\u003C/p\u003E\n\u003Cp\u003EYou want builds that work identically everywhere. Local machines, CI servers, new developer laptops. Everyone should get the same generated code from the same inputs.\u003C/p\u003E\n\u003Cp\u003E\u003Cstrong\u003EThis probably isn\u0027t for you if:\u003C/strong\u003E\u003C/p\u003E\n\u003Cp\u003EYour schema is essentially static. If schema changes are rare, manual regeneration isn\u0027t that painful.\u003C/p\u003E\n\u003Cp\u003EYou\u0027re using code-first migrations. If migrations are your source of truth, you\u0027re solving a different problem.\u003C/p\u003E\n\u003Cp\u003EYou\u0027re not using EF Core Power Tools already. This package automates EF Core Power Tools; if you\u0027re using a different generation approach, this doesn\u0027t apply.\u003C/p\u003E\n\u003Chr /\u003E\n\u003Ch2 id=\u0022the-groan-addressed\u0022\u003EThe Groan, Addressed\u003C/h2\u003E\n\u003Cp\u003E\u0026quot;Where did database first go?\u0026quot;\u003C/p\u003E\n\u003Cp\u003EIt\u0027s been years since EF Core launched without reverse engineering, and the tooling has caught up. EF Core Power Tools exists. The CLI exists. The capability is there.\u003C/p\u003E\n\u003Cp\u003EBut capability isn\u0027t the same as workflow. Having the tools isn\u0027t the same as having a process that works reliably across a team, across time, across environments.\u003C/p\u003E\n\u003Cp\u003EJD.Efcpt.Build is an attempt to close that gap. To take the capability that exists and make it automatic. To make the build the owner of model generation, so humans don\u0027t have to remember to do it.\u003C/p\u003E\n\u003Cp\u003EYour database schema is the source of truth. This package just makes sure your code reflects that truth, every time you build, without manual intervention.\u003C/p\u003E\n\u003Cp\u003EOne less thing to coordinate. One less thing to forget. One less thing to go wrong in production because a manual step got skipped.\u003C/p\u003E\n\u003Cp\u003EThat\u0027s the pitch. Give it a try if it fits your situation.\u003C/p\u003E\n\u003Chr /\u003E\n\u003Cp\u003E\u003Cem\u003EJD.Efcpt.Build is \u003Ca href=\u0022https://github.com/JerrettDavis/JD.Efcpt.Build\u0022\u003Eopen source\u003C/a\u003E and available on \u003Ca href=\u0022https://www.nuget.org/packages/JD.Efcpt.Build\u0022\u003ENuGet\u003C/a\u003E.\u003C/em\u003E\u003C/p\u003E\n",
    "stub": "JD.Efcpt.Build automates EF Core Power Tools scaffolding during the build, keeping database-first models in sync with schema changes without manual regeneration.",
    "wordCount": 1953,
    "useToc": true,
    "source": "syndicated",
    "canonicalUrl": "https://jerrettdavis.com/blog/jd.efcpt.build"
  },
  {
    "id": "you-dont-hate-abstractions",
    "title": "You Don\u0027t Hate Abstractions",
    "date": "2025-12-05T00:00:00",
    "description": "Why developers misuse abstractions, how to design them from behavior and domain first, and how patterns, automation, and declarative modeling lead to better systems.",
    "featured": "/images/posts/you-don\u0027t-hate-abstractions/featured.png",
    "tags": [
      "software-architecture",
      "software-design",
      "abstractions",
      "design-patterns",
      "domain-driven-design",
      "behavior-driven-development",
      "clean-architecture",
      "patternkit",
      "developer-practices",
      "automation"
    ],
    "categories": [
      "Programming",
      "Software Engineering",
      "Architecture"
    ],
    "series": null,
    "seriesOrder": null,
    "content": "It\u2019s an hour until you\u2019re free for the weekend, and you\u2019re trying to knock out one\nlast ticket before you escape into whatever assuredly action-packed plans await you.\nYou spot a seemingly harmless task: \u0022Add Middle Initial to User Name Display.\u0022\n\nYou chuckle. Easy. A palate cleanser. A victory lap.  \nYou assign the ticket, flip it from \u0060New\u0060 to \u0060Active\u0060, and let your IDE warm up  \nwhile you drift into a pleasant daydream about not being here.\n\nBut then the search results begin to appear.  \nSlowly. Line by line.\n\nAnd your reverie begins to rot.\n\n\u003E \u0060IUserNameStrategy\u0060, \u0060UserNameContext\u0060, \u0060UserNameDisplayStrategyFactory\u0060,  \n\u003E \u0060StandardUserNameDisplayStrategy\u0060, \u0060FormalUserNameDisplayStrategy\u0060,  \n\u003E \u0060InformalUserNameDisplayStrategy\u0060, \u0060UserNameDisplayModule\u0060, \u2026\n\nIncredulous, your chuckle hardens into a throat-scraping noise somewhere\nbetween a laugh and a cry.\n\n\u0022What in the Gang-of-Fuck is happening here,\u0022 you think, feeling your pulse tick up.\n\u0022Either someone read [Refactoring.Guru](https://refactoring.guru) like it was scripture and decided to\nbaptize the codebase in every pattern they learned, or a grizzled enterprise\nveteran escaped some Netflix-adjacent monolith and is trying to upskill in\nTypeScript. Because surely no sane developer would build this much redirection\nfor such a trivial feature\u2026 right?\u0022\n\nThat tiny, spiraling task is a perfect microcosm of a continuous debate across engineering circles: when does abstraction help, and when does it become a hindrance?\n\n---\n\nI recently stumbled across the humorous article [You\u0027re Not Building Netflix: Stop Coding Like You Are](https://dev.to/adamthedeveloper/youre-not-building-netflix-stop-coding-like-you-are-1707?ref=dailydev) by Adam \u2014 The Developer. But though it resonates in many ways, its broader critique is ultimately misdirected.\n\nAdam opens with a more complete version of the code mocked in my prologue, and uses the verbosity and obscurity of that abstraction pile as the springboard for a rebuke of using enterprise patterns pretty much across the board. While the author does allow for some abstractions, they\u0027re limited in application and scope.\n\nThe problem isn\u2019t that the complaint is wrong. It\u2019s that it points the finger at the wrong culprit, just about literally missing the forest for the trees.\n\nAbstractions are fundamental and essential. They are the elementary particles of software, the quarks and leptons that bind into the subatomic structures that become the atoms our earliest techno-wizards fused into molecules. Today we combine those same basic elements into the compounds and contraptions made for use by millions. Without abstractions, we are left helpless in the ever-increasing parallel streams of pulsating electrical currents, rushing through specialized, intricately forged rocks that artisan wizards once trapped lightning inside and somehow convinced to think.\n\nBut even with all that power at our disposal, the way we use these building blocks matters. Chemistry offers a fitting parallel. Food chemists, for example, have spent decades learning how to repurpose industrial byproducts into stabilizers, textures, preservatives, and anything else that can be quietly slipped into a recipe. Much of this work is impressive and innovative, but some of it is little more than creative waste disposal disguised as convenience: a brilliant hack in the short term and a lingering problem in the long one.\n\nDevelopers can fall into the same pattern. We learn a new technique or pattern or clever trick and then spend the next year pouring it into every beaker we can find. We are not always discovering better processes. Sometimes we are just repackaging the same product and calling it progress. When that happens, we are not solving problems. We are manufacturing new ones that future maintainers will curse our names over.\n\nA developer must be architect, engineer, mechanic, and driver all at once. It is fine to know how to fix a specific issue, but once that problem is solved, that knowledge should become a building block for solving the next one. If we keep returning to maintain the same solution day after day, then what we built was never a solution at all. It was a slow-burning maintenance burden that we misfiled as \u0022done.\u0022\n\nAbstractions exist to reduce complexity, not to multiply it. Their purpose is to lighten the cognitive load, to lift the details off your desk so you can see the shape of the problem in front of you. Terse, repetitive, wire-on-the-floor code that looks like it tumbled out of a [flickering green CRT from 1999 may impress the authors who have stared at machine code long enough to discern hair color from a data stream](https://youtu.be/MvEXkd3O2ow), but it does not serve the broader team or the system that outlives them. Abstractions only do their job when they are aligned with the actual problem being solved, and that brings us to the part many developers skip entirely: modeling your software after the problem you are solving.\n\n## Seeing the Problem Before Solving It\n\nWhen you build a system, any system, even a disposable script, the first responsibility is understanding why it exists. What problem does it address. Has that problem been solved before. If so, what makes the existing solution insufficient for you now. Understanding that difference is the foundation that everything else must sit on.\n\nI learned this the hard way as a homeowner. My house is old enough to have grounded me if I talked back to it as a teenager. A couple of years ago we went through a near-total remodel. We did some work before and shortly after our daughter was born, but within a year new problems started surfacing. We brought in a structural engineer. The slab foundation was heaving. After some exploration we discovered the culprit: the original cast iron sewage line had split along both the top and bottom, creating pressure changes and settling issues throughout the house.\n\nThe fix was not small. We pulled up every inch of flooring. Replaced baseboards. Repaired drywall. Fixed the broken line. Repainted entire sections. Redid trim. Installed piers. Pumped in foundation foam. Cashed in favors. Lost many weekends. And yet, even with all that, it still cost far less than buying an equivalent house in the current market at the current rates.\n\nThe lesson is simple. Things are rarely a total loss. Even when a structure looks hopeless, even when someone has effectively set fire to best practices, even when regulations or markets or technologies have shifted dramatically, there are almost always assets worth salvaging inside the wreckage. You should not bulldoze unless you know you have truly exhausted the alternatives.\n\nBefore throwing away any system and starting another from scratch, assess what you already have. Understand what is broken, what is sound, and what simply needs reinforcement. Software, like houses, tends to rot in specific places for specific reasons. Understanding those reasons is what separates renovation from reinvention.\n\n## The Nightstand Problem\n\nThe same principle applies at a smaller scale. You may already own a perfectly functional house with perfectly functional furniture, yet still want a nightstand you do not currently possess. Your choices are straightforward. You can hope someone has decided to let go of one that happens to meet your criteria. That is the open source gamble. You can buy one, constrained only by budget and whatever definition of quality the manufacturer is committed to that week. Or you can build one yourself, limited only by your skills, imagination, and tolerance for sawdust.\n\nIf your goal is personal satisfaction or experimentation, then by all means build the nightstand. But if your goal is to sell or support a product that helps make money, you are no longer just hobby-carpenting. You are operating in the domain of enterprise software.\n\nAnd when you are building enterprise software, you must view the system from the top down while designing from the bottom up. From the top down, you think about every consumer of your system. In academic terms these are actors. Any system intended to be used, whether by humans or machines, is defined by the interactions between its actors and its responsibilities. Even an autonomous system is both an actor and the architected environment it operates within.\n\nThis perspective matters because it forces your abstractions to model the real world rather than some internal taxonomy of clever names. Good abstractions emerge from an understanding of the domain. Bad abstractions emerge from an understanding of a design pattern book.\n\nAnd if you want maintainability, clarity, and longevity, you always want the first.\n\n## Building from Both Directions\n\nDesigning software means working from two directions at once. On one hand, you must understand the behavior your system must exhibit. On the other hand, you must understand the shape of the world it lives in. Systems are not invented whole cloth; they crystallize out of the interactions between intentions and constraints. If you ignore either direction, you end up with something brittle, confused, overbuilt, or perpetually unfinished.\n\nThere is nothing sacred about any particular architectural style. Pick Domain-Driven, Clean, Vertical Slice, Hexagonal, Layered, or something entirely different. The choice matters far less than your consistency and your commitment to encapsulating concerns properly. Different problems require different arrangements of the same conceptual ingredients. From high altitude, two domains may look identical. Once you descend toward the details, you often discover that one is a bird and the other is an airplane. The trick is knowing when to zoom out and when to zoom in.\n\nPlenty of developers jump immediately into code, but the outside of the system is always the real beginning. What is it supposed to do. Who uses it. Who does it talk to. Who builds it. Who runs it. Who deploys it. Who monitors it. How do you prove it works. These questions define the problem space, and the problem space determines the boundaries and responsibilities your abstractions must reflect.\n\nEven something as small as a script must obey this reality.\n\nConsider a simple provisioning script. First it reads a certificate from the local filesystem so it can authenticate with a remote host. Next it opens an SFTP connection to a distribution server and retrieves a zip file. Then it extracts the archive to a temporary directory provided by the operating system. Finally it executes whatever installers or configuration commands the archive contains.\n\nOn the surface this is straightforward, yet every step is shaped by the environment in which it operates. Tools differ between platforms. Available executables change. File paths and separators vary. Temporary directory locations vary. Even the existence or reliability of SFTP clients varies. None of this means we must implement every possible alternative upfront, but it does mean we should acknowledge the existence of alternatives and avoid designing ourselves into a corner where adding support later requires rewriting the entire script.\n\nThis principle scales upward. You may choose to place your application data inside a database, but scattering SQL statements across your codebase is an anti-pattern in nearly any architecture not explicitly about database engines or ORM internals. Unless you are writing an RDBMS, data access is rarely the star of the show. The real substance lives in the application logic that interprets, transforms, regulates, or composes that data. Mixing data access concerns directly into that logic creates friction. Separating them reduces friction, which improves maintainability, which improves confidence, which improves speed.\n\nThe guiding question is always the same: does this choice help my system model the problem more clearly, or does it merely model my current implementation?\nIf it is the former, great. If it is the latter, you are accumulating technical debt even if the code looks clean.\n\nAbstractions aligned with the domain allow your system to grow gracefully. But abstractions aligned with your tooling force your system to grow awkwardly and inconsistently.\n\nThis is the difference between designing from both directions and designing from just one.\n\n## Behavior as the Backbone of Architecture\n\nAt some point in every software project, the discussion inevitably turns to architecture. Engineers debate whether they should adopt Domain-Driven Design or Clean Architecture, whether their services ought to be hexagonal, layered, vertical-sliced, modular, or some other fashionable geometric configuration, and whether interfaces belong everywhere or nowhere at all. These conversations are interesting, even entertaining, but they often drift into abstraction for abstraction\u2019s sake. The problem is rarely the patterns themselves; rather, it is that these debates frequently occur in a vacuum, disconnected from the actual behaviors the system must exhibit. Humans love patterns, but software only cares about whether it does the right thing.\n\nThe most reliable way to design a system, therefore, is to begin with its behavior. A system exists to do something, and if we do not articulate that something clearly, everything downstream becomes guesswork and improvisation. This is precisely where behavior-driven development demonstrates its value. I explore this more deeply in [BDD: Make the Business Write Your Tests](https://jerrettdavis.com/blog/posts/making-the-business-write-your-tests-with-bdd), but in short, BDD forces us to express the responsibilities of the system in language that is precise, verifiable, and shared by both technical and nontechnical stakeholders. A behavior becomes a specification, a test, a boundary, and a contract all at once.\n\nFrom an architectural perspective, this shift in thinking is transformative. When we model the largest and most meaningful behaviors first and place an abstraction around them, we create an outer shell that defines the system at a conceptual level. From there, we move inward, breaking behaviors down iteratively into smaller and more specific responsibilities. Each division suggests a natural abstraction, but these abstractions are not arbitrary. They emerge directly from the behavior they represent. They are shaped not by the developer\u2019s preferred patterns but by the needs of the domain itself. This recursive approach ensures that abstractions mirror intent rather than implementation details.\n\nImportantly, this recursion is not fractal. We are not attempting to subdivide reality endlessly. Rather, we refine behaviors only until they are sufficiently well understood to be implemented cleanly. Much as one does not explain quantum chromodynamics to teach someone how to scramble an egg, we do not decompose software beyond what clarity and accuracy require. And while many languages encourage the use of interfaces as the primary mechanism for abstraction, the interface is not the abstraction itself. It is merely a convenient way to enforce a contract. The real abstraction is the conceptual boundary it represents. Whether that boundary is expressed as an interface, a type, a configuration object, or a module is irrelevant as long as the contract is clear and consistent.\n\nThis is why starting with abstractions like an \u0060IHost\u0060 that orchestrates an \u0060IApplication\u0060 works so well. These constructs mirror the system\u2019s highest-level behaviors. Once defined, they allow us to drill inward, step by step, carving out responsibilities until the domain takes shape as a set of interlocking, behavior-aligned components. When abstractions are created this way, they tend to be stable. They align with the problem domain rather than the transient needs of a particular implementation, and therefore they seldom need to change unless the underlying behavior changes.\n\nFrequent modification of an abstraction is a warning sign. A well-formed abstraction typically changes only under three conditions: the business behavior has evolved, an overlooked edge case has surfaced, or the original abstraction contained a conceptual flaw. Outside of those circumstances, the need to repeatedly modify an abstraction usually indicates that its boundaries were drawn incorrectly. When adjusting one behavior forces changes across multiple components, the issue is rarely \u0022too many\u0022 or \u0022too few\u0022 abstractions in an abstract sense. Instead, it is a failure of alignment. The abstraction does not adequately contain the concerns it is supposed to model, and complexity is leaking out of its container and into the rest of the codebase.\n\nModern tooling makes this problem even more evident. With the availability of source generators, analyzers, expressive type systems, code scaffolding, and dynamic configuration pipelines, there is increasingly little justification for sprawling boilerplate or repetitive structural code. Boilerplate is not a mark of engineering rigor. It is simply untested and uninteresting glue repeated dozens of times because someone did not take steps to automate it. Good abstractions, by contrast, elevate meaning. They allow the domain to be expressed directly without forcing the developer to wade through noise.\n\nThis leads naturally to what I consider the ideal state of modern development: a system that is entirely automated from the moment code touches a repository until the moment it reaches a production-like environment. Compilation, testing, packaging, deployment, orchestration, and infrastructure provisioning should not require human involvement. The only manual step should be expressing intent in the form of new or updated behaviors. Every function that exists within the system should originate as a behavior-driven specification capable of running the entire application inside a controlled test environment, complete with containerized dependencies and UI automation tools such as Playwright. Those same tests should also be able to stub dependencies so the scenarios can run in isolation. When the system itself is treated as the first unit under test, orchestration becomes a priority rather than an afterthought.\n\nAchieving this level of automation depends on stability, and that stability depends on disciplined abstraction. Any element that may vary across environments, including configuration values, credentials, infrastructure, connection details, and policies, must be isolated behind settings and contracts that the application can consume without knowing anything about the environment it runs in. Once this encapsulation is in place, behavior-driven specifications can operate confidently, verifying the correctness of the system from the outside in even while its internal components remain free to evolve.\n\nFinally, it is worth stating explicitly that hand-writing repetitive boilerplate code in a CRUD-heavy application, such as repositories, controllers, mappers, DTOs, validators, or entire edge-to-edge layers, is not admirable craftsmanship. It is busywork. If you have twenty entities with identical structural behavior and you are manually writing twenty sets of nearly identical files, the issue is not insufficient discipline. It is insufficient automation. Whether through source generators, templates, reflection-based pipelines, or dynamic modules, these problems can and should be solved generically. Engineers should focus their manual effort on the places where meaning lives: the domain, the behavior, and the boundaries.\n\nGood abstractions do not eliminate complexity; they contain it. Bad abstractions distribute it. And behavior-driven, problem-first design is how we tell the difference.\n\n## From Story to Spec: Describing Behavior First\n\nTo make this concrete, return to our original \u0022Add Middle Initial to User Name Display\u0022 ticket. Most teams would handle this with a couple of unit tests directly against whatever \u0060UserNameService\u0060 or \u0060UserNameFormatter\u0060 happens to exist. The tests would exercise a particular class, call a particular method, and assert on a particular string. That can work, but it starts at the implementation, not at the behavior.\n\nIf instead we begin with behavior, the specification sounds more like this:\n\n When a user has a middle name, show the middle initial between the first and last name.\n When a user does not have a middle name, omit the gap entirely.\n When a display style changes (for example, \u0022formal\u0022 versus \u0022informal\u0022), the rules about how the middle initial appears should still hold.\n\nThat is the contract. It does not mention classes, factories, or strategies. It talks about what the system must do from the outside.\n\nWith something like my project [TinyBDD](https://github.com/jerrettdavis/TinyBDD), that kind of behavior becomes executable in a fairly direct way. Using the xUnit adapter, a scenario might look like this:\n\n\u0060\u0060\u0060csharp\nusing TinyBDD.Xunit;\nusing Xunit;\n\n[Feature(\u0022User name display\u0022)]\npublic class UserNameDisplayScenarios : TinyBddXunitBase\n{\n    [Scenario(\u0022Standard display includes middle initial when present\u0022)]\n    [Fact]\n    public async Task MiddleInitialIsRenderedWhenPresent()\n    {\n        await Given(\u0022a user with first, middle, and last name\u0022, () =\u003E\n                new UserName(\u0022Ada\u0022, \u0022M\u0022, \u0022Lovelace\u0022))\n            .When(\u0022formatting the user name for standard display\u0022, user =\u003E\n                UserNameDisplay.Standard.Format(user))\n            .Then(\u0022the result places the middle initial between first and last\u0022, formatted =\u003E\n                formatted == \u0022Ada M. Lovelace\u0022)\n            .AssertPassed();\n    }\n\n    [Scenario(\u0022Standard display omits missing middle initial\u0022)]\n    [Fact]\n    public async Task NoMiddleInitialWhenMissing()\n    {\n        await Given(\u0022a user with only first and last name\u0022, () =\u003E\n                new UserName(\u0022Ada\u0022, null, \u0022Lovelace\u0022))\n            .When(\u0022formatting the user name for standard display\u0022, user =\u003E\n                UserNameDisplay.Standard.Format(user))\n            .Then(\u0022no dangling spaces or periods appear\u0022, formatted =\u003E\n                formatted == \u0022Ada Lovelace\u0022)\n            .AssertPassed();\n    }\n}\n\u0060\u0060\u0060\n\nIn these scenarios, the behavior is the first-class citizen. The test does not care whether you use a \u0060UserNameDisplayStrategyFactory\u0060, a dependency-injected \u0060IUserNameFormatter\u0060, or a static helper hidden in a dusty corner of your codebase. It cares that given a user, when you format their name, you get the right string.\n\nThe abstractions are already visible in the code, but only as a side effect of expressing behavior:\n\n \u0060UserName\u0060 represents the domain concept of a person\u2019s name, not a UI or persistence model.\n \u0060UserNameDisplay.Standard\u0060 represents a particular display style that the business cares about.\n The behavior is encoded in the transition from \u0060UserName\u0060 to the formatted string, not in a particular class hierarchy.\n\nNotice what is not present: we do not have separate strategies for every permutation of name structure, locale, and display preference. We have a single coherent abstraction around \u0022displaying a user name in the standard way,\u0022 and the test drives the rules we actually need.\n\n## Letting Abstractions Fall Out of the Domain\n\nOnce you have a behavior-focused spec, the abstractions almost draw themselves. One reasonable implementation might look like this:\n\n\u0060\u0060\u0060csharp\npublic sealed record UserName(\n    string First,\n    string? Middle,\n    string Last);\n\npublic interface IUserNameDisplay\n{\n    string Format(UserName name);\n}\n\npublic sealed class StandardUserNameDisplay : IUserNameDisplay\n{\n    public string Format(UserName name)\n    {\n        if (!string.IsNullOrWhiteSpace(name.Middle))\n        {\n            return $\u0022{name.First} {name.Middle[0]}. {name.Last}\u0022;\n        }\n\n        return $\u0022{name.First} {name.Last}\u0022;\n    }\n}\n\npublic static class UserNameDisplay\n{\n    public static readonly IUserNameDisplay Standard = new StandardUserNameDisplay();\n}\n\u0060\u0060\u0060\n\nThis is not an argument that every trivial formatting problem deserves an interface and a concrete class. You could inline this logic in a static helper and your tests above would still pass. The point is that the abstraction here is small, meaningful, and directly aligned with the behavior we care about. If later the domain grows to include multiple display styles, cultures, or localization concerns, there is already a clear seam to extend. You can introduce additional \u0060IUserNameDisplay\u0060 implementations where and when they are genuinely needed, not because a pattern catalog declared that every noun deserves a factory.\n\nIf, however, you discover that adding a new behavior requires touching half the classes in the system, that is a sign you have modeled implementation variants rather than domain concepts. The behavior spec remains constant; the code churn reveals where your abstractions are misaligned.\n\n## Scaling the Same Idea Up to the System Level\n\nSo far this is all very local. A name goes in, a formatted string comes out. Real systems have much more interesting behaviors: accepting traffic, orchestrating workflows, integrating with external services, healing from transient failures, deploying safely, and so on.\n\nThe same discipline still applies. You can treat the application itself as the unit under test and express its behavior with the same style of specification. A high-level scenario might read something like this:\n\n Given a configured application host and its dependencies\n When the host starts\n Then the public API responds to a health probe\n And all critical services report healthy\n And any failing dependency is surfaced clearly rather than silently ignored\n\nAs an executable TinyBDD scenario, that might look like:\n\n\u0060\u0060\u0060csharp\nusing TinyBDD.Xunit;\nusing Xunit;\n\n[Feature(\u0022Application startup and health\u0022)]\npublic class ApplicationHealthScenarios : TinyBddXunitBase\n{\n    [Scenario(\u0022Host starts and exposes a healthy API surface\u0022)]\n    [Fact]\n    public async Task HostStartsAndReportsHealthy()\n    {\n        await Given(\u0022a test host with default configuration\u0022, () =\u003E\n                TestApplicationHost.CreateDefault())\n            .When(\u0022the host is started\u0022, async host =\u003E\n            {\n                await host.StartAsync();\n                return host;\n            })\n            .Then(\u0022the health endpoint returns OK\u0022, async host =\u003E\n                await AssertHealthEndpointOk(host, \u0022/health\u0022))\n            .And(\u0022all critical health checks pass\u0022, async host =\u003E\n                await AssertCriticalChecksPass(host))\n            .AssertPassed();\n    }\n\n    private static Task AssertHealthEndpointOk(TestApplicationHost host, string path)\n    {\n        // This could exercise a real HTTP endpoint against a TestServer or containerized instance.\n        // The assertion lives here, but the behavior is defined in the scenario above.\n        throw new NotImplementedException();\n    }\n\n    private static Task AssertCriticalChecksPass(TestApplicationHost host)\n    {\n        // Could query IHealthCheckPublisher, metrics, logs, or an in-memory probe endpoint.\n        throw new NotImplementedException();\n    }\n}\n\u0060\u0060\u0060\n\nThe implementation details behind \u0060TestApplicationHost\u0060 are intentionally omitted here, because they are not the main point. What matters is that at the boundary, we are still describing behavior: the host starts, the API responds, health checks pass. Internally, \u0060TestApplicationHost\u0060 can wrap an \u0060IHost\u0060, use Testcontainers, spin up a \u0060WebApplicationFactory\u0060, or compose a full stack in Docker. The abstraction exists to let the behavior remain stable while infrastructure details evolve.\n\nThis is the same pattern you used on the small scale with \u0060UserNameDisplay\u0060, only now it operates at the level of the entire application. The outermost abstraction represents the system as it is experienced from the outside. Everything underneath exists to satisfy that experience.\n\n## Declarative Core, Automated Edge\n\nOnce specifications like these exist, they become the backbone of your automation. The natural end state is a development flow where:\n\n A new behavior is introduced as a TinyBDD scenario.\n That scenario boots the application in a realistic but controlled environment.\n The rest of the stack compiles, configures, and deploys itself into a test harness without manual intervention.\n The same scenarios run in local, CI, and pre-production environments, with only configuration differing between them.\n\nThe actual application code can then remain highly declarative. Controllers or handlers describe what should happen when a request arrives. Domain services express rules and policies in terms of value objects and aggregates. Infrastructure concerns hide behind interfaces and adapters. Source generators or templates can remove boilerplate around repetitive CRUD or mapping concerns. The tests remain focused on behavior: does the system do what we said it would do.\n\nAbstractions in this world are not ornamental. They are the scaffolding that holds the behavior in place while the infrastructure and implementation details shift around it. As long as the specifications stay clear and the boundaries remain aligned with the domain, you can move quickly without losing correctness. And if you ever find yourself adding yet another \u0060UserNameDisplayStrategyFactoryFactory\u0060 just to keep a scenario passing, you will at least have a clear, behavior-centric lens through which to recognize that something has gone wrong.\n\n## Design Patterns, Declarative Thinking, and Solving the General Case\n\nBefore closing, there is one more point worth addressing, because it tends to resurface in every conversation about abstraction: the role of design patterns. Patterns are often misunderstood in practice. For some teams they become a dictionary of shapes to copy. For others they become a superstition, something to be avoided because they \u0022feel enterprise.\u0022 In reality, design patterns are nothing more than reusable expressions of relationships that occur frequently in software. They only become harmful when applied without context.\n\nUsed well, patterns are a form of declarative modeling. They describe how things relate, not how many classes must be introduced to satisfy a template. This distinction is one reason I created [PatternKit](https://github.com/jerrettdavis/patternkit)\nwhich contains fluent implementations of every GoF pattern. The aim is not to celebrate patterns for their own sake, but to show that they can be expressed clearly and idiomatically, without the ceremony that has accumulated around them over the past few decades. A fluent strategy or builder is readable because it conveys meaning, not because it adheres to a UML diagram. A properly shaped composite or decorator is useful because it matches the problem at hand, not because the catalog says \u0022now is the time.\u0022\n\nPatterns at their best are accelerants for thought. They give structure to complex behavior. They reveal seams in the domain. They help us express intent without prescribing a particular class arrangement. When applied declaratively, patterns become lightweight tools that reinforce clarity rather than obstacles that obscure it.\n\nThis is the same principle that guides good abstractions. We should always aim to solve the general case when appropriate, rather than re-solving the same narrow problem in twenty different places. Shared operations belong in helpers or extensions not because we want fewer lines of code, but because meaning belongs in one place rather than scattered across many. Wrapping behavior in a well-designed abstraction is not indulgence; it is about shaping the domain so the rest of the system can grow without friction. Once the domain is sufficiently modeled, higher-order helpers, pipelines, or policy objects can provide a unified vocabulary for orchestrating that domain. These are the moments when patterns shine: when they articulate a common structure behind several similar problems and offer a clean way to express the variation between them.\n\nPatterns should not be forced; they should emerge. If you find yourself retrofitting the domain to suit a pattern, the pattern is wrong. If a pattern clarifies the domain, it is the right one. When in doubt, your behaviors and your domain model will tell you the truth.\n\n## Closing Thoughts\n\n\nAt every scale, from a one-line helper to a distributed system with dozens of services, the same principles hold. Begin with behavior. Shape abstractions around the real problems rather than your implementation preferences. Allow patterns to emerge naturally when they clarify meaning. Lean on automation and declarative structures to eliminate noise. Let stability arise from good boundaries rather than rigid frameworks. And above all, keep re-examining the domain as it evolves. Systems live for years; code is rewritten many times. The only reliable compass is the domain itself.\n\nYou do not hate abstractions. What you hate are the wrong ones: misplaced layers, premature hierarchies, needless ceremony, half-understood patterns applied because someone did not want to think. But abstractions that arise from behavior and domain, shaped carefully and used intentionally, are not burdens. They are the tools that let us build systems that last.\n\nAnd if you stay anchored to this style of approach, you won\u2019t end up navigating a small cavern system of strategies and factories and wondering how a middle initial turned into a guided tour of every pattern someone ever took a little too seriously. You won\u2019t need to fear abstraction at all. Instead, you will wield it where it belongs.",
    "contentHtml": "\u003Cp\u003EIt\u2019s an hour until you\u2019re free for the weekend, and you\u2019re trying to knock out one\nlast ticket before you escape into whatever assuredly action-packed plans await you.\nYou spot a seemingly harmless task: \u0026quot;Add Middle Initial to User Name Display.\u0026quot;\u003C/p\u003E\n\u003Cp\u003EYou chuckle. Easy. A palate cleanser. A victory lap.\u003Cbr /\u003E\nYou assign the ticket, flip it from \u003Ccode\u003ENew\u003C/code\u003E to \u003Ccode\u003EActive\u003C/code\u003E, and let your IDE warm up\u003Cbr /\u003E\nwhile you drift into a pleasant daydream about not being here.\u003C/p\u003E\n\u003Cp\u003EBut then the search results begin to appear.\u003Cbr /\u003E\nSlowly. Line by line.\u003C/p\u003E\n\u003Cp\u003EAnd your reverie begins to rot.\u003C/p\u003E\n\u003Cblockquote\u003E\n\u003Cp\u003E\u003Ccode\u003EIUserNameStrategy\u003C/code\u003E, \u003Ccode\u003EUserNameContext\u003C/code\u003E, \u003Ccode\u003EUserNameDisplayStrategyFactory\u003C/code\u003E,\u003Cbr /\u003E\n\u003Ccode\u003EStandardUserNameDisplayStrategy\u003C/code\u003E, \u003Ccode\u003EFormalUserNameDisplayStrategy\u003C/code\u003E,\u003Cbr /\u003E\n\u003Ccode\u003EInformalUserNameDisplayStrategy\u003C/code\u003E, \u003Ccode\u003EUserNameDisplayModule\u003C/code\u003E, \u2026\u003C/p\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003EIncredulous, your chuckle hardens into a throat-scraping noise somewhere\nbetween a laugh and a cry.\u003C/p\u003E\n\u003Cp\u003E\u0026quot;What in the Gang-of-Fuck is happening here,\u0026quot; you think, feeling your pulse tick up.\n\u0026quot;Either someone read \u003Ca href=\u0022https://refactoring.guru\u0022\u003ERefactoring.Guru\u003C/a\u003E like it was scripture and decided to\nbaptize the codebase in every pattern they learned, or a grizzled enterprise\nveteran escaped some Netflix-adjacent monolith and is trying to upskill in\nTypeScript. Because surely no sane developer would build this much redirection\nfor such a trivial feature\u2026 right?\u0026quot;\u003C/p\u003E\n\u003Cp\u003EThat tiny, spiraling task is a perfect microcosm of a continuous debate across engineering circles: when does abstraction help, and when does it become a hindrance?\u003C/p\u003E\n\u003Chr /\u003E\n\u003Cp\u003EI recently stumbled across the humorous article \u003Ca href=\u0022https://dev.to/adamthedeveloper/youre-not-building-netflix-stop-coding-like-you-are-1707?ref=dailydev\u0022\u003EYou\u0027re Not Building Netflix: Stop Coding Like You Are\u003C/a\u003E by Adam \u2014 The Developer. But though it resonates in many ways, its broader critique is ultimately misdirected.\u003C/p\u003E\n\u003Cp\u003EAdam opens with a more complete version of the code mocked in my prologue, and uses the verbosity and obscurity of that abstraction pile as the springboard for a rebuke of using enterprise patterns pretty much across the board. While the author does allow for some abstractions, they\u0027re limited in application and scope.\u003C/p\u003E\n\u003Cp\u003EThe problem isn\u2019t that the complaint is wrong. It\u2019s that it points the finger at the wrong culprit, just about literally missing the forest for the trees.\u003C/p\u003E\n\u003Cp\u003EAbstractions are fundamental and essential. They are the elementary particles of software, the quarks and leptons that bind into the subatomic structures that become the atoms our earliest techno-wizards fused into molecules. Today we combine those same basic elements into the compounds and contraptions made for use by millions. Without abstractions, we are left helpless in the ever-increasing parallel streams of pulsating electrical currents, rushing through specialized, intricately forged rocks that artisan wizards once trapped lightning inside and somehow convinced to think.\u003C/p\u003E\n\u003Cp\u003EBut even with all that power at our disposal, the way we use these building blocks matters. Chemistry offers a fitting parallel. Food chemists, for example, have spent decades learning how to repurpose industrial byproducts into stabilizers, textures, preservatives, and anything else that can be quietly slipped into a recipe. Much of this work is impressive and innovative, but some of it is little more than creative waste disposal disguised as convenience: a brilliant hack in the short term and a lingering problem in the long one.\u003C/p\u003E\n\u003Cp\u003EDevelopers can fall into the same pattern. We learn a new technique or pattern or clever trick and then spend the next year pouring it into every beaker we can find. We are not always discovering better processes. Sometimes we are just repackaging the same product and calling it progress. When that happens, we are not solving problems. We are manufacturing new ones that future maintainers will curse our names over.\u003C/p\u003E\n\u003Cp\u003EA developer must be architect, engineer, mechanic, and driver all at once. It is fine to know how to fix a specific issue, but once that problem is solved, that knowledge should become a building block for solving the next one. If we keep returning to maintain the same solution day after day, then what we built was never a solution at all. It was a slow-burning maintenance burden that we misfiled as \u0026quot;done.\u0026quot;\u003C/p\u003E\n\u003Cp\u003EAbstractions exist to reduce complexity, not to multiply it. Their purpose is to lighten the cognitive load, to lift the details off your desk so you can see the shape of the problem in front of you. Terse, repetitive, wire-on-the-floor code that looks like it tumbled out of a \u003Ca href=\u0022https://youtu.be/MvEXkd3O2ow\u0022\u003Eflickering green CRT from 1999 may impress the authors who have stared at machine code long enough to discern hair color from a data stream\u003C/a\u003E, but it does not serve the broader team or the system that outlives them. Abstractions only do their job when they are aligned with the actual problem being solved, and that brings us to the part many developers skip entirely: modeling your software after the problem you are solving.\u003C/p\u003E\n\u003Ch2 id=\u0022seeing-the-problem-before-solving-it\u0022\u003ESeeing the Problem Before Solving It\u003C/h2\u003E\n\u003Cp\u003EWhen you build a system, any system, even a disposable script, the first responsibility is understanding why it exists. What problem does it address. Has that problem been solved before. If so, what makes the existing solution insufficient for you now. Understanding that difference is the foundation that everything else must sit on.\u003C/p\u003E\n\u003Cp\u003EI learned this the hard way as a homeowner. My house is old enough to have grounded me if I talked back to it as a teenager. A couple of years ago we went through a near-total remodel. We did some work before and shortly after our daughter was born, but within a year new problems started surfacing. We brought in a structural engineer. The slab foundation was heaving. After some exploration we discovered the culprit: the original cast iron sewage line had split along both the top and bottom, creating pressure changes and settling issues throughout the house.\u003C/p\u003E\n\u003Cp\u003EThe fix was not small. We pulled up every inch of flooring. Replaced baseboards. Repaired drywall. Fixed the broken line. Repainted entire sections. Redid trim. Installed piers. Pumped in foundation foam. Cashed in favors. Lost many weekends. And yet, even with all that, it still cost far less than buying an equivalent house in the current market at the current rates.\u003C/p\u003E\n\u003Cp\u003EThe lesson is simple. Things are rarely a total loss. Even when a structure looks hopeless, even when someone has effectively set fire to best practices, even when regulations or markets or technologies have shifted dramatically, there are almost always assets worth salvaging inside the wreckage. You should not bulldoze unless you know you have truly exhausted the alternatives.\u003C/p\u003E\n\u003Cp\u003EBefore throwing away any system and starting another from scratch, assess what you already have. Understand what is broken, what is sound, and what simply needs reinforcement. Software, like houses, tends to rot in specific places for specific reasons. Understanding those reasons is what separates renovation from reinvention.\u003C/p\u003E\n\u003Ch2 id=\u0022the-nightstand-problem\u0022\u003EThe Nightstand Problem\u003C/h2\u003E\n\u003Cp\u003EThe same principle applies at a smaller scale. You may already own a perfectly functional house with perfectly functional furniture, yet still want a nightstand you do not currently possess. Your choices are straightforward. You can hope someone has decided to let go of one that happens to meet your criteria. That is the open source gamble. You can buy one, constrained only by budget and whatever definition of quality the manufacturer is committed to that week. Or you can build one yourself, limited only by your skills, imagination, and tolerance for sawdust.\u003C/p\u003E\n\u003Cp\u003EIf your goal is personal satisfaction or experimentation, then by all means build the nightstand. But if your goal is to sell or support a product that helps make money, you are no longer just hobby-carpenting. You are operating in the domain of enterprise software.\u003C/p\u003E\n\u003Cp\u003EAnd when you are building enterprise software, you must view the system from the top down while designing from the bottom up. From the top down, you think about every consumer of your system. In academic terms these are actors. Any system intended to be used, whether by humans or machines, is defined by the interactions between its actors and its responsibilities. Even an autonomous system is both an actor and the architected environment it operates within.\u003C/p\u003E\n\u003Cp\u003EThis perspective matters because it forces your abstractions to model the real world rather than some internal taxonomy of clever names. Good abstractions emerge from an understanding of the domain. Bad abstractions emerge from an understanding of a design pattern book.\u003C/p\u003E\n\u003Cp\u003EAnd if you want maintainability, clarity, and longevity, you always want the first.\u003C/p\u003E\n\u003Ch2 id=\u0022building-from-both-directions\u0022\u003EBuilding from Both Directions\u003C/h2\u003E\n\u003Cp\u003EDesigning software means working from two directions at once. On one hand, you must understand the behavior your system must exhibit. On the other hand, you must understand the shape of the world it lives in. Systems are not invented whole cloth; they crystallize out of the interactions between intentions and constraints. If you ignore either direction, you end up with something brittle, confused, overbuilt, or perpetually unfinished.\u003C/p\u003E\n\u003Cp\u003EThere is nothing sacred about any particular architectural style. Pick Domain-Driven, Clean, Vertical Slice, Hexagonal, Layered, or something entirely different. The choice matters far less than your consistency and your commitment to encapsulating concerns properly. Different problems require different arrangements of the same conceptual ingredients. From high altitude, two domains may look identical. Once you descend toward the details, you often discover that one is a bird and the other is an airplane. The trick is knowing when to zoom out and when to zoom in.\u003C/p\u003E\n\u003Cp\u003EPlenty of developers jump immediately into code, but the outside of the system is always the real beginning. What is it supposed to do. Who uses it. Who does it talk to. Who builds it. Who runs it. Who deploys it. Who monitors it. How do you prove it works. These questions define the problem space, and the problem space determines the boundaries and responsibilities your abstractions must reflect.\u003C/p\u003E\n\u003Cp\u003EEven something as small as a script must obey this reality.\u003C/p\u003E\n\u003Cp\u003EConsider a simple provisioning script. First it reads a certificate from the local filesystem so it can authenticate with a remote host. Next it opens an SFTP connection to a distribution server and retrieves a zip file. Then it extracts the archive to a temporary directory provided by the operating system. Finally it executes whatever installers or configuration commands the archive contains.\u003C/p\u003E\n\u003Cp\u003EOn the surface this is straightforward, yet every step is shaped by the environment in which it operates. Tools differ between platforms. Available executables change. File paths and separators vary. Temporary directory locations vary. Even the existence or reliability of SFTP clients varies. None of this means we must implement every possible alternative upfront, but it does mean we should acknowledge the existence of alternatives and avoid designing ourselves into a corner where adding support later requires rewriting the entire script.\u003C/p\u003E\n\u003Cp\u003EThis principle scales upward. You may choose to place your application data inside a database, but scattering SQL statements across your codebase is an anti-pattern in nearly any architecture not explicitly about database engines or ORM internals. Unless you are writing an RDBMS, data access is rarely the star of the show. The real substance lives in the application logic that interprets, transforms, regulates, or composes that data. Mixing data access concerns directly into that logic creates friction. Separating them reduces friction, which improves maintainability, which improves confidence, which improves speed.\u003C/p\u003E\n\u003Cp\u003EThe guiding question is always the same: does this choice help my system model the problem more clearly, or does it merely model my current implementation?\nIf it is the former, great. If it is the latter, you are accumulating technical debt even if the code looks clean.\u003C/p\u003E\n\u003Cp\u003EAbstractions aligned with the domain allow your system to grow gracefully. But abstractions aligned with your tooling force your system to grow awkwardly and inconsistently.\u003C/p\u003E\n\u003Cp\u003EThis is the difference between designing from both directions and designing from just one.\u003C/p\u003E\n\u003Ch2 id=\u0022behavior-as-the-backbone-of-architecture\u0022\u003EBehavior as the Backbone of Architecture\u003C/h2\u003E\n\u003Cp\u003EAt some point in every software project, the discussion inevitably turns to architecture. Engineers debate whether they should adopt Domain-Driven Design or Clean Architecture, whether their services ought to be hexagonal, layered, vertical-sliced, modular, or some other fashionable geometric configuration, and whether interfaces belong everywhere or nowhere at all. These conversations are interesting, even entertaining, but they often drift into abstraction for abstraction\u2019s sake. The problem is rarely the patterns themselves; rather, it is that these debates frequently occur in a vacuum, disconnected from the actual behaviors the system must exhibit. Humans love patterns, but software only cares about whether it does the right thing.\u003C/p\u003E\n\u003Cp\u003EThe most reliable way to design a system, therefore, is to begin with its behavior. A system exists to do something, and if we do not articulate that something clearly, everything downstream becomes guesswork and improvisation. This is precisely where behavior-driven development demonstrates its value. I explore this more deeply in \u003Ca href=\u0022https://jerrettdavis.com/blog/posts/making-the-business-write-your-tests-with-bdd\u0022\u003EBDD: Make the Business Write Your Tests\u003C/a\u003E, but in short, BDD forces us to express the responsibilities of the system in language that is precise, verifiable, and shared by both technical and nontechnical stakeholders. A behavior becomes a specification, a test, a boundary, and a contract all at once.\u003C/p\u003E\n\u003Cp\u003EFrom an architectural perspective, this shift in thinking is transformative. When we model the largest and most meaningful behaviors first and place an abstraction around them, we create an outer shell that defines the system at a conceptual level. From there, we move inward, breaking behaviors down iteratively into smaller and more specific responsibilities. Each division suggests a natural abstraction, but these abstractions are not arbitrary. They emerge directly from the behavior they represent. They are shaped not by the developer\u2019s preferred patterns but by the needs of the domain itself. This recursive approach ensures that abstractions mirror intent rather than implementation details.\u003C/p\u003E\n\u003Cp\u003EImportantly, this recursion is not fractal. We are not attempting to subdivide reality endlessly. Rather, we refine behaviors only until they are sufficiently well understood to be implemented cleanly. Much as one does not explain quantum chromodynamics to teach someone how to scramble an egg, we do not decompose software beyond what clarity and accuracy require. And while many languages encourage the use of interfaces as the primary mechanism for abstraction, the interface is not the abstraction itself. It is merely a convenient way to enforce a contract. The real abstraction is the conceptual boundary it represents. Whether that boundary is expressed as an interface, a type, a configuration object, or a module is irrelevant as long as the contract is clear and consistent.\u003C/p\u003E\n\u003Cp\u003EThis is why starting with abstractions like an \u003Ccode\u003EIHost\u003C/code\u003E that orchestrates an \u003Ccode\u003EIApplication\u003C/code\u003E works so well. These constructs mirror the system\u2019s highest-level behaviors. Once defined, they allow us to drill inward, step by step, carving out responsibilities until the domain takes shape as a set of interlocking, behavior-aligned components. When abstractions are created this way, they tend to be stable. They align with the problem domain rather than the transient needs of a particular implementation, and therefore they seldom need to change unless the underlying behavior changes.\u003C/p\u003E\n\u003Cp\u003EFrequent modification of an abstraction is a warning sign. A well-formed abstraction typically changes only under three conditions: the business behavior has evolved, an overlooked edge case has surfaced, or the original abstraction contained a conceptual flaw. Outside of those circumstances, the need to repeatedly modify an abstraction usually indicates that its boundaries were drawn incorrectly. When adjusting one behavior forces changes across multiple components, the issue is rarely \u0026quot;too many\u0026quot; or \u0026quot;too few\u0026quot; abstractions in an abstract sense. Instead, it is a failure of alignment. The abstraction does not adequately contain the concerns it is supposed to model, and complexity is leaking out of its container and into the rest of the codebase.\u003C/p\u003E\n\u003Cp\u003EModern tooling makes this problem even more evident. With the availability of source generators, analyzers, expressive type systems, code scaffolding, and dynamic configuration pipelines, there is increasingly little justification for sprawling boilerplate or repetitive structural code. Boilerplate is not a mark of engineering rigor. It is simply untested and uninteresting glue repeated dozens of times because someone did not take steps to automate it. Good abstractions, by contrast, elevate meaning. They allow the domain to be expressed directly without forcing the developer to wade through noise.\u003C/p\u003E\n\u003Cp\u003EThis leads naturally to what I consider the ideal state of modern development: a system that is entirely automated from the moment code touches a repository until the moment it reaches a production-like environment. Compilation, testing, packaging, deployment, orchestration, and infrastructure provisioning should not require human involvement. The only manual step should be expressing intent in the form of new or updated behaviors. Every function that exists within the system should originate as a behavior-driven specification capable of running the entire application inside a controlled test environment, complete with containerized dependencies and UI automation tools such as Playwright. Those same tests should also be able to stub dependencies so the scenarios can run in isolation. When the system itself is treated as the first unit under test, orchestration becomes a priority rather than an afterthought.\u003C/p\u003E\n\u003Cp\u003EAchieving this level of automation depends on stability, and that stability depends on disciplined abstraction. Any element that may vary across environments, including configuration values, credentials, infrastructure, connection details, and policies, must be isolated behind settings and contracts that the application can consume without knowing anything about the environment it runs in. Once this encapsulation is in place, behavior-driven specifications can operate confidently, verifying the correctness of the system from the outside in even while its internal components remain free to evolve.\u003C/p\u003E\n\u003Cp\u003EFinally, it is worth stating explicitly that hand-writing repetitive boilerplate code in a CRUD-heavy application, such as repositories, controllers, mappers, DTOs, validators, or entire edge-to-edge layers, is not admirable craftsmanship. It is busywork. If you have twenty entities with identical structural behavior and you are manually writing twenty sets of nearly identical files, the issue is not insufficient discipline. It is insufficient automation. Whether through source generators, templates, reflection-based pipelines, or dynamic modules, these problems can and should be solved generically. Engineers should focus their manual effort on the places where meaning lives: the domain, the behavior, and the boundaries.\u003C/p\u003E\n\u003Cp\u003EGood abstractions do not eliminate complexity; they contain it. Bad abstractions distribute it. And behavior-driven, problem-first design is how we tell the difference.\u003C/p\u003E\n\u003Ch2 id=\u0022from-story-to-spec-describing-behavior-first\u0022\u003EFrom Story to Spec: Describing Behavior First\u003C/h2\u003E\n\u003Cp\u003ETo make this concrete, return to our original \u0026quot;Add Middle Initial to User Name Display\u0026quot; ticket. Most teams would handle this with a couple of unit tests directly against whatever \u003Ccode\u003EUserNameService\u003C/code\u003E or \u003Ccode\u003EUserNameFormatter\u003C/code\u003E happens to exist. The tests would exercise a particular class, call a particular method, and assert on a particular string. That can work, but it starts at the implementation, not at the behavior.\u003C/p\u003E\n\u003Cp\u003EIf instead we begin with behavior, the specification sounds more like this:\u003C/p\u003E\n\u003Cp\u003EWhen a user has a middle name, show the middle initial between the first and last name.\nWhen a user does not have a middle name, omit the gap entirely.\nWhen a display style changes (for example, \u0026quot;formal\u0026quot; versus \u0026quot;informal\u0026quot;), the rules about how the middle initial appears should still hold.\u003C/p\u003E\n\u003Cp\u003EThat is the contract. It does not mention classes, factories, or strategies. It talks about what the system must do from the outside.\u003C/p\u003E\n\u003Cp\u003EWith something like my project \u003Ca href=\u0022https://github.com/jerrettdavis/TinyBDD\u0022\u003ETinyBDD\u003C/a\u003E, that kind of behavior becomes executable in a fairly direct way. Using the xUnit adapter, a scenario might look like this:\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-csharp\u0022\u003Eusing TinyBDD.Xunit;\nusing Xunit;\n\n[Feature(\u0026quot;User name display\u0026quot;)]\npublic class UserNameDisplayScenarios : TinyBddXunitBase\n{\n    [Scenario(\u0026quot;Standard display includes middle initial when present\u0026quot;)]\n    [Fact]\n    public async Task MiddleInitialIsRenderedWhenPresent()\n    {\n        await Given(\u0026quot;a user with first, middle, and last name\u0026quot;, () =\u0026gt;\n                new UserName(\u0026quot;Ada\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;Lovelace\u0026quot;))\n            .When(\u0026quot;formatting the user name for standard display\u0026quot;, user =\u0026gt;\n                UserNameDisplay.Standard.Format(user))\n            .Then(\u0026quot;the result places the middle initial between first and last\u0026quot;, formatted =\u0026gt;\n                formatted == \u0026quot;Ada M. Lovelace\u0026quot;)\n            .AssertPassed();\n    }\n\n    [Scenario(\u0026quot;Standard display omits missing middle initial\u0026quot;)]\n    [Fact]\n    public async Task NoMiddleInitialWhenMissing()\n    {\n        await Given(\u0026quot;a user with only first and last name\u0026quot;, () =\u0026gt;\n                new UserName(\u0026quot;Ada\u0026quot;, null, \u0026quot;Lovelace\u0026quot;))\n            .When(\u0026quot;formatting the user name for standard display\u0026quot;, user =\u0026gt;\n                UserNameDisplay.Standard.Format(user))\n            .Then(\u0026quot;no dangling spaces or periods appear\u0026quot;, formatted =\u0026gt;\n                formatted == \u0026quot;Ada Lovelace\u0026quot;)\n            .AssertPassed();\n    }\n}\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EIn these scenarios, the behavior is the first-class citizen. The test does not care whether you use a \u003Ccode\u003EUserNameDisplayStrategyFactory\u003C/code\u003E, a dependency-injected \u003Ccode\u003EIUserNameFormatter\u003C/code\u003E, or a static helper hidden in a dusty corner of your codebase. It cares that given a user, when you format their name, you get the right string.\u003C/p\u003E\n\u003Cp\u003EThe abstractions are already visible in the code, but only as a side effect of expressing behavior:\u003C/p\u003E\n\u003Cp\u003E\u003Ccode\u003EUserName\u003C/code\u003E represents the domain concept of a person\u2019s name, not a UI or persistence model.\n\u003Ccode\u003EUserNameDisplay.Standard\u003C/code\u003E represents a particular display style that the business cares about.\nThe behavior is encoded in the transition from \u003Ccode\u003EUserName\u003C/code\u003E to the formatted string, not in a particular class hierarchy.\u003C/p\u003E\n\u003Cp\u003ENotice what is not present: we do not have separate strategies for every permutation of name structure, locale, and display preference. We have a single coherent abstraction around \u0026quot;displaying a user name in the standard way,\u0026quot; and the test drives the rules we actually need.\u003C/p\u003E\n\u003Ch2 id=\u0022letting-abstractions-fall-out-of-the-domain\u0022\u003ELetting Abstractions Fall Out of the Domain\u003C/h2\u003E\n\u003Cp\u003EOnce you have a behavior-focused spec, the abstractions almost draw themselves. One reasonable implementation might look like this:\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-csharp\u0022\u003Epublic sealed record UserName(\n    string First,\n    string? Middle,\n    string Last);\n\npublic interface IUserNameDisplay\n{\n    string Format(UserName name);\n}\n\npublic sealed class StandardUserNameDisplay : IUserNameDisplay\n{\n    public string Format(UserName name)\n    {\n        if (!string.IsNullOrWhiteSpace(name.Middle))\n        {\n            return $\u0026quot;{name.First} {name.Middle[0]}. {name.Last}\u0026quot;;\n        }\n\n        return $\u0026quot;{name.First} {name.Last}\u0026quot;;\n    }\n}\n\npublic static class UserNameDisplay\n{\n    public static readonly IUserNameDisplay Standard = new StandardUserNameDisplay();\n}\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EThis is not an argument that every trivial formatting problem deserves an interface and a concrete class. You could inline this logic in a static helper and your tests above would still pass. The point is that the abstraction here is small, meaningful, and directly aligned with the behavior we care about. If later the domain grows to include multiple display styles, cultures, or localization concerns, there is already a clear seam to extend. You can introduce additional \u003Ccode\u003EIUserNameDisplay\u003C/code\u003E implementations where and when they are genuinely needed, not because a pattern catalog declared that every noun deserves a factory.\u003C/p\u003E\n\u003Cp\u003EIf, however, you discover that adding a new behavior requires touching half the classes in the system, that is a sign you have modeled implementation variants rather than domain concepts. The behavior spec remains constant; the code churn reveals where your abstractions are misaligned.\u003C/p\u003E\n\u003Ch2 id=\u0022scaling-the-same-idea-up-to-the-system-level\u0022\u003EScaling the Same Idea Up to the System Level\u003C/h2\u003E\n\u003Cp\u003ESo far this is all very local. A name goes in, a formatted string comes out. Real systems have much more interesting behaviors: accepting traffic, orchestrating workflows, integrating with external services, healing from transient failures, deploying safely, and so on.\u003C/p\u003E\n\u003Cp\u003EThe same discipline still applies. You can treat the application itself as the unit under test and express its behavior with the same style of specification. A high-level scenario might read something like this:\u003C/p\u003E\n\u003Cp\u003EGiven a configured application host and its dependencies\nWhen the host starts\nThen the public API responds to a health probe\nAnd all critical services report healthy\nAnd any failing dependency is surfaced clearly rather than silently ignored\u003C/p\u003E\n\u003Cp\u003EAs an executable TinyBDD scenario, that might look like:\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-csharp\u0022\u003Eusing TinyBDD.Xunit;\nusing Xunit;\n\n[Feature(\u0026quot;Application startup and health\u0026quot;)]\npublic class ApplicationHealthScenarios : TinyBddXunitBase\n{\n    [Scenario(\u0026quot;Host starts and exposes a healthy API surface\u0026quot;)]\n    [Fact]\n    public async Task HostStartsAndReportsHealthy()\n    {\n        await Given(\u0026quot;a test host with default configuration\u0026quot;, () =\u0026gt;\n                TestApplicationHost.CreateDefault())\n            .When(\u0026quot;the host is started\u0026quot;, async host =\u0026gt;\n            {\n                await host.StartAsync();\n                return host;\n            })\n            .Then(\u0026quot;the health endpoint returns OK\u0026quot;, async host =\u0026gt;\n                await AssertHealthEndpointOk(host, \u0026quot;/health\u0026quot;))\n            .And(\u0026quot;all critical health checks pass\u0026quot;, async host =\u0026gt;\n                await AssertCriticalChecksPass(host))\n            .AssertPassed();\n    }\n\n    private static Task AssertHealthEndpointOk(TestApplicationHost host, string path)\n    {\n        // This could exercise a real HTTP endpoint against a TestServer or containerized instance.\n        // The assertion lives here, but the behavior is defined in the scenario above.\n        throw new NotImplementedException();\n    }\n\n    private static Task AssertCriticalChecksPass(TestApplicationHost host)\n    {\n        // Could query IHealthCheckPublisher, metrics, logs, or an in-memory probe endpoint.\n        throw new NotImplementedException();\n    }\n}\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EThe implementation details behind \u003Ccode\u003ETestApplicationHost\u003C/code\u003E are intentionally omitted here, because they are not the main point. What matters is that at the boundary, we are still describing behavior: the host starts, the API responds, health checks pass. Internally, \u003Ccode\u003ETestApplicationHost\u003C/code\u003E can wrap an \u003Ccode\u003EIHost\u003C/code\u003E, use Testcontainers, spin up a \u003Ccode\u003EWebApplicationFactory\u003C/code\u003E, or compose a full stack in Docker. The abstraction exists to let the behavior remain stable while infrastructure details evolve.\u003C/p\u003E\n\u003Cp\u003EThis is the same pattern you used on the small scale with \u003Ccode\u003EUserNameDisplay\u003C/code\u003E, only now it operates at the level of the entire application. The outermost abstraction represents the system as it is experienced from the outside. Everything underneath exists to satisfy that experience.\u003C/p\u003E\n\u003Ch2 id=\u0022declarative-core-automated-edge\u0022\u003EDeclarative Core, Automated Edge\u003C/h2\u003E\n\u003Cp\u003EOnce specifications like these exist, they become the backbone of your automation. The natural end state is a development flow where:\u003C/p\u003E\n\u003Cp\u003EA new behavior is introduced as a TinyBDD scenario.\nThat scenario boots the application in a realistic but controlled environment.\nThe rest of the stack compiles, configures, and deploys itself into a test harness without manual intervention.\nThe same scenarios run in local, CI, and pre-production environments, with only configuration differing between them.\u003C/p\u003E\n\u003Cp\u003EThe actual application code can then remain highly declarative. Controllers or handlers describe what should happen when a request arrives. Domain services express rules and policies in terms of value objects and aggregates. Infrastructure concerns hide behind interfaces and adapters. Source generators or templates can remove boilerplate around repetitive CRUD or mapping concerns. The tests remain focused on behavior: does the system do what we said it would do.\u003C/p\u003E\n\u003Cp\u003EAbstractions in this world are not ornamental. They are the scaffolding that holds the behavior in place while the infrastructure and implementation details shift around it. As long as the specifications stay clear and the boundaries remain aligned with the domain, you can move quickly without losing correctness. And if you ever find yourself adding yet another \u003Ccode\u003EUserNameDisplayStrategyFactoryFactory\u003C/code\u003E just to keep a scenario passing, you will at least have a clear, behavior-centric lens through which to recognize that something has gone wrong.\u003C/p\u003E\n\u003Ch2 id=\u0022design-patterns-declarative-thinking-and-solving-the-general-case\u0022\u003EDesign Patterns, Declarative Thinking, and Solving the General Case\u003C/h2\u003E\n\u003Cp\u003EBefore closing, there is one more point worth addressing, because it tends to resurface in every conversation about abstraction: the role of design patterns. Patterns are often misunderstood in practice. For some teams they become a dictionary of shapes to copy. For others they become a superstition, something to be avoided because they \u0026quot;feel enterprise.\u0026quot; In reality, design patterns are nothing more than reusable expressions of relationships that occur frequently in software. They only become harmful when applied without context.\u003C/p\u003E\n\u003Cp\u003EUsed well, patterns are a form of declarative modeling. They describe how things relate, not how many classes must be introduced to satisfy a template. This distinction is one reason I created \u003Ca href=\u0022https://github.com/jerrettdavis/patternkit\u0022\u003EPatternKit\u003C/a\u003E\nwhich contains fluent implementations of every GoF pattern. The aim is not to celebrate patterns for their own sake, but to show that they can be expressed clearly and idiomatically, without the ceremony that has accumulated around them over the past few decades. A fluent strategy or builder is readable because it conveys meaning, not because it adheres to a UML diagram. A properly shaped composite or decorator is useful because it matches the problem at hand, not because the catalog says \u0026quot;now is the time.\u0026quot;\u003C/p\u003E\n\u003Cp\u003EPatterns at their best are accelerants for thought. They give structure to complex behavior. They reveal seams in the domain. They help us express intent without prescribing a particular class arrangement. When applied declaratively, patterns become lightweight tools that reinforce clarity rather than obstacles that obscure it.\u003C/p\u003E\n\u003Cp\u003EThis is the same principle that guides good abstractions. We should always aim to solve the general case when appropriate, rather than re-solving the same narrow problem in twenty different places. Shared operations belong in helpers or extensions not because we want fewer lines of code, but because meaning belongs in one place rather than scattered across many. Wrapping behavior in a well-designed abstraction is not indulgence; it is about shaping the domain so the rest of the system can grow without friction. Once the domain is sufficiently modeled, higher-order helpers, pipelines, or policy objects can provide a unified vocabulary for orchestrating that domain. These are the moments when patterns shine: when they articulate a common structure behind several similar problems and offer a clean way to express the variation between them.\u003C/p\u003E\n\u003Cp\u003EPatterns should not be forced; they should emerge. If you find yourself retrofitting the domain to suit a pattern, the pattern is wrong. If a pattern clarifies the domain, it is the right one. When in doubt, your behaviors and your domain model will tell you the truth.\u003C/p\u003E\n\u003Ch2 id=\u0022closing-thoughts\u0022\u003EClosing Thoughts\u003C/h2\u003E\n\u003Cp\u003EAt every scale, from a one-line helper to a distributed system with dozens of services, the same principles hold. Begin with behavior. Shape abstractions around the real problems rather than your implementation preferences. Allow patterns to emerge naturally when they clarify meaning. Lean on automation and declarative structures to eliminate noise. Let stability arise from good boundaries rather than rigid frameworks. And above all, keep re-examining the domain as it evolves. Systems live for years; code is rewritten many times. The only reliable compass is the domain itself.\u003C/p\u003E\n\u003Cp\u003EYou do not hate abstractions. What you hate are the wrong ones: misplaced layers, premature hierarchies, needless ceremony, half-understood patterns applied because someone did not want to think. But abstractions that arise from behavior and domain, shaped carefully and used intentionally, are not burdens. They are the tools that let us build systems that last.\u003C/p\u003E\n\u003Cp\u003EAnd if you stay anchored to this style of approach, you won\u2019t end up navigating a small cavern system of strategies and factories and wondering how a middle initial turned into a guided tour of every pattern someone ever took a little too seriously. You won\u2019t need to fear abstraction at all. Instead, you will wield it where it belongs.\u003C/p\u003E\n",
    "stub": "Why developers misuse abstractions, how to design them from behavior and domain first, and how patterns, automation, and declarative modeling lead to better systems.",
    "wordCount": 4593,
    "useToc": true,
    "source": "syndicated",
    "canonicalUrl": "https://jerrettdavis.com/blog/you-dont-hate-abstractions"
  },
  {
    "id": "tinybdd",
    "title": "Announcing TinyBDD: Fluent, Executable Scenarios for .NET",
    "date": "2025-09-17T00:00:00",
    "description": "Turn acceptance criteria into executable tests in minutes, use a fluent Gherkin-ish syntax for unit and integration testing, and let your tests shape a cleaner architecture.",
    "featured": null,
    "tags": [
      "bdd",
      "tdd",
      "testing",
      "architecture",
      "solid",
      "playwright",
      "fluent",
      "dotnet",
      "tinybdd"
    ],
    "categories": [
      "Programming",
      "Software Engineering",
      "Programming/Tooling"
    ],
    "series": null,
    "seriesOrder": null,
    "content": "# Announcing TinyBDD: Fluent, Executable Scenarios for .NET\r\n\r\n\uD83D\uDC49 [GitHub](https://github.com/jerrettdavis/tinybdd) \u00B7 [NuGet](https://www.nuget.org/packages/TinyBDD) \u00B7 [Docs](https://jerrettdavis.github.io/TinyBDD/)\r\n\r\n---\r\n\r\nWhat if the shortest path from \u0022we need this\u0022 to \u0022it works in prod\u0022 was just a single fluent line of code?\r\n\r\nTinyBDD is my attempt to make that path real. It\u0027s a lightweight .NET library that lets you write tests in a fluent, Gherkin-ish style\u2014tests that read like acceptance criteria but execute like unit tests. The goal is not ceremony, but clarity: a shared, human-parsable DSL that can span from domain rules to browser automation without losing intent.  \r\n\r\nThis post is the practical follow-up to my earlier [essay on BDD](https://jerrettdavis.com/blog/posts/making-the-business-write-your-tests-with-bdd). There, I dug into the *why*. Here, we\u0027ll focus on the *how*: how to go from acceptance criteria to running tests in minutes, how to use Given/When/Then to model even the smallest units, how to orchestrate full end-to-end flows with Playwright, and how writing this way naturally nudges your architecture toward SOLID and composable design.\r\n\r\n---\r\n\r\n## From acceptance criteria to running tests in minutes\r\n\r\nEvery team has seen a story like this written in Jira or Confluence:\r\n\r\n\u0060\u0060\u0060\r\n\r\nScenario: Gold member gets free shipping\r\nGiven the customer is a \u0022gold\u0022 member\r\nAnd they have a cart totaling $12.00\r\nWhen they checkout with standard shipping\r\nThen the shipping cost is $0.00\r\nAnd the order total is $12.00\r\n\r\n\u0060\u0060\u0060\r\n\r\nWith TinyBDD, you don\u0027t need separate \u0060.feature\u0060 files unless you want them. You can capture that same intent directly in your test framework, keeping the semantics without the tooling overhead:\r\n\r\n\u0060\u0060\u0060csharp\r\nawait Given(\u0022a gold customer with a $12 cart\u0022, () =\u003E new Cart(Customer.Gold(), 12.00m))\r\n     .When(\u0022they choose standard shipping\u0022, cart =\u003E cart.Checkout(Shipping.Standard))\r\n     .Then(\u0022shipping is free\u0022, order =\u003E order.ShippingTotal == 0.00m)             // Pass/Fail with booleans\r\n     .And(\u0022order total is $12.00\u0022, order =\u003E Expect.For(order.Total).ToBe(12.00m)) // Or Assertions\r\n     .AssertPassed();\r\n\u0060\u0060\u0060\u0060\r\n\r\nThe keywords map one-to-one with the business story. Each step is explicit and composable, and the whole chain is easy to read\u2014even for someone outside the dev team. Because the language matches what stakeholders already use, the test itself becomes a living contract.\r\n\r\n---\r\n\r\n## Unit tests that read like behavior\r\n\r\nBehavior-driven style isn\u0027t just for top-level acceptance tests. It works equally well for small pieces of logic: a pure function, a discount rule, a transformer. By expressing them as Given/When/Then, you get readability\u2014tiny scenarios that explain the intent before diving into implementation detail\u2014and design pressure, because the format gently encourages pure, composable functions.\r\n\r\nExample: a simple discount calculation.\r\n\r\n\u0060\u0060\u0060csharp\r\nawait Given(\u0022a silver customer with $100 cart\u0022, () =\u003E (Tier: \u0022silver\u0022, Amount: 100m))\r\n     .When(\u0022discount is applied\u0022, x =\u003E Discounts.Apply(x.Tier, x.Amount))\r\n     .Then(\u0022result is $95\u0022, result =\u003E result == 95m)\r\n     .AssertPassed();\r\n\u0060\u0060\u0060\r\n\r\nEven at this scale, the benefits are obvious. You isolate the decision logic, assert outcomes in plain language, and end up with code that composes neatly into bigger flows later.\r\n\r\n---\r\n\r\n## End-to-end UI tests with Playwright\r\n\r\nTinyBDD also works at the other end of the spectrum: full-stack, end-to-end tests. Here, the key is keeping steps thin and expressive while pushing implementation detail into helpers like page objects or service wrappers. That way, the scenario text stays stable even if the UI shifts underneath.\r\n\r\n\u0060\u0060\u0060csharp\r\nawait Given(\u0022a new browser and logged-in gold user\u0022, async () =\u003E\r\n{\r\n    var pw = await PlaywrightFactory.LaunchAsync();\r\n    var page = await pw.NewPageAsync();\r\n    await AuthSteps.LoginAsGoldAsync(page);\r\n    return page;\r\n})\r\n.When(\u0022user adds a $12 item to cart\u0022, async page =\u003E\r\n{\r\n    await CatalogSteps.AddItemAsync(page, \u0022SKU-123\u0022, 12.00m);\r\n    return page;\r\n})\r\n.And(\u0022proceeds to checkout with standard shipping\u0022, CheckoutSteps.StandardAsync)\r\n.Then(\u0022shipping is free\u0022, async page =\u003E\r\n{\r\n    var shipping = await CartSteps.ReadShippingAsync(page);\r\n    return shipping == 0.00m;\r\n})\r\n.And(\u0022order total is $12.00\u0022, async page =\u003E\r\n{\r\n    var total = await CartSteps.ReadTotalAsync(page);\r\n    return total == 12.00m;\r\n})\r\n.AssertPassed();\r\n\u0060\u0060\u0060\r\n\r\nScenarios like this are readable enough for a stakeholder to skim, while still giving engineers the control they need under the hood. Stable wording, deterministic helpers, and tagging (\u0060smoke\u0060, \u0060ui\u0060, \u0060checkout\u0060) all contribute to making suites like this maintainable in real CI pipelines.\r\n\r\n---\r\n\r\n### Patterns that keep this maintainable\r\n\r\nThe trick to making end-to-end scenarios sustainable is resisting the temptation to let your steps do all the heavy lifting. The step chain should stay thin and intention-revealing, while the real mechanics live in helpers: page objects, domain services, or test utilities. This keeps the scenario text stable even as the implementation evolves. A good rule of thumb is that a non-technical stakeholder should be able to scan the steps and nod along without ever seeing the helper code. Deterministic helpers\u2014free from hidden global state\u2014are key to repeatable results. And once you have a handful of scenarios, you\u0027ll want to tag them (\u0060smoke\u0060, \u0060ui\u0060, \u0060checkout\u0060, etc.) so that CI pipelines can run fast slices for quick feedback and broader sweeps when confidence matters most.\r\n\r\n---\r\n\r\n### Let tests guide your design\r\n\r\nWhen you write tests in a behavior-first style, architectural friction surfaces quickly. A step that requires half a dozen parameters is rarely a coincidence\u2014it usually means your modules are too tightly coupled. Repeating the same tedious setup across multiple scenarios suggests the absence of a proper abstraction. And if you struggle to phrase a step cleanly, the problem may not be the test at all, but the clarity of your domain language.\r\n\r\nThese moments of friction are signals. Often, the fix is to extract a pure function from a messy edge, create a port or adapter to decouple infrastructure from business rules, or split a workflow into smaller seams that deserve their own scenarios. In other words: the pressure you feel in writing the test is your design telling you what it wants to become.\r\n\r\n---\r\n\r\n### BDD and the drift toward SOLID and functional design\r\n\r\nConsistently writing scenarios has a shaping effect on code. Steps that do one clear thing align with the Single Responsibility Principle. The ability to add new scenarios without editing existing ones echoes the Open/Closed Principle. And abstractions that are narrow, well-defined, and swappable make substituting fakes and stubs trivial, pushing you toward Liskov, ISP, and DIP almost by default.\r\n\r\nThe same is true for functional composition. Pure functions naturally slide into Given/When/Then flows. Side effects are easiest to reason about when pushed to the edges\u2014fetching in a Given, transforming in a When, and observing in a Then. And when steps are small and named, they read like a pipeline instead of a mess of conditionals. By following the test style, you often find yourself following the design style too.\r\n\r\n---\r\n\r\n### A practical way to start tomorrow\r\n\r\nIf this feels overwhelming, don\u0027t boil the ocean. Start with one slice of functionality that everyone values and recognizes\u2014maybe a login path or a simple checkout. Write just two or three scenarios, and make sure the wording mirrors how the business describes the flow. Delegate the mechanics to helpers, not the scenario text. Keep your domain logic in pure functions wherever possible so it\u0027s trivial to call from a \u0060When\u0060 step. And once you\u0027ve got a couple of green runs, wire in some tags so you can choose between smoke tests, integration runs, or the full suite depending on your CI needs.\r\n\r\nAs you go, pay attention to the words. If step text feels clumsy, it probably means your ubiquitous language is clumsy too. Refining that wording in collaboration with stakeholders isn\u0027t overhead\u2014it\u0027s the work. And when naming friction crops up, it\u0027s often a smell that your design needs another seam or abstraction.\r\n\r\n---\r\n\r\n### Fluent examples you can copy-paste\r\n\r\nUnit-level:\r\n\r\n\u0060\u0060\u0060csharp\r\nawait Given(\u0022subtotal is $120 and tier is gold\u0022, () =\u003E (Subtotal: 120m, Tier: \u0022gold\u0022))\r\n     .When(\u0022finalize price\u0022, x =\u003E Pricing.Finalize(x.Tier, x.Subtotal))\r\n     .Then(\u0022applies 10% discount\u0022, price =\u003E price == 108m)\r\n     .AssertPassed();\r\n\u0060\u0060\u0060\r\n\r\nAPI-level:\r\n\r\n\u0060\u0060\u0060csharp\r\nawait Given(\u0022a seeded test tenant\u0022, TestData.SeedTenantAsync)\r\n     .When(\u0022posting to /invoices\u0022, async _ =\u003E await Api.PostAsync(\u0022/invoices\u0022, new { amount = 250 }))\r\n     .Then(\u0022returns 201\u0022, r =\u003E r.StatusCode == 201)\r\n     .And(\u0022body contains invoice id\u0022, r =\u003E r.Json.Value\u003Cstring\u003E(\u0022id\u0022) is { Length: \u003E 0 })\r\n     .AssertPassed();\r\n\u0060\u0060\u0060\r\n\r\nUI-level:\r\n\r\n\u0060\u0060\u0060csharp\r\nawait Given(\u0022a logged-in admin\u0022, BrowserSteps.LoginAsAdminAsync)\r\n     .When(\u0022they create a user named Dana\u0022, page =\u003E AdminUsers.CreateAsync(page, \u0022Dana\u0022))\r\n     .Then(\u0022Dana appears in the grid\u0022, page =\u003E AdminUsers.ExistsAsync(page, \u0022Dana\u0022))\r\n     .AssertPassed();\r\n\u0060\u0060\u0060\r\n\r\n---\r\n\r\n### Readable Gherkin-style output\r\n\r\nOne of the nicest touches in TinyBDD is how your scenarios *report themselves* when you run the tests. Pair your scenarios with the appropriate base class (\u0060TinyBddXunitBase\u0060, \u0060TinyBddXunitV3Base\u0060, \u0060TinyBddNUnitBase\u0060, or \u0060TinyBddMSTestBase\u0060), and the test runner will print structured Gherkin output alongside normal results.  \r\n\r\nThat means the Given/When/Then flow you wrote doesn\u0027t just execute\u2014it shows up exactly as you\u0027d expect, step by step, with timings and pass/fail indicators. It turns your test logs into living specifications.\r\n\r\nFor example, here\u0027s the output from a mediator scenario:\r\n\r\n\u0060\u0060\u0060\r\n\r\nFeature: Behavioral - Mediator (commands, notifications, streaming, behaviors)\r\nScenario: Send: command handler runs through behaviors and returns value\r\nGiven a mediator with pre/post/whole behaviors and a Ping-\u003EPong handler [OK] 2 ms\r\nWhen sending Ping(5) [OK] 4 ms\r\nThen result is pong:5 [OK] 2 ms\r\nAnd behaviors logged pre, whole before/after, and post [OK] 0 ms\r\n\r\n\u0060\u0060\u0060\r\n\r\nInstead of squinting at assertions in code, you see a natural-language story of what happened. That\u0027s invaluable when sharing results with stakeholders or debugging failures in CI. And because the feature and scenario titles come from your test class and attributes, the logs stay consistent with the language you use in code reviews, planning, and conversations.\r\n\r\n---\r\n\r\n### Avoiding common anti-patterns\r\n\r\nEvery test framework accumulates bad habits if left unchecked, and TinyBDD is no exception. The most obvious trap is clever wording: steps like \u0022When magic happens\u0022 don\u0027t help anyone and fail to serve as documentation. Instead, the wording should describe an intention that a stakeholder would immediately recognize, such as \u0022When the admin disables the account\u0022. Another trap is letting a single step conceal multiple actions or checks. Keep your flow honest: \u0060When\u0060 should drive effects, and \u0060Then\u0060 or \u0060And\u0060 should assert results.\r\n\r\nSetup is another danger zone. If your \u0060Given\u0060 steps are littered with manual wiring of objects, it\u0027s time for factories or builders to take over. And at the UI layer, brittle selectors quickly make tests flaky; encapsulating them in page objects and using explicit test IDs pays off many times over. Avoiding these pitfalls keeps your suite readable, stable, and genuinely valuable.\r\n\r\n---\r\n\r\n### The payoff\r\n\r\nWhen scenarios read like the business and execute like the code, something special happens. Your tests stop being just a safety net and start becoming living documentation. They never go stale because they\u0027re executable. They provide immediate feedback on drift, so change becomes safer. They subtly nudge your codebase toward SOLID principles and functional seams. And for new developers, they become the best possible onboarding guide: open the suite, read the stories, and understand how the system behaves.\r\n\r\nYou don\u0027t need to retool your world to reach this point. Start with one scenario. Make it pass. Share it with your team. Repeat. In a few sprints, you\u0027ll have a suite of stories that stack from units to workflows, and a codebase that\u0027s easier to evolve because the behaviors are crystal clear.\r\n\r\n---\r\n\r\n### Appendix \u2014 A quick PR lens\r\n\r\nAs you review changes, ask yourself: does this PR add or update scenarios that the business would recognize? Do the steps read like natural English, each mapping to a single intent? Are the domain rules isolated in pure functions rather than tangled in infrastructure? Did we create or clarify a port instead of hard-coding dependencies? Can we tag and run this slice of scenarios independently in CI?\r\n\r\nIf you can answer \u0022yes\u0022 to most of those, you\u0027re not just writing tests\u2014you\u0027re building shared understanding, guiding design, and accelerating delivery. That\u0027s the real promise of TinyBDD.\r\n\r\n---\r\n\r\n\uD83D\uDC49 [Get TinyBDD on GitHub](https://github.com/jerrettdavis/tinybdd) \u00B7 [NuGet](https://www.nuget.org/packages/TinyBDD) \u00B7 [Docs](https://jerrettdavis.github.io/TinyBDD/)",
    "contentHtml": "\u003Ch1 id=\u0022announcing-tinybdd-fluent-executable-scenarios-for.net\u0022\u003EAnnouncing TinyBDD: Fluent, Executable Scenarios for .NET\u003C/h1\u003E\n\u003Cp\u003E\uD83D\uDC49 \u003Ca href=\u0022https://github.com/jerrettdavis/tinybdd\u0022\u003EGitHub\u003C/a\u003E \u00B7 \u003Ca href=\u0022https://www.nuget.org/packages/TinyBDD\u0022\u003ENuGet\u003C/a\u003E \u00B7 \u003Ca href=\u0022https://jerrettdavis.github.io/TinyBDD/\u0022\u003EDocs\u003C/a\u003E\u003C/p\u003E\n\u003Chr /\u003E\n\u003Cp\u003EWhat if the shortest path from \u0026quot;we need this\u0026quot; to \u0026quot;it works in prod\u0026quot; was just a single fluent line of code?\u003C/p\u003E\n\u003Cp\u003ETinyBDD is my attempt to make that path real. It\u0027s a lightweight .NET library that lets you write tests in a fluent, Gherkin-ish style\u2014tests that read like acceptance criteria but execute like unit tests. The goal is not ceremony, but clarity: a shared, human-parsable DSL that can span from domain rules to browser automation without losing intent.\u003C/p\u003E\n\u003Cp\u003EThis post is the practical follow-up to my earlier \u003Ca href=\u0022https://jerrettdavis.com/blog/posts/making-the-business-write-your-tests-with-bdd\u0022\u003Eessay on BDD\u003C/a\u003E. There, I dug into the \u003Cem\u003Ewhy\u003C/em\u003E. Here, we\u0027ll focus on the \u003Cem\u003Ehow\u003C/em\u003E: how to go from acceptance criteria to running tests in minutes, how to use Given/When/Then to model even the smallest units, how to orchestrate full end-to-end flows with Playwright, and how writing this way naturally nudges your architecture toward SOLID and composable design.\u003C/p\u003E\n\u003Chr /\u003E\n\u003Ch2 id=\u0022from-acceptance-criteria-to-running-tests-in-minutes\u0022\u003EFrom acceptance criteria to running tests in minutes\u003C/h2\u003E\n\u003Cp\u003EEvery team has seen a story like this written in Jira or Confluence:\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode\u003E\nScenario: Gold member gets free shipping\nGiven the customer is a \u0026quot;gold\u0026quot; member\nAnd they have a cart totaling $12.00\nWhen they checkout with standard shipping\nThen the shipping cost is $0.00\nAnd the order total is $12.00\n\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EWith TinyBDD, you don\u0027t need separate \u003Ccode\u003E.feature\u003C/code\u003E files unless you want them. You can capture that same intent directly in your test framework, keeping the semantics without the tooling overhead:\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-csharp\u0022\u003Eawait Given(\u0026quot;a gold customer with a $12 cart\u0026quot;, () =\u0026gt; new Cart(Customer.Gold(), 12.00m))\n     .When(\u0026quot;they choose standard shipping\u0026quot;, cart =\u0026gt; cart.Checkout(Shipping.Standard))\n     .Then(\u0026quot;shipping is free\u0026quot;, order =\u0026gt; order.ShippingTotal == 0.00m)             // Pass/Fail with booleans\n     .And(\u0026quot;order total is $12.00\u0026quot;, order =\u0026gt; Expect.For(order.Total).ToBe(12.00m)) // Or Assertions\n     .AssertPassed();\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EThe keywords map one-to-one with the business story. Each step is explicit and composable, and the whole chain is easy to read\u2014even for someone outside the dev team. Because the language matches what stakeholders already use, the test itself becomes a living contract.\u003C/p\u003E\n\u003Chr /\u003E\n\u003Ch2 id=\u0022unit-tests-that-read-like-behavior\u0022\u003EUnit tests that read like behavior\u003C/h2\u003E\n\u003Cp\u003EBehavior-driven style isn\u0027t just for top-level acceptance tests. It works equally well for small pieces of logic: a pure function, a discount rule, a transformer. By expressing them as Given/When/Then, you get readability\u2014tiny scenarios that explain the intent before diving into implementation detail\u2014and design pressure, because the format gently encourages pure, composable functions.\u003C/p\u003E\n\u003Cp\u003EExample: a simple discount calculation.\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-csharp\u0022\u003Eawait Given(\u0026quot;a silver customer with $100 cart\u0026quot;, () =\u0026gt; (Tier: \u0026quot;silver\u0026quot;, Amount: 100m))\n     .When(\u0026quot;discount is applied\u0026quot;, x =\u0026gt; Discounts.Apply(x.Tier, x.Amount))\n     .Then(\u0026quot;result is $95\u0026quot;, result =\u0026gt; result == 95m)\n     .AssertPassed();\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EEven at this scale, the benefits are obvious. You isolate the decision logic, assert outcomes in plain language, and end up with code that composes neatly into bigger flows later.\u003C/p\u003E\n\u003Chr /\u003E\n\u003Ch2 id=\u0022end-to-end-ui-tests-with-playwright\u0022\u003EEnd-to-end UI tests with Playwright\u003C/h2\u003E\n\u003Cp\u003ETinyBDD also works at the other end of the spectrum: full-stack, end-to-end tests. Here, the key is keeping steps thin and expressive while pushing implementation detail into helpers like page objects or service wrappers. That way, the scenario text stays stable even if the UI shifts underneath.\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-csharp\u0022\u003Eawait Given(\u0026quot;a new browser and logged-in gold user\u0026quot;, async () =\u0026gt;\n{\n    var pw = await PlaywrightFactory.LaunchAsync();\n    var page = await pw.NewPageAsync();\n    await AuthSteps.LoginAsGoldAsync(page);\n    return page;\n})\n.When(\u0026quot;user adds a $12 item to cart\u0026quot;, async page =\u0026gt;\n{\n    await CatalogSteps.AddItemAsync(page, \u0026quot;SKU-123\u0026quot;, 12.00m);\n    return page;\n})\n.And(\u0026quot;proceeds to checkout with standard shipping\u0026quot;, CheckoutSteps.StandardAsync)\n.Then(\u0026quot;shipping is free\u0026quot;, async page =\u0026gt;\n{\n    var shipping = await CartSteps.ReadShippingAsync(page);\n    return shipping == 0.00m;\n})\n.And(\u0026quot;order total is $12.00\u0026quot;, async page =\u0026gt;\n{\n    var total = await CartSteps.ReadTotalAsync(page);\n    return total == 12.00m;\n})\n.AssertPassed();\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EScenarios like this are readable enough for a stakeholder to skim, while still giving engineers the control they need under the hood. Stable wording, deterministic helpers, and tagging (\u003Ccode\u003Esmoke\u003C/code\u003E, \u003Ccode\u003Eui\u003C/code\u003E, \u003Ccode\u003Echeckout\u003C/code\u003E) all contribute to making suites like this maintainable in real CI pipelines.\u003C/p\u003E\n\u003Chr /\u003E\n\u003Ch3 id=\u0022patterns-that-keep-this-maintainable\u0022\u003EPatterns that keep this maintainable\u003C/h3\u003E\n\u003Cp\u003EThe trick to making end-to-end scenarios sustainable is resisting the temptation to let your steps do all the heavy lifting. The step chain should stay thin and intention-revealing, while the real mechanics live in helpers: page objects, domain services, or test utilities. This keeps the scenario text stable even as the implementation evolves. A good rule of thumb is that a non-technical stakeholder should be able to scan the steps and nod along without ever seeing the helper code. Deterministic helpers\u2014free from hidden global state\u2014are key to repeatable results. And once you have a handful of scenarios, you\u0027ll want to tag them (\u003Ccode\u003Esmoke\u003C/code\u003E, \u003Ccode\u003Eui\u003C/code\u003E, \u003Ccode\u003Echeckout\u003C/code\u003E, etc.) so that CI pipelines can run fast slices for quick feedback and broader sweeps when confidence matters most.\u003C/p\u003E\n\u003Chr /\u003E\n\u003Ch3 id=\u0022let-tests-guide-your-design\u0022\u003ELet tests guide your design\u003C/h3\u003E\n\u003Cp\u003EWhen you write tests in a behavior-first style, architectural friction surfaces quickly. A step that requires half a dozen parameters is rarely a coincidence\u2014it usually means your modules are too tightly coupled. Repeating the same tedious setup across multiple scenarios suggests the absence of a proper abstraction. And if you struggle to phrase a step cleanly, the problem may not be the test at all, but the clarity of your domain language.\u003C/p\u003E\n\u003Cp\u003EThese moments of friction are signals. Often, the fix is to extract a pure function from a messy edge, create a port or adapter to decouple infrastructure from business rules, or split a workflow into smaller seams that deserve their own scenarios. In other words: the pressure you feel in writing the test is your design telling you what it wants to become.\u003C/p\u003E\n\u003Chr /\u003E\n\u003Ch3 id=\u0022bdd-and-the-drift-toward-solid-and-functional-design\u0022\u003EBDD and the drift toward SOLID and functional design\u003C/h3\u003E\n\u003Cp\u003EConsistently writing scenarios has a shaping effect on code. Steps that do one clear thing align with the Single Responsibility Principle. The ability to add new scenarios without editing existing ones echoes the Open/Closed Principle. And abstractions that are narrow, well-defined, and swappable make substituting fakes and stubs trivial, pushing you toward Liskov, ISP, and DIP almost by default.\u003C/p\u003E\n\u003Cp\u003EThe same is true for functional composition. Pure functions naturally slide into Given/When/Then flows. Side effects are easiest to reason about when pushed to the edges\u2014fetching in a Given, transforming in a When, and observing in a Then. And when steps are small and named, they read like a pipeline instead of a mess of conditionals. By following the test style, you often find yourself following the design style too.\u003C/p\u003E\n\u003Chr /\u003E\n\u003Ch3 id=\u0022a-practical-way-to-start-tomorrow\u0022\u003EA practical way to start tomorrow\u003C/h3\u003E\n\u003Cp\u003EIf this feels overwhelming, don\u0027t boil the ocean. Start with one slice of functionality that everyone values and recognizes\u2014maybe a login path or a simple checkout. Write just two or three scenarios, and make sure the wording mirrors how the business describes the flow. Delegate the mechanics to helpers, not the scenario text. Keep your domain logic in pure functions wherever possible so it\u0027s trivial to call from a \u003Ccode\u003EWhen\u003C/code\u003E step. And once you\u0027ve got a couple of green runs, wire in some tags so you can choose between smoke tests, integration runs, or the full suite depending on your CI needs.\u003C/p\u003E\n\u003Cp\u003EAs you go, pay attention to the words. If step text feels clumsy, it probably means your ubiquitous language is clumsy too. Refining that wording in collaboration with stakeholders isn\u0027t overhead\u2014it\u0027s the work. And when naming friction crops up, it\u0027s often a smell that your design needs another seam or abstraction.\u003C/p\u003E\n\u003Chr /\u003E\n\u003Ch3 id=\u0022fluent-examples-you-can-copy-paste\u0022\u003EFluent examples you can copy-paste\u003C/h3\u003E\n\u003Cp\u003EUnit-level:\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-csharp\u0022\u003Eawait Given(\u0026quot;subtotal is $120 and tier is gold\u0026quot;, () =\u0026gt; (Subtotal: 120m, Tier: \u0026quot;gold\u0026quot;))\n     .When(\u0026quot;finalize price\u0026quot;, x =\u0026gt; Pricing.Finalize(x.Tier, x.Subtotal))\n     .Then(\u0026quot;applies 10% discount\u0026quot;, price =\u0026gt; price == 108m)\n     .AssertPassed();\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EAPI-level:\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-csharp\u0022\u003Eawait Given(\u0026quot;a seeded test tenant\u0026quot;, TestData.SeedTenantAsync)\n     .When(\u0026quot;posting to /invoices\u0026quot;, async _ =\u0026gt; await Api.PostAsync(\u0026quot;/invoices\u0026quot;, new { amount = 250 }))\n     .Then(\u0026quot;returns 201\u0026quot;, r =\u0026gt; r.StatusCode == 201)\n     .And(\u0026quot;body contains invoice id\u0026quot;, r =\u0026gt; r.Json.Value\u0026lt;string\u0026gt;(\u0026quot;id\u0026quot;) is { Length: \u0026gt; 0 })\n     .AssertPassed();\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EUI-level:\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-csharp\u0022\u003Eawait Given(\u0026quot;a logged-in admin\u0026quot;, BrowserSteps.LoginAsAdminAsync)\n     .When(\u0026quot;they create a user named Dana\u0026quot;, page =\u0026gt; AdminUsers.CreateAsync(page, \u0026quot;Dana\u0026quot;))\n     .Then(\u0026quot;Dana appears in the grid\u0026quot;, page =\u0026gt; AdminUsers.ExistsAsync(page, \u0026quot;Dana\u0026quot;))\n     .AssertPassed();\n\u003C/code\u003E\u003C/pre\u003E\n\u003Chr /\u003E\n\u003Ch3 id=\u0022readable-gherkin-style-output\u0022\u003EReadable Gherkin-style output\u003C/h3\u003E\n\u003Cp\u003EOne of the nicest touches in TinyBDD is how your scenarios \u003Cem\u003Ereport themselves\u003C/em\u003E when you run the tests. Pair your scenarios with the appropriate base class (\u003Ccode\u003ETinyBddXunitBase\u003C/code\u003E, \u003Ccode\u003ETinyBddXunitV3Base\u003C/code\u003E, \u003Ccode\u003ETinyBddNUnitBase\u003C/code\u003E, or \u003Ccode\u003ETinyBddMSTestBase\u003C/code\u003E), and the test runner will print structured Gherkin output alongside normal results.\u003C/p\u003E\n\u003Cp\u003EThat means the Given/When/Then flow you wrote doesn\u0027t just execute\u2014it shows up exactly as you\u0027d expect, step by step, with timings and pass/fail indicators. It turns your test logs into living specifications.\u003C/p\u003E\n\u003Cp\u003EFor example, here\u0027s the output from a mediator scenario:\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode\u003E\nFeature: Behavioral - Mediator (commands, notifications, streaming, behaviors)\nScenario: Send: command handler runs through behaviors and returns value\nGiven a mediator with pre/post/whole behaviors and a Ping-\u0026gt;Pong handler [OK] 2 ms\nWhen sending Ping(5) [OK] 4 ms\nThen result is pong:5 [OK] 2 ms\nAnd behaviors logged pre, whole before/after, and post [OK] 0 ms\n\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EInstead of squinting at assertions in code, you see a natural-language story of what happened. That\u0027s invaluable when sharing results with stakeholders or debugging failures in CI. And because the feature and scenario titles come from your test class and attributes, the logs stay consistent with the language you use in code reviews, planning, and conversations.\u003C/p\u003E\n\u003Chr /\u003E\n\u003Ch3 id=\u0022avoiding-common-anti-patterns\u0022\u003EAvoiding common anti-patterns\u003C/h3\u003E\n\u003Cp\u003EEvery test framework accumulates bad habits if left unchecked, and TinyBDD is no exception. The most obvious trap is clever wording: steps like \u0026quot;When magic happens\u0026quot; don\u0027t help anyone and fail to serve as documentation. Instead, the wording should describe an intention that a stakeholder would immediately recognize, such as \u0026quot;When the admin disables the account\u0026quot;. Another trap is letting a single step conceal multiple actions or checks. Keep your flow honest: \u003Ccode\u003EWhen\u003C/code\u003E should drive effects, and \u003Ccode\u003EThen\u003C/code\u003E or \u003Ccode\u003EAnd\u003C/code\u003E should assert results.\u003C/p\u003E\n\u003Cp\u003ESetup is another danger zone. If your \u003Ccode\u003EGiven\u003C/code\u003E steps are littered with manual wiring of objects, it\u0027s time for factories or builders to take over. And at the UI layer, brittle selectors quickly make tests flaky; encapsulating them in page objects and using explicit test IDs pays off many times over. Avoiding these pitfalls keeps your suite readable, stable, and genuinely valuable.\u003C/p\u003E\n\u003Chr /\u003E\n\u003Ch3 id=\u0022the-payoff\u0022\u003EThe payoff\u003C/h3\u003E\n\u003Cp\u003EWhen scenarios read like the business and execute like the code, something special happens. Your tests stop being just a safety net and start becoming living documentation. They never go stale because they\u0027re executable. They provide immediate feedback on drift, so change becomes safer. They subtly nudge your codebase toward SOLID principles and functional seams. And for new developers, they become the best possible onboarding guide: open the suite, read the stories, and understand how the system behaves.\u003C/p\u003E\n\u003Cp\u003EYou don\u0027t need to retool your world to reach this point. Start with one scenario. Make it pass. Share it with your team. Repeat. In a few sprints, you\u0027ll have a suite of stories that stack from units to workflows, and a codebase that\u0027s easier to evolve because the behaviors are crystal clear.\u003C/p\u003E\n\u003Chr /\u003E\n\u003Ch3 id=\u0022appendix-a-quick-pr-lens\u0022\u003EAppendix \u2014 A quick PR lens\u003C/h3\u003E\n\u003Cp\u003EAs you review changes, ask yourself: does this PR add or update scenarios that the business would recognize? Do the steps read like natural English, each mapping to a single intent? Are the domain rules isolated in pure functions rather than tangled in infrastructure? Did we create or clarify a port instead of hard-coding dependencies? Can we tag and run this slice of scenarios independently in CI?\u003C/p\u003E\n\u003Cp\u003EIf you can answer \u0026quot;yes\u0026quot; to most of those, you\u0027re not just writing tests\u2014you\u0027re building shared understanding, guiding design, and accelerating delivery. That\u0027s the real promise of TinyBDD.\u003C/p\u003E\n\u003Chr /\u003E\n\u003Cp\u003E\uD83D\uDC49 \u003Ca href=\u0022https://github.com/jerrettdavis/tinybdd\u0022\u003EGet TinyBDD on GitHub\u003C/a\u003E \u00B7 \u003Ca href=\u0022https://www.nuget.org/packages/TinyBDD\u0022\u003ENuGet\u003C/a\u003E \u00B7 \u003Ca href=\u0022https://jerrettdavis.github.io/TinyBDD/\u0022\u003EDocs\u003C/a\u003E\u003C/p\u003E\n",
    "stub": "Turn acceptance criteria into executable tests in minutes, use a fluent Gherkin-ish syntax for unit and integration testing, and let your tests shape a cleaner architecture.",
    "wordCount": 512,
    "useToc": false,
    "source": "syndicated",
    "canonicalUrl": "https://jerrettdavis.com/blog/tinybdd"
  },
  {
    "id": "making-the-business-write-your-tests-with-bdd",
    "title": "BDD: Make the Business Write Your Tests",
    "date": "2025-09-01T00:00:00",
    "description": "What if your business wrote the tests, and developers just made them pass?",
    "featured": null,
    "tags": [
      "coding",
      "programming",
      "bdd",
      "behavior-driven-development",
      "testing",
      "tests",
      "architecture"
    ],
    "categories": [
      "Programming",
      "Software Engineering",
      "Architecture"
    ],
    "series": null,
    "seriesOrder": null,
    "content": "What if the business wrote the tests, and developers just made them pass?  \n\nThat\u0027s the promise of behavior-driven development (BDD). Instead of developers guessing at requirements, chasing \nSlack threads, and interpreting vague Jira tickets, we let the people who know the *why* express it in a form \nthat\u0027s precise enough to execute.  \n\nThroughout my career, I\u0027ve worked with teams of all shapes and sizes, and one pattern is universal:  \nnobody *loves* writing tests. Most developers grudgingly agree they\u0027re important, but tests are often seen \nas a tax\u2014time spent writing code that doesn\u0027t \u0022ship features.\u0022  \n\nThe result is predictable: coverage gaps, brittle suites, and requirements that live in Confluence but \nnever make it into code. Testing becomes a burden instead of a superpower.\n\nBDD flips that dynamic. Instead of treating tests as a chore, it turns them into a shared language between \ndevelopers, QA, and the business. Suddenly, everyone is speaking the same language about features and \nscenarios, and tests become the living documentation of what the system is supposed to do.  \n\nBefore we dive into how this works, let\u0027s establish a few \u0022Laws of Testing.\u0022 These aren\u0027t divine truths, \nbut they\u0027re a solid starting point\u2014guidelines that help ensure tests actually drive design instead of \njust describing what already exists.\n\n### The Laws of Testing\n\n1. No application code will be written until tests are defined.  \n2. Systems should be tested as they are meant to be used.  \n3. Automate tests to the maximum extent technically and financially feasible.  \n4. Every business requirement must have a corresponding test. If the requirement changes, the test must change.  \n5. Any code not covered by realistic, automated tests should be treated as magic\u2014and magic should not be trusted.  \n\nNotice that I\u0027m not prescribing a specific tool, framework, or level of granularity. These laws simply give you \na pragmatic foundation to make testing a first-class part of building software, not an afterthought.  \n\n## Behavior-Driven Development\n\nNow for the fun part: putting this into practice.  \n\nBehavior-Driven Development (*BDD*) is designed to bridge the gap between business stakeholders, users, developers, \nQA, and everyone else involved in a project. There are many flavors of BDD, but at their core, they all focus on telling \ncohesive, testable stories using a shared Domain-Specific Language (DSL).  \n\nThere\u0027s a long-standing joke in IT\u2014still true even in the post-ChatGPT era\u2014that it\u0027s easier to teach a subject-matter \nexpert to code than it is to get them to clearly articulate what they actually want built.  \n\nOver the years, countless tools have promised to help non-technical folks bring their ideas to life, but theory and \npractice rarely align. *Everything works in theory.* We can keep building better tools to bridge the gap between \ndecision-makers and developers, but no tool can prevent the inevitable: **requirements change**.  \n\nBusiness needs evolve constantly. Even in a perfectly stable market, a company\u0027s technology stack would still have to \nadapt\u2014whether to security updates, API deprecations, or new compliance requirements. That\u0027s why business and technology \nteams need a living contract: a shared, human-readable specification of what\u0027s supposed to exist, expressed in language \nthat everyone can understand.  \n\nBDD provides exactly that. It takes the \u0022what\u0022 from the business and turns it into executable specifications for developers, \nensuring the system stays aligned with today\u0027s needs\u2014not just the assumptions written down last quarter.\n\n## What BDD Actually Is\n\nBehavior-Driven Development (BDD) is essentially turning your acceptance criteria into executable code.  \n\nIf your company already writes:\n- Acceptance Criteria in Jira or Azure DevOps\n- Business Requirement Documents (BRDs) in Word or Confluence\n- Test Scripts for QA teams to follow manually\n\n\u2026you\u0027re already halfway there. BDD simply takes those artifacts, expresses them in a precise, human-readable format, \nand wires them into your test suite so they can be run automatically.  \n\nInstead of existing only as text that humans must interpret, your requirements become living specifications that \nare always in sync with what the system actually does. If the system drifts, the tests fail \u2014 letting you catch gaps \nbefore production users do.\n\nKey ideas:\n\n- **Ubiquitous language**: agree on domain terms and reuse them in requirements, tests, and code.  \n- **Outside-in**: start from observable outcomes (what users and the business care about) and let those drive your implementation.  \n- **Executable documentation**: your feature files become a source of truth that stays up to date because failing tests \n  force the team to update them when the business changes its mind.\n\n## Why It Clicks with the Business\n\nBDD speaks the same language the business already uses.  \n\nInstead of:  \n\u003E \u0022Verify that gold members receive free shipping on orders over $10.\u0022 \n\nBuried in a PDF or Confluence page, we can write:\n\n\u0060\u0060\u0060gherkin\nScenario: Gold member gets free shipping\n  Given the customer is a \u0022gold\u0022 member\n  And they have a cart totaling $12.00\n  When they checkout with standard shipping\n  Then the shipping cost is $0.00\n  And the order total is $12.00\n\u0060\u0060\u0060\n\nStakeholders can read and confirm this just like they\u0027d review an acceptance test \u2014 but now this doubles as a\nmachine-executable test that can be run in CI.\n\n## How to Start Without Overhauling Everything\n\nYou don\u0027t need to replace all your documentation overnight. Try this:\n\n1. Take an existing acceptance criterion for a small feature.\n2. Rewrite it as a Gherkin scenario.\n3. Wire up just enough step definitions to make it run.\n4. Let it fail (red).\n5. Write or adjust code until it passes (green).\n6. Refactor code *and* scenario until both are clear and maintainable.\n\nThat\u0027s it \u2014 you\u0027ve just done outside-in development. Over time, you can replace more of your static BRDs and\ntest scripts with living specs that run automatically and stay current.\n\n---\n\nBy framing BDD as an evolution of what teams already do, it becomes less intimidating: you\u0027re not adding \u0022one more thing,\u0022\nyou\u0027re making what you already write executable, consistent, and always up to date.\n\n## Starting from Existing Code (The Developer \u002B QA Perspective)\n\nOf course, not every team has pristine requirements or well-maintained acceptance criteria.  \nSometimes the *only* source of truth is the code itself, or a set of outdated manual test scripts in a shared folder.  \nThat\u0027s okay \u2014 you can still apply BDD principles to what you already have.  \n\n### Step 1: Surface What the Code Already Does\n\nStart by exploring the system from the outside in, and not by reading the code first.  \n\n- Walk through the application like a user would: click buttons, submit forms, run API calls.  \n- Write down what you observe in plain language, almost as if you were writing a support guide.  \n- Identify key flows: logins, purchases, data exports, admin tasks, error handling.  \n\nYou\u0027re essentially reverse-engineering the behavior that already exists. The goal isn\u0027t to perfectly model the internals, but to describe what happens when X occurs.\n\n### Step 2: Capture Behavior as Scenarios\n\nTurn those observations into scenarios, even if they\u0027re manual at first.  \n\n\u0060\u0060\u0060gherkin\nScenario: User can reset password\n  Given I am on the login page\n  When I click \u0022Forgot password\u0022\n  And I submit my email address\n  Then I receive a password reset link by email\n  And I can set a new password\n\u0060\u0060\u0060\u0060\n\nThis becomes your backfill documentation and a checklist you can use to validate future changes.\n\n### Step 3: Choose a Testing Library\n\nOnce you have a handful of scenarios, pick a suitable tool that matches your tech stack.\nIt doesn\u0027t have to be fancy \u2014 choose something that can:\n\n* Parse Gherkin or similar DSL ([Cucumber](https://cucumber.io/docs/installation/), SpecFlow/[Reqnroll](https://reqnroll.net/), [Behave](https://behave.readthedocs.io/en/latest/), [TinyBDD](https://github.com/JerrettDavis/TinyBDD) \uD83D\uDE09, etc.)\n* Run in your CI/CD pipeline\n* Integrate with the type of app you have (UI automation, API tests, service-level tests)\n\nDon\u0027t over-engineer at this stage. The goal is simply to make a scenario run end-to-end.\n\n### Step 4: Build Thin Step Definitions\n\nWhen you wire up steps to code, keep the step definitions thin and reusable:\n\n* Treat them like glue code \u2014 they orchestrate, not implement.\n* Push logic into abstractions (page objects, API clients, domain helpers).\n* Keep language aligned with the business terms you captured earlier.\n\nThis separation makes your steps easy to read and your automation maintainable even as your app changes.\n\n### Step 5: Automate the Most Valuable Flows First\n\nDon\u0027t try to automate everything on day one. Pick a few high-value, low-volatility flows:\n\n* Happy-path checkouts\n* Core login and authentication\n* Critical reporting or data pipelines\n\nStart small, get them running reliably in CI, and expand gradually. The point is not to hit 100% coverage overnight, but to start to \ngain comfort and momentum in writing BDD tests end-to-end.\n---\n\nBy working backward from the app\u0027s behavior, you create a bridge between what the code does and what the business expects \neven when no documentation exists. Over time, your automated scenarios become the new source of truth, letting developers and QA refactor or ship new features with confidence.\n\n\n## Bringing It All Together\n\nWhether you\u0027re starting from crisp acceptance criteria or working backwards from a codebase that only lives in developers\u0027 heads, \nthe goal of BDD is the same: **close the gap between what the business needs and what the code does**.\n\nYou don\u0027t have to overhaul your entire testing strategy in a single sprint. You just have to start:\n\n- Write one scenario.\n- Get it running end-to-end.\n- Share it with your team.\n- Keep going.\n\nOver time, you\u0027ll build up a living specification that grows with the system, catching regressions early and making onboarding new developers dramatically easier.  \n\n### What a Mature BDD Practice Looks Like\n\nA well-adopted BDD process creates a feedback loop that keeps everyone aligned:\n\n- **Business \u0026 product teams** write or review scenarios as part of refinement.  \n- **Developers** implement features by making those scenarios pass.  \n- **QA** contributes new scenarios for edge cases and validates existing ones stay green.  \n- **CI/CD pipelines** run the whole suite automatically, so everyone knows the current state of the system at a glance.  \n\nWhen this loop is healthy, you get a shared understanding of what \u0022done\u0022 really means, and confidence that your software still works tomorrow, next quarter, and next year.\n\n### Final Thoughts\n\nBDD isn\u0027t a silver bullet, but it *is* a forcing function for clearer requirements, more reliable software, and tighter collaboration across teams.  \n\nIf you\u0027ve ever wished the business could \u0022just write the tests,\u0022 BDD is the closest thing we have to that dream. Start small, \nstay consistent, and watch as those scenarios turn into a living, breathing specification that guides your development for years to come.",
    "contentHtml": "\u003Cp\u003EWhat if the business wrote the tests, and developers just made them pass?\u003C/p\u003E\n\u003Cp\u003EThat\u0027s the promise of behavior-driven development (BDD). Instead of developers guessing at requirements, chasing\nSlack threads, and interpreting vague Jira tickets, we let the people who know the \u003Cem\u003Ewhy\u003C/em\u003E express it in a form\nthat\u0027s precise enough to execute.\u003C/p\u003E\n\u003Cp\u003EThroughout my career, I\u0027ve worked with teams of all shapes and sizes, and one pattern is universal:\u003Cbr /\u003E\nnobody \u003Cem\u003Eloves\u003C/em\u003E writing tests. Most developers grudgingly agree they\u0027re important, but tests are often seen\nas a tax\u2014time spent writing code that doesn\u0027t \u0026quot;ship features.\u0026quot;\u003C/p\u003E\n\u003Cp\u003EThe result is predictable: coverage gaps, brittle suites, and requirements that live in Confluence but\nnever make it into code. Testing becomes a burden instead of a superpower.\u003C/p\u003E\n\u003Cp\u003EBDD flips that dynamic. Instead of treating tests as a chore, it turns them into a shared language between\ndevelopers, QA, and the business. Suddenly, everyone is speaking the same language about features and\nscenarios, and tests become the living documentation of what the system is supposed to do.\u003C/p\u003E\n\u003Cp\u003EBefore we dive into how this works, let\u0027s establish a few \u0026quot;Laws of Testing.\u0026quot; These aren\u0027t divine truths,\nbut they\u0027re a solid starting point\u2014guidelines that help ensure tests actually drive design instead of\njust describing what already exists.\u003C/p\u003E\n\u003Ch3 id=\u0022the-laws-of-testing\u0022\u003EThe Laws of Testing\u003C/h3\u003E\n\u003Col\u003E\n\u003Cli\u003ENo application code will be written until tests are defined.\u003C/li\u003E\n\u003Cli\u003ESystems should be tested as they are meant to be used.\u003C/li\u003E\n\u003Cli\u003EAutomate tests to the maximum extent technically and financially feasible.\u003C/li\u003E\n\u003Cli\u003EEvery business requirement must have a corresponding test. If the requirement changes, the test must change.\u003C/li\u003E\n\u003Cli\u003EAny code not covered by realistic, automated tests should be treated as magic\u2014and magic should not be trusted.\u003C/li\u003E\n\u003C/ol\u003E\n\u003Cp\u003ENotice that I\u0027m not prescribing a specific tool, framework, or level of granularity. These laws simply give you\na pragmatic foundation to make testing a first-class part of building software, not an afterthought.\u003C/p\u003E\n\u003Ch2 id=\u0022behavior-driven-development\u0022\u003EBehavior-Driven Development\u003C/h2\u003E\n\u003Cp\u003ENow for the fun part: putting this into practice.\u003C/p\u003E\n\u003Cp\u003EBehavior-Driven Development (\u003Cem\u003EBDD\u003C/em\u003E) is designed to bridge the gap between business stakeholders, users, developers,\nQA, and everyone else involved in a project. There are many flavors of BDD, but at their core, they all focus on telling\ncohesive, testable stories using a shared Domain-Specific Language (DSL).\u003C/p\u003E\n\u003Cp\u003EThere\u0027s a long-standing joke in IT\u2014still true even in the post-ChatGPT era\u2014that it\u0027s easier to teach a subject-matter\nexpert to code than it is to get them to clearly articulate what they actually want built.\u003C/p\u003E\n\u003Cp\u003EOver the years, countless tools have promised to help non-technical folks bring their ideas to life, but theory and\npractice rarely align. \u003Cem\u003EEverything works in theory.\u003C/em\u003E We can keep building better tools to bridge the gap between\ndecision-makers and developers, but no tool can prevent the inevitable: \u003Cstrong\u003Erequirements change\u003C/strong\u003E.\u003C/p\u003E\n\u003Cp\u003EBusiness needs evolve constantly. Even in a perfectly stable market, a company\u0027s technology stack would still have to\nadapt\u2014whether to security updates, API deprecations, or new compliance requirements. That\u0027s why business and technology\nteams need a living contract: a shared, human-readable specification of what\u0027s supposed to exist, expressed in language\nthat everyone can understand.\u003C/p\u003E\n\u003Cp\u003EBDD provides exactly that. It takes the \u0026quot;what\u0026quot; from the business and turns it into executable specifications for developers,\nensuring the system stays aligned with today\u0027s needs\u2014not just the assumptions written down last quarter.\u003C/p\u003E\n\u003Ch2 id=\u0022what-bdd-actually-is\u0022\u003EWhat BDD Actually Is\u003C/h2\u003E\n\u003Cp\u003EBehavior-Driven Development (BDD) is essentially turning your acceptance criteria into executable code.\u003C/p\u003E\n\u003Cp\u003EIf your company already writes:\u003C/p\u003E\n\u003Cul\u003E\n\u003Cli\u003EAcceptance Criteria in Jira or Azure DevOps\u003C/li\u003E\n\u003Cli\u003EBusiness Requirement Documents (BRDs) in Word or Confluence\u003C/li\u003E\n\u003Cli\u003ETest Scripts for QA teams to follow manually\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cp\u003E\u2026you\u0027re already halfway there. BDD simply takes those artifacts, expresses them in a precise, human-readable format,\nand wires them into your test suite so they can be run automatically.\u003C/p\u003E\n\u003Cp\u003EInstead of existing only as text that humans must interpret, your requirements become living specifications that\nare always in sync with what the system actually does. If the system drifts, the tests fail \u2014 letting you catch gaps\nbefore production users do.\u003C/p\u003E\n\u003Cp\u003EKey ideas:\u003C/p\u003E\n\u003Cul\u003E\n\u003Cli\u003E\u003Cstrong\u003EUbiquitous language\u003C/strong\u003E: agree on domain terms and reuse them in requirements, tests, and code.\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003EOutside-in\u003C/strong\u003E: start from observable outcomes (what users and the business care about) and let those drive your implementation.\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003EExecutable documentation\u003C/strong\u003E: your feature files become a source of truth that stays up to date because failing tests\nforce the team to update them when the business changes its mind.\u003C/li\u003E\n\u003C/ul\u003E\n\u003Ch2 id=\u0022why-it-clicks-with-the-business\u0022\u003EWhy It Clicks with the Business\u003C/h2\u003E\n\u003Cp\u003EBDD speaks the same language the business already uses.\u003C/p\u003E\n\u003Cp\u003EInstead of:\u003C/p\u003E\n\u003Cblockquote\u003E\n\u003Cp\u003E\u0026quot;Verify that gold members receive free shipping on orders over $10.\u0026quot;\u003C/p\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003EBuried in a PDF or Confluence page, we can write:\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-gherkin\u0022\u003EScenario: Gold member gets free shipping\n  Given the customer is a \u0026quot;gold\u0026quot; member\n  And they have a cart totaling $12.00\n  When they checkout with standard shipping\n  Then the shipping cost is $0.00\n  And the order total is $12.00\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EStakeholders can read and confirm this just like they\u0027d review an acceptance test \u2014 but now this doubles as a\nmachine-executable test that can be run in CI.\u003C/p\u003E\n\u003Ch2 id=\u0022how-to-start-without-overhauling-everything\u0022\u003EHow to Start Without Overhauling Everything\u003C/h2\u003E\n\u003Cp\u003EYou don\u0027t need to replace all your documentation overnight. Try this:\u003C/p\u003E\n\u003Col\u003E\n\u003Cli\u003ETake an existing acceptance criterion for a small feature.\u003C/li\u003E\n\u003Cli\u003ERewrite it as a Gherkin scenario.\u003C/li\u003E\n\u003Cli\u003EWire up just enough step definitions to make it run.\u003C/li\u003E\n\u003Cli\u003ELet it fail (red).\u003C/li\u003E\n\u003Cli\u003EWrite or adjust code until it passes (green).\u003C/li\u003E\n\u003Cli\u003ERefactor code \u003Cem\u003Eand\u003C/em\u003E scenario until both are clear and maintainable.\u003C/li\u003E\n\u003C/ol\u003E\n\u003Cp\u003EThat\u0027s it \u2014 you\u0027ve just done outside-in development. Over time, you can replace more of your static BRDs and\ntest scripts with living specs that run automatically and stay current.\u003C/p\u003E\n\u003Chr /\u003E\n\u003Cp\u003EBy framing BDD as an evolution of what teams already do, it becomes less intimidating: you\u0027re not adding \u0026quot;one more thing,\u0026quot;\nyou\u0027re making what you already write executable, consistent, and always up to date.\u003C/p\u003E\n\u003Ch2 id=\u0022starting-from-existing-code-the-developer-qa-perspective\u0022\u003EStarting from Existing Code (The Developer \u002B QA Perspective)\u003C/h2\u003E\n\u003Cp\u003EOf course, not every team has pristine requirements or well-maintained acceptance criteria.\u003Cbr /\u003E\nSometimes the \u003Cem\u003Eonly\u003C/em\u003E source of truth is the code itself, or a set of outdated manual test scripts in a shared folder.\u003Cbr /\u003E\nThat\u0027s okay \u2014 you can still apply BDD principles to what you already have.\u003C/p\u003E\n\u003Ch3 id=\u0022step-1-surface-what-the-code-already-does\u0022\u003EStep 1: Surface What the Code Already Does\u003C/h3\u003E\n\u003Cp\u003EStart by exploring the system from the outside in, and not by reading the code first.\u003C/p\u003E\n\u003Cul\u003E\n\u003Cli\u003EWalk through the application like a user would: click buttons, submit forms, run API calls.\u003C/li\u003E\n\u003Cli\u003EWrite down what you observe in plain language, almost as if you were writing a support guide.\u003C/li\u003E\n\u003Cli\u003EIdentify key flows: logins, purchases, data exports, admin tasks, error handling.\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cp\u003EYou\u0027re essentially reverse-engineering the behavior that already exists. The goal isn\u0027t to perfectly model the internals, but to describe what happens when X occurs.\u003C/p\u003E\n\u003Ch3 id=\u0022step-2-capture-behavior-as-scenarios\u0022\u003EStep 2: Capture Behavior as Scenarios\u003C/h3\u003E\n\u003Cp\u003ETurn those observations into scenarios, even if they\u0027re manual at first.\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-gherkin\u0022\u003EScenario: User can reset password\n  Given I am on the login page\n  When I click \u0026quot;Forgot password\u0026quot;\n  And I submit my email address\n  Then I receive a password reset link by email\n  And I can set a new password\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EThis becomes your backfill documentation and a checklist you can use to validate future changes.\u003C/p\u003E\n\u003Ch3 id=\u0022step-3-choose-a-testing-library\u0022\u003EStep 3: Choose a Testing Library\u003C/h3\u003E\n\u003Cp\u003EOnce you have a handful of scenarios, pick a suitable tool that matches your tech stack.\nIt doesn\u0027t have to be fancy \u2014 choose something that can:\u003C/p\u003E\n\u003Cul\u003E\n\u003Cli\u003EParse Gherkin or similar DSL (\u003Ca href=\u0022https://cucumber.io/docs/installation/\u0022\u003ECucumber\u003C/a\u003E, SpecFlow/\u003Ca href=\u0022https://reqnroll.net/\u0022\u003EReqnroll\u003C/a\u003E, \u003Ca href=\u0022https://behave.readthedocs.io/en/latest/\u0022\u003EBehave\u003C/a\u003E, \u003Ca href=\u0022https://github.com/JerrettDavis/TinyBDD\u0022\u003ETinyBDD\u003C/a\u003E \uD83D\uDE09, etc.)\u003C/li\u003E\n\u003Cli\u003ERun in your CI/CD pipeline\u003C/li\u003E\n\u003Cli\u003EIntegrate with the type of app you have (UI automation, API tests, service-level tests)\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cp\u003EDon\u0027t over-engineer at this stage. The goal is simply to make a scenario run end-to-end.\u003C/p\u003E\n\u003Ch3 id=\u0022step-4-build-thin-step-definitions\u0022\u003EStep 4: Build Thin Step Definitions\u003C/h3\u003E\n\u003Cp\u003EWhen you wire up steps to code, keep the step definitions thin and reusable:\u003C/p\u003E\n\u003Cul\u003E\n\u003Cli\u003ETreat them like glue code \u2014 they orchestrate, not implement.\u003C/li\u003E\n\u003Cli\u003EPush logic into abstractions (page objects, API clients, domain helpers).\u003C/li\u003E\n\u003Cli\u003EKeep language aligned with the business terms you captured earlier.\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cp\u003EThis separation makes your steps easy to read and your automation maintainable even as your app changes.\u003C/p\u003E\n\u003Ch3 id=\u0022step-5-automate-the-most-valuable-flows-first\u0022\u003EStep 5: Automate the Most Valuable Flows First\u003C/h3\u003E\n\u003Cp\u003EDon\u0027t try to automate everything on day one. Pick a few high-value, low-volatility flows:\u003C/p\u003E\n\u003Cul\u003E\n\u003Cli\u003EHappy-path checkouts\u003C/li\u003E\n\u003Cli\u003ECore login and authentication\u003C/li\u003E\n\u003Cli\u003ECritical reporting or data pipelines\u003C/li\u003E\n\u003C/ul\u003E\n\u003Ch2 id=\u0022start-small-get-them-running-reliably-in-ci-and-expand-gradually.the-point-is-not-to-hit-100-coverage-overnight-but-to-start-to-gain-comfort-and-momentum-in-writing-bdd-tests-end-to-end\u0022\u003EStart small, get them running reliably in CI, and expand gradually. The point is not to hit 100% coverage overnight, but to start to\ngain comfort and momentum in writing BDD tests end-to-end.\u003C/h2\u003E\n\u003Cp\u003EBy working backward from the app\u0027s behavior, you create a bridge between what the code does and what the business expects\neven when no documentation exists. Over time, your automated scenarios become the new source of truth, letting developers and QA refactor or ship new features with confidence.\u003C/p\u003E\n\u003Ch2 id=\u0022bringing-it-all-together\u0022\u003EBringing It All Together\u003C/h2\u003E\n\u003Cp\u003EWhether you\u0027re starting from crisp acceptance criteria or working backwards from a codebase that only lives in developers\u0027 heads,\nthe goal of BDD is the same: \u003Cstrong\u003Eclose the gap between what the business needs and what the code does\u003C/strong\u003E.\u003C/p\u003E\n\u003Cp\u003EYou don\u0027t have to overhaul your entire testing strategy in a single sprint. You just have to start:\u003C/p\u003E\n\u003Cul\u003E\n\u003Cli\u003EWrite one scenario.\u003C/li\u003E\n\u003Cli\u003EGet it running end-to-end.\u003C/li\u003E\n\u003Cli\u003EShare it with your team.\u003C/li\u003E\n\u003Cli\u003EKeep going.\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cp\u003EOver time, you\u0027ll build up a living specification that grows with the system, catching regressions early and making onboarding new developers dramatically easier.\u003C/p\u003E\n\u003Ch3 id=\u0022what-a-mature-bdd-practice-looks-like\u0022\u003EWhat a Mature BDD Practice Looks Like\u003C/h3\u003E\n\u003Cp\u003EA well-adopted BDD process creates a feedback loop that keeps everyone aligned:\u003C/p\u003E\n\u003Cul\u003E\n\u003Cli\u003E\u003Cstrong\u003EBusiness \u0026amp; product teams\u003C/strong\u003E write or review scenarios as part of refinement.\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003EDevelopers\u003C/strong\u003E implement features by making those scenarios pass.\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003EQA\u003C/strong\u003E contributes new scenarios for edge cases and validates existing ones stay green.\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003ECI/CD pipelines\u003C/strong\u003E run the whole suite automatically, so everyone knows the current state of the system at a glance.\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cp\u003EWhen this loop is healthy, you get a shared understanding of what \u0026quot;done\u0026quot; really means, and confidence that your software still works tomorrow, next quarter, and next year.\u003C/p\u003E\n\u003Ch3 id=\u0022final-thoughts\u0022\u003EFinal Thoughts\u003C/h3\u003E\n\u003Cp\u003EBDD isn\u0027t a silver bullet, but it \u003Cem\u003Eis\u003C/em\u003E a forcing function for clearer requirements, more reliable software, and tighter collaboration across teams.\u003C/p\u003E\n\u003Cp\u003EIf you\u0027ve ever wished the business could \u0026quot;just write the tests,\u0026quot; BDD is the closest thing we have to that dream. Start small,\nstay consistent, and watch as those scenarios turn into a living, breathing specification that guides your development for years to come.\u003C/p\u003E\n",
    "stub": "What if your business wrote the tests, and developers just made them pass?",
    "wordCount": 1646,
    "useToc": false,
    "source": "syndicated",
    "canonicalUrl": "https://jerrettdavis.com/blog/making-the-business-write-your-tests-with-bdd"
  },
  {
    "id": "what-is-code",
    "title": "What is \u0022Code\u0022?",
    "date": "2025-08-11T00:00:00",
    "description": "A beginner-friendly exploration of what \u0022code\u0022 really is \u2014 from its many forms and history to why it matters today. We\u0027ll look at how programming languages evolved, how code shapes the world around us, and why you don\u0027t need to be a professional to start creating with it.",
    "featured": null,
    "tags": [
      "coding",
      "programming"
    ],
    "categories": [
      "Programming",
      "Software Engineering"
    ],
    "series": null,
    "seriesOrder": null,
    "content": "Technology is a wonderful thing. Humans, and perhaps even our direct ancestors, have been employing it for millennia. From stone tools to quantum computers, humans can\u0027t seem to resist tinkering with whatever the universe gives them.\n\nAnd with even our most remote, uncontacted relatives still in possession of weapons and shelter, it\u0027s safe to say every person on this planet experiences technology from birth to their final days. Right now, as you read this, you\u0027re surrounded by, and literally in contact with, an almost uncountable web of human inventions.\n\nTechnology shapes our lives, and in turn, we shape its future. It can be used for feats of great creation or acts of horrific destruction. Among all its branches, one of the most powerful and most accessible is **code**.\n\nYou don\u0027t need a factory, a lab, or a workshop to work with code. All it takes is a computer and a spark of curiosity. If you\u0027ve ever stacked LEGO bricks into something from your imagination, built a spreadsheet formula to save hours of work, set a coffee maker to brew before you wake, or fixed something in a pinch with whatever was at hand, you\u0027ve tapped into the same problem-solving instinct that drives programming.\n\nAt its heart, coding is the act of taking what you have and arranging it into something that does what you want. It\u0027s imagination with rules, creativity with feedback. And once you\u0027ve seen something you\u0027ve built come alive, whether it\u0027s a blinking LED, a tool that saves someone\u0027s day, or an app that makes a stranger smile, it\u0027s hard not to want to build more.\n\n---\n\n## What is \u0022Code\u0022?\n\nCode comes in many flavors. There are programming languages like C#, Assembly, C\u002B\u002B, Rust, Python, and countless others, each with its own syntax, style, and purpose. There\u0027s **G-code**, the set of instructions understood by CNC mills, 3D printers, lasers, and other computer-controlled machines. Even music notation is a kind of code, a written sequence of symbols that, when interpreted, produces something meaningful.\n\nThe \u0022code\u0022 most familiar to all of us is **human language**. Spoken or written, it can form relationships, transfer knowledge, and inspire change. But language is inherently ambiguous, which makes it ill-suited for telling computers exactly what to do. That\u0027s why programming languages exist: precise dialects designed for humans to read and for machines to execute without hesitation or confusion.\n\nPut simply:\n\n\u003E **Code is a system of signals, symbols, letters, words, or other constructs used to convey a message.**\n\nAt the very bottom of every modern computer lies **binary** \u2014 two values, usually 1 and 0, representing \u0022on\u0022 and \u0022off\u0022 or \u0022true\u0022 and \u0022false.\u0022 Working directly in binary is tedious, so we created layers of abstraction to make our lives easier.\n\nA step above binary is **assembly language**. Assembly comes in many dialects, each tied to the instruction set of a specific processor. It\u0027s still \u0022close to the metal\u0022 but far more readable than raw ones and zeros.\n\n---\n\n### A (Somewhat Accurate and Still Overly Dramatic) History of Programming Languages\n\nThe history of programming languages is a tapestry of invention, frustration, and the occasional all-nighter. Imagine it told as a fantasy epic.\n\nFirst came **Binary**, and from Binary were forged all other tongues of the machine. But the work was slow, and the scribes of silicon longed for an easier way to command their creations.\n\nIn the early 1950s, pioneers began to shape new languages: **Regional Assembly Language** (1951), **Autocode** (1952), and **IPL** (1954), the forerunner to LISP. Grace Hopper\u0027s **FLOW-MATIC** (1955) would pave the way to **COBOL** (1959), while **FORTRAN** (1957) brought the first compiler to life. The 1960s saw the birth of **LISP** (1958), **ALGOL** (1958, 1960), **BASIC** (1964), **PL/I** (1964), **BCPL** (1967), and **B** (1969), leading to the mighty **C** (1972).\n\nFrom the 1970s onward, the floodgates opened: **Pascal** (1970), **Smalltalk** (1972), **Prolog** (1972), **SQL** (1978), **C\u002B\u002B** (1980), **Ada** (1983), **Perl** (1987), **Python** (1991), **Java** (1995), **JavaScript** (1995), **Ruby** (1995), **PHP** (1995), and many more.\n\nEach era brought its own heroes, philosophies, and quirks. Some languages were swift and elegant, others sprawling and stubborn. But all of them, in their way, expanded what was possible.\n\nToday, we live in an age of abundance. We have languages, frameworks, and tools for nearly every conceivable task. And if the exact tool you need doesn\u0027t exist, the beauty of code is that you can create it yourself.\n\n---\n\n## Why Code Matters\n\nCode is power, not in the dystopian sense, but in the sense that it lets one person shape behavior, automate work, and solve problems in ways that scale far beyond their own hands.\n\nWith code, you can:\n\n* Automate boring tasks\n* Create tools that help people\n* Control machines in the physical world\n* Build art, games, and interactive experiences\n* Invent entirely new kinds of technology\n\nYou don\u0027t need to be a professional programmer to benefit from learning it. Even small bits of code can make life easier, save time, or open the door to entirely new possibilities.\n\n---\n\n## What\u0027s Next?\n\nThis post answered \u0022What is code?\u0022 in broad strokes: its forms, history, and why it matters. In the next part of this series, we\u0027ll start laying the **foundations of programming**:\n\n1. **Variables \u0026 Data Types** \u2014 the labeled containers of programming.\n2. **Operators** \u2014 the tools for working with your data.\n3. **Logic \u0026 Flow** \u2014 the recipes that decide what happens when.\n\nBy the time we\u0027re done, you\u0027ll not just *know* what code is, you\u0027ll be able to write it, read it, and use it to build something that matters to you.",
    "contentHtml": "\u003Cp\u003ETechnology is a wonderful thing. Humans, and perhaps even our direct ancestors, have been employing it for millennia. From stone tools to quantum computers, humans can\u0027t seem to resist tinkering with whatever the universe gives them.\u003C/p\u003E\n\u003Cp\u003EAnd with even our most remote, uncontacted relatives still in possession of weapons and shelter, it\u0027s safe to say every person on this planet experiences technology from birth to their final days. Right now, as you read this, you\u0027re surrounded by, and literally in contact with, an almost uncountable web of human inventions.\u003C/p\u003E\n\u003Cp\u003ETechnology shapes our lives, and in turn, we shape its future. It can be used for feats of great creation or acts of horrific destruction. Among all its branches, one of the most powerful and most accessible is \u003Cstrong\u003Ecode\u003C/strong\u003E.\u003C/p\u003E\n\u003Cp\u003EYou don\u0027t need a factory, a lab, or a workshop to work with code. All it takes is a computer and a spark of curiosity. If you\u0027ve ever stacked LEGO bricks into something from your imagination, built a spreadsheet formula to save hours of work, set a coffee maker to brew before you wake, or fixed something in a pinch with whatever was at hand, you\u0027ve tapped into the same problem-solving instinct that drives programming.\u003C/p\u003E\n\u003Cp\u003EAt its heart, coding is the act of taking what you have and arranging it into something that does what you want. It\u0027s imagination with rules, creativity with feedback. And once you\u0027ve seen something you\u0027ve built come alive, whether it\u0027s a blinking LED, a tool that saves someone\u0027s day, or an app that makes a stranger smile, it\u0027s hard not to want to build more.\u003C/p\u003E\n\u003Chr /\u003E\n\u003Ch2 id=\u0022what-is-code\u0022\u003EWhat is \u0026quot;Code\u0026quot;?\u003C/h2\u003E\n\u003Cp\u003ECode comes in many flavors. There are programming languages like C#, Assembly, C\u002B\u002B, Rust, Python, and countless others, each with its own syntax, style, and purpose. There\u0027s \u003Cstrong\u003EG-code\u003C/strong\u003E, the set of instructions understood by CNC mills, 3D printers, lasers, and other computer-controlled machines. Even music notation is a kind of code, a written sequence of symbols that, when interpreted, produces something meaningful.\u003C/p\u003E\n\u003Cp\u003EThe \u0026quot;code\u0026quot; most familiar to all of us is \u003Cstrong\u003Ehuman language\u003C/strong\u003E. Spoken or written, it can form relationships, transfer knowledge, and inspire change. But language is inherently ambiguous, which makes it ill-suited for telling computers exactly what to do. That\u0027s why programming languages exist: precise dialects designed for humans to read and for machines to execute without hesitation or confusion.\u003C/p\u003E\n\u003Cp\u003EPut simply:\u003C/p\u003E\n\u003Cblockquote\u003E\n\u003Cp\u003E\u003Cstrong\u003ECode is a system of signals, symbols, letters, words, or other constructs used to convey a message.\u003C/strong\u003E\u003C/p\u003E\n\u003C/blockquote\u003E\n\u003Cp\u003EAt the very bottom of every modern computer lies \u003Cstrong\u003Ebinary\u003C/strong\u003E \u2014 two values, usually 1 and 0, representing \u0026quot;on\u0026quot; and \u0026quot;off\u0026quot; or \u0026quot;true\u0026quot; and \u0026quot;false.\u0026quot; Working directly in binary is tedious, so we created layers of abstraction to make our lives easier.\u003C/p\u003E\n\u003Cp\u003EA step above binary is \u003Cstrong\u003Eassembly language\u003C/strong\u003E. Assembly comes in many dialects, each tied to the instruction set of a specific processor. It\u0027s still \u0026quot;close to the metal\u0026quot; but far more readable than raw ones and zeros.\u003C/p\u003E\n\u003Chr /\u003E\n\u003Ch3 id=\u0022a-somewhat-accurate-and-still-overly-dramatic-history-of-programming-languages\u0022\u003EA (Somewhat Accurate and Still Overly Dramatic) History of Programming Languages\u003C/h3\u003E\n\u003Cp\u003EThe history of programming languages is a tapestry of invention, frustration, and the occasional all-nighter. Imagine it told as a fantasy epic.\u003C/p\u003E\n\u003Cp\u003EFirst came \u003Cstrong\u003EBinary\u003C/strong\u003E, and from Binary were forged all other tongues of the machine. But the work was slow, and the scribes of silicon longed for an easier way to command their creations.\u003C/p\u003E\n\u003Cp\u003EIn the early 1950s, pioneers began to shape new languages: \u003Cstrong\u003ERegional Assembly Language\u003C/strong\u003E (1951), \u003Cstrong\u003EAutocode\u003C/strong\u003E (1952), and \u003Cstrong\u003EIPL\u003C/strong\u003E (1954), the forerunner to LISP. Grace Hopper\u0027s \u003Cstrong\u003EFLOW-MATIC\u003C/strong\u003E (1955) would pave the way to \u003Cstrong\u003ECOBOL\u003C/strong\u003E (1959), while \u003Cstrong\u003EFORTRAN\u003C/strong\u003E (1957) brought the first compiler to life. The 1960s saw the birth of \u003Cstrong\u003ELISP\u003C/strong\u003E (1958), \u003Cstrong\u003EALGOL\u003C/strong\u003E (1958, 1960), \u003Cstrong\u003EBASIC\u003C/strong\u003E (1964), \u003Cstrong\u003EPL/I\u003C/strong\u003E (1964), \u003Cstrong\u003EBCPL\u003C/strong\u003E (1967), and \u003Cstrong\u003EB\u003C/strong\u003E (1969), leading to the mighty \u003Cstrong\u003EC\u003C/strong\u003E (1972).\u003C/p\u003E\n\u003Cp\u003EFrom the 1970s onward, the floodgates opened: \u003Cstrong\u003EPascal\u003C/strong\u003E (1970), \u003Cstrong\u003ESmalltalk\u003C/strong\u003E (1972), \u003Cstrong\u003EProlog\u003C/strong\u003E (1972), \u003Cstrong\u003ESQL\u003C/strong\u003E (1978), \u003Cstrong\u003EC\u002B\u002B\u003C/strong\u003E (1980), \u003Cstrong\u003EAda\u003C/strong\u003E (1983), \u003Cstrong\u003EPerl\u003C/strong\u003E (1987), \u003Cstrong\u003EPython\u003C/strong\u003E (1991), \u003Cstrong\u003EJava\u003C/strong\u003E (1995), \u003Cstrong\u003EJavaScript\u003C/strong\u003E (1995), \u003Cstrong\u003ERuby\u003C/strong\u003E (1995), \u003Cstrong\u003EPHP\u003C/strong\u003E (1995), and many more.\u003C/p\u003E\n\u003Cp\u003EEach era brought its own heroes, philosophies, and quirks. Some languages were swift and elegant, others sprawling and stubborn. But all of them, in their way, expanded what was possible.\u003C/p\u003E\n\u003Cp\u003EToday, we live in an age of abundance. We have languages, frameworks, and tools for nearly every conceivable task. And if the exact tool you need doesn\u0027t exist, the beauty of code is that you can create it yourself.\u003C/p\u003E\n\u003Chr /\u003E\n\u003Ch2 id=\u0022why-code-matters\u0022\u003EWhy Code Matters\u003C/h2\u003E\n\u003Cp\u003ECode is power, not in the dystopian sense, but in the sense that it lets one person shape behavior, automate work, and solve problems in ways that scale far beyond their own hands.\u003C/p\u003E\n\u003Cp\u003EWith code, you can:\u003C/p\u003E\n\u003Cul\u003E\n\u003Cli\u003EAutomate boring tasks\u003C/li\u003E\n\u003Cli\u003ECreate tools that help people\u003C/li\u003E\n\u003Cli\u003EControl machines in the physical world\u003C/li\u003E\n\u003Cli\u003EBuild art, games, and interactive experiences\u003C/li\u003E\n\u003Cli\u003EInvent entirely new kinds of technology\u003C/li\u003E\n\u003C/ul\u003E\n\u003Cp\u003EYou don\u0027t need to be a professional programmer to benefit from learning it. Even small bits of code can make life easier, save time, or open the door to entirely new possibilities.\u003C/p\u003E\n\u003Chr /\u003E\n\u003Ch2 id=\u0022whats-next\u0022\u003EWhat\u0027s Next?\u003C/h2\u003E\n\u003Cp\u003EThis post answered \u0026quot;What is code?\u0026quot; in broad strokes: its forms, history, and why it matters. In the next part of this series, we\u0027ll start laying the \u003Cstrong\u003Efoundations of programming\u003C/strong\u003E:\u003C/p\u003E\n\u003Col\u003E\n\u003Cli\u003E\u003Cstrong\u003EVariables \u0026amp; Data Types\u003C/strong\u003E \u2014 the labeled containers of programming.\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003EOperators\u003C/strong\u003E \u2014 the tools for working with your data.\u003C/li\u003E\n\u003Cli\u003E\u003Cstrong\u003ELogic \u0026amp; Flow\u003C/strong\u003E \u2014 the recipes that decide what happens when.\u003C/li\u003E\n\u003C/ol\u003E\n\u003Cp\u003EBy the time we\u0027re done, you\u0027ll not just \u003Cem\u003Eknow\u003C/em\u003E what code is, you\u0027ll be able to write it, read it, and use it to build something that matters to you.\u003C/p\u003E\n",
    "stub": "A beginner-friendly exploration of what \u0022code\u0022 really is \u2014 from its many forms and history to why it matters today. We\u0027ll look at how programming languages evolved, how code shapes the world around us, and why you don\u0027t need to be a professional to start creating with it.",
    "wordCount": 922,
    "useToc": false,
    "source": "syndicated",
    "canonicalUrl": "https://jerrettdavis.com/blog/what-is-code"
  },
  {
    "id": "automating-daily-database-restores",
    "title": "Automating Nightly Local Database Refreshes from Azure Blob Storage with Docker",
    "date": "2024-02-27T00:00:00",
    "description": "Learn how to automate the retrieval and restoration of production database backups from Azure Blob Storage to a local SQL Server using Docker. This concise guide provides step-by-step instructions and scripts, enabling easy setup for development, testing, and analysis purposes.",
    "featured": "/images/posts/automating-daily-database-restores/featured.png",
    "tags": [
      "docker",
      "bash",
      "azure",
      "scripting",
      "automation"
    ],
    "categories": [
      "Programming",
      "Programming/Automation"
    ],
    "series": null,
    "seriesOrder": null,
    "content": "# Background\nIn cloud-hosted applications, it is common to restrict access to production databases. This is a good practice, but it can\nmake it difficult for various teams to access the data they need for development, testing, reporting, and data analysis.\nOne way to solve this problem is to create a daily process that copies the production database to a location that is accessible\nto the teams that need it. In this article, we will create a docker container that will download the latest backup of a\nproduction database from Azure Blob Storage and restore it to a local SQL Server instance.\n\n# Prerequisites\nThis article assumes that you have already set up an automation to back up your production database to Azure Blob Storage.\nIf you have not done this, you can follow the instructions in this article: [Automating Nightly Database Backups to Azure Blob Storage](https://techcommunity.microsoft.com/t5/azure-database-support-blog/automate-exporting-of-azure-sql-database-as-bacpac-to-blog/ba-p/2409213).\nThe script in the article is reaching end of life, and an updated version can be found [here](https://github.com/josegomera/AzureAutomation/tree/master/scripts/sqlDatabase).\n\nDocker must be installed and configured to used Linux containers on your local machine. If you do not\nhave Docker installed, you can download it from [Docker\u0027s website](https://www.docker.com/products/docker-desktop).\n\n# Setting up the Docker Container\nThe first step is to create a Dockerfile that will be used to build the container. The Dockerfile will contain the instructions\non how to configure the container and what commands to run when the container is started. For our environment, we will\nrequire Microsoft SQL Server to be installed in the container. We will use the official Microsoft SQL Server Docker image as a base.\n\n## Base Dockerfile\nCreate a new directory on your local machine and create a file called \u0060Dockerfile\u0060 in the directory. Add the following content to the file:\n\n\u0060\u0060\u0060Dockerfile\nFROM mcr.microsoft.com/mssql/server:2022-CU11-ubuntu-22.04\n\n# Set environment variables for the container\nARG ACCOUNT_NAME\nENV ACCOUNT_NAME=$ACCOUNT_NAME\nARG ACCOUNT_KEY\nENV ACCOUNT_KEY=$ACCOUNT_KEY\nARG CONTAINER_NAME\nENV CONTAINER_NAME=$CONTAINER_NAME\n\nARG CRON_SCHEDULE=\u00220 4 * * *\u0022\nENV CRON_SCHEDULE=$CRON_SCHEDULE\n\nARG DATABASE_NAME=MyDatabase\nENV DATABASE_NAME=$DATABASE_NAME\nENV MSSQL_SA_PASSWORD=yourStrong(!)Password\nENV ACCEPT_EULA=Y\nENV MSSQL_PID=Developer\n\n# Create a working directory for our tools and scripts and copy all the files from the host machine to the container\nCOPY . /sql\nWORKDIR /sql\n\u0060\u0060\u0060\n\n## Install Azure CLI\nTo find and download the latest backup of the production database, we will need to install the Azure CLI in the container.\nWe\u0027ll also need wget, cron, unzip, and a few other utilities to help us automate the process. Microsoft does offer a script\nto install the Azure CLI, but in my testing it did not work as expected in the Docker container. Instead, we will use\nthe [manual installation instructions](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli-linux?pivots=apt#option-2-step-by-step-installation-instructions).\n\nUpdate the \u0060Dockerfile\u0060 to include the following lines after the \u0060WORKDIR\u0060 line:\n\n\u0060\u0060\u0060Dockerfile\n# Must be root to install packages\nUSER root\n\n# Install Dependencies\nRUN apt-get update \u0026\u0026 \\\n    apt-get install -y --no-install-recommends \\\n    unzip cron wget apt-transport-https \\\n    software-properties-common ca-certificates curl \\\n    apt-transport-https lsb-release gnupg \u0026\u0026 \\\n    rm -rf /var/lib/apt/lists/*\n\n# Install az cli\nRUN mkdir -p /etc/apt/keyrings \u0026\u0026 \\\n    curl -sLS https://packages.microsoft.com/keys/microsoft.asc | \\\n        gpg --dearmor | \\\n        tee /etc/apt/keyrings/microsoft.gpg \u003E /dev/null \u0026\u0026 \\\n    chmod go\u002Br /etc/apt/keyrings/microsoft.gpg \u0026\u0026 \\\n    AZ_DIST=$(lsb_release -cs) \u0026\u0026 \\\n    echo \u0022deb [arch=amd64 signed-by=/etc/apt/keyrings/microsoft.gpg] https://packages.microsoft.com/repos/azure-cli/ $AZ_DIST main\u0022 | \\\n    tee /etc/apt/sources.list.d/azure-cli.list \u0026\u0026 \\\n    apt-get update \u0026\u0026 \\\n    apt-get install azure-cli \u0026\u0026 \\\n    rm -rf /var/lib/apt/lists/*\n\u0060\u0060\u0060\n\n## Install SqlPackage\nTo import the backup bacpac file that we\u0027re going to download from Azure Blob Storage, we will need to install the \u0060sqlpackage\u0060 utility.\nThis utility is used to import and export bacpac files to and from SQL Server. The utility has [evergreen links](https://learn.microsoft.com/en-us/sql/tools/sqlpackage/sqlpackage-download?view=sql-server-ver16#supported-operating-systems)\navailable, so we can use the link to download the latest version of the utility.\n\nAdd the following lines to the end of the \u0060Dockerfile\u0060:\n\n\u0060\u0060\u0060Dockerfile\n# Install SQLPackage for Linux and make it executable\nRUN wget -q -O sqlpackage.zip https://aka.ms/sqlpackage-linux \\\n    \u0026\u0026 unzip -qq sqlpackage.zip -d /sql/sqlpackage \\\n    \u0026\u0026 chmod \u002Bx /sql/sqlpackage/sqlpackage \\\n    \u0026\u0026 rm sqlpackage.zip\n\u0060\u0060\u0060\nThe above lines download the latest version of the \u0060sqlpackage\u0060 utility from Microsoft\u0027s website, unzip it, make it executable, and then remove the zip file.\n\n\n## Entrypoint\n\nFinally, we need to add our entrypoint script to the container. This script will be run when the container starts, and it\nwill perform all the necessary steps to download the latest backup from Azure Blob Storage and restore it to the local SQL Server instance.\n\nAdd the following lines to the end of the \u0060Dockerfile\u0060:\n\n\u0060\u0060\u0060Dockerfile\n# Switch back to mssql user\nUSER mssql\n\nEXPOSE 1433\n\nCMD /bin/bash ./entrypoint.sh\n\u0060\u0060\u0060\nWe need to switch back to the \u0060mssql\u0060 user to run the SQL Server process, so we make use of the \u0060USER\u0060 command to do this.\nThe \u0060EXPOSE\u0060 command tells Docker that the container listens on the specified network ports at runtime.\nThe \u0060CMD\u0060 command specifies the command that will be run when the container starts. In this case, we are running the \u0060entrypoint.sh\u0060 script.\n\nWe will make some final changes to our \u0060Dockerfile\u0060 later, but for now, save the file and close it.\n\n# The Scripts\nAs we alluded to at the end of our \u0060Dockerfile\u0060, we need to create an \u0060entrypoint.sh\u0060 script that will be run when the container starts.\nSince this container is based on the official Microsoft SQL Server Docker image, we need to ensure the original entrypoint is also run\nalongside our custom entrypoint script. To do this, we need to create an additional script that we will call in our \u0060entrypoint.sh\u0060 script.\n\n## entrypoint.sh\nCreate two new files in the same directory as your \u0060Dockerfile\u0060 called \u0060entrypoint.sh\u0060 and \u0060initialize-database-and-jobs.sh\u0060.\nAdd the following content to \u0060entrypoint.sh\u0060:\n\n\u0060\u0060\u0060bash\n#!/bin/bash\n/sql/initialize-database-and-jobs.sh \u0026 /opt/mssql/bin/sqlservr\n\u0060\u0060\u0060\n\nYou\u0027ll note that we are running the \u0060initialize-database-and-jobs.sh\u0060 script in the background and then starting the SQL Server process.\nThis \u0060\u0026\u0060 syntax is necessary to ensure that the original entrypoint script is also run without the docker container exiting immediately after\nthe script completes.\n\n## initialize-database-and-jobs.sh\n\nAdd the following content to \u0060initialize-database-and-jobs.sh\u0060:\n\u0060\u0060\u0060bash\n#!/bin/bash\n\n# wait 30 seconds for SQL Server to start up\necho \u0022Waiting for SQL Server to start\u0022\nsleep 30s\n\n# Download the bacpac file from the Azure Blob Storage\necho \u0022Downloading bacpac file from Azure Blob Storage\u0022\nbash /sql/download-latest.sh $ACCOUNT_NAME $ACCOUNT_KEY $CONTAINER_NAME /sql/backup.bacpac\nbackupJob=$?\n\nif [ \u0022$backupJob\u0022 -eq 0 ]\nthen\n    echo \u0022Successfully downloaded bacpac file from Azure Blob Storage!\u0022\n    echo \u0022Enabling SQL Server authentication...\u0022\n    /opt/mssql-tools/bin/sqlcmd -S localhost -U sa -P $MSSQL_SA_PASSWORD -d master -i /sql/enable-authentication.sql\n    echo \u0022SQL Server authentication enabled. Waiting for 10 seconds before importing the bacpac file...\u0022\n    sleep 10s\n\n    # Import the bacpac file into the SQL Server\n    /sql/sqlpackage/sqlpackage /a:import /sf:/sql/backup.bacpac /tsn:localhost,1433 /tdn:$DATABASE_NAME /tu:sa /tp:$MSSQL_SA_PASSWORD /ttsc:True\n\n    # Set up 4am CRON job to re-import the database\n    echo \u0022$CRON_SCHEDULE /bin/bash /sql/reimport-database.sh\u0022 | crontab -\n    echo \u0022CRON job set up successfully\u0022\n    exit 0\nelse\n    echo \u0022Failed to download bacpac file from Azure Blob Storage\u0022\n    exit 1\nfi\n\u0060\u0060\u0060\n\nThis short script does quite a few operations. First, it\u0027s recommended to wait for SQL Server to start up before attempting to connect to it.\nWe set up a 30-second timer to wait for SQL Server to start. We then download the latest backup from Azure Blob Storage using the \u0060download-latest.sh\u0060 script.\nIf the download is successful, we use the built-in \u0060sqlcmd\u0060 utility to enable SQL Server authentication. We then wait for 10 seconds to ensure that the\nSQL Server has stabilized. We then use the \u0060sqlpackage\u0060 utility to import the bacpac file into the SQL Server. Finally, we set up a CRON job to run the\n\u0060reimport-database.sh\u0060 script at a frequency specified by the \u0060CRON_SCHEDULE\u0060 environment variable. We then exit the script with a success code.\n\nWe need to create the \u0060download-latest.sh\u0060 and \u0060reimport-database.sh\u0060 scripts that are called in the \u0060initialize-database-and-jobs.sh\u0060 script.\n\n## download-latest.sh\n\nCreate a new file called \u0060download-latest.sh\u0060 and add the following content:\n\u0060\u0060\u0060bash\n#!/bin/bash\n\n# Description: This script downloads the latest backup from an Azure Storage Account\n# Usage: bash DownloadLatest.sh \u003CstorageAccountName\u003E \u003CstorageAccountKey\u003E \u003CcontainerName\u003E \u003ClocalPath\u003E\n\naccountName=$1\naccountKey=$2\ncontainerName=$3\nlocalPath=${4:-\u0022./backup.bacpac\u0022}\n\n# Get the name of the latest blob\nfirstBlob=$(az storage blob list \\\n    --account-key $accountKey \\\n    --account-name $accountName \\\n    -c $containerName \\\n    --query \u0022[?properties.lastModified!=null \u0026\u0026 ends_with(name, \u0027.bacpac\u0027)]|sort_by(@, \u0026properties.lastModified)[-1].name\u0022 \\\n    -o tsv)\n\n# Check if $firstBlob is not null (i.e., there are blobs found)\nif [ -n \u0022$firstBlob\u0022 ]; then\n    az storage blob download --account-key $accountKey --account-name $accountName -c $containerName --name $firstBlob --file $localPath --output none\n    exit 0\nelse\n    exit 1\nfi\n\u0060\u0060\u0060\n\nThe Azure CLI lets us write queries to filter the results of the \u0060az storage blob list\u0060 command. The queries are written in\n[JMESPath](https://jmespath.org/), which is a query language for JSON. In this case, we are filtering the results to only include blobs that end with the\n\u0060.bacpac\u0060 extension and then selecting the first one as ordered by the \u0060lastModified\u0060 property. If there are no blobs found, the script exits with a failure code.\nIf we find a blob, we download it to the local path specified by the \u0060localPath\u0060 variable.\n\n## enable-authentication.sql\n\nCreate a new file called \u0060enable-authentication.sql\u0060 and add the following content:\n\u0060\u0060\u0060sql\nsp_configure \u0027contained database authentication\u0027, 1;\nGO\nRECONFIGURE;\nGO\n\u0060\u0060\u0060\n\nThis script enables contained database authentication in the SQL Server instance. This is necessary to allow the \u0060sa\u0060 user to authenticate to the database\nwhen importing the bacpac file.\n\n## reimport-database.sh\n\nCreate a new file called \u0060reimport-database.sh\u0060 and add the following content:\n\u0060\u0060\u0060bash\n#!/bin/bash\n\necho \u0022Downloading bacpac file from Azure Blob Storage\u0022\nbash /sql/download-latest.sh $ACCOUNT_NAME $ACCOUNT_KEY $CONTAINER_NAME /sql/backup.bacpac\nbackupJob=$?\nif [ \u0022$backupJob\u0022 -eq 0 ]\nthen\n    echo \u0022Successfully downloaded bacpac file from Azure Blob Storage!\u0022\n    echo \u0022Kill all connections to the database\u0022\n    /opt/mssql-tools/bin/sqlcmd -S localhost -U sa -P $MSSQL_SA_PASSWORD -d master -i /sql/kill-all-connections.sql\n    databaseName=$DATABASE_NAME\n    existingDatabaseName=\u0022${databaseName}_$(date \u002B%s)\u0022\n    echo \u0022Renaming existing database to $existingDatabaseName\u0022\n    /opt/mssql-tools/bin/sqlcmd -S localhost -U sa -P $MSSQL_SA_PASSWORD -d master -Q \u0022ALTER DATABASE $databaseName MODIFY NAME = $existingDatabaseName;\u0022\n    echo \u0022Renamed existing database to $existingDatabaseName\u0022\n    echo \u0022Importing bacpac file into the SQL Server\u0022\n    /sql/sqlpackage/sqlpackage /a:import /sf:/sql/backup.bacpac /tsn:localhost,1433 /tdn:$DATABASE_NAME /tu:sa /tp:$MSSQL_SA_PASSWORD /ttsc:True\nelse\n    echo \u0022Failed to download bacpac file from Azure Blob Storage\u0022\n    exit 1\nfi\n\u0060\u0060\u0060\n\nYou\u0027ll notice that this script is very similar to the \u0060initialize-database-and-jobs.sh\u0060 script. The main difference is that we are renaming the existing database\nbefore importing the new bacpac file. This is necessary because the \u0060sqlpackage\u0060 utility does not support overwriting an existing database.\nWe also need to kill all connections to the database before renaming it. We do this by running a SQL script that we will create called \u0060kill-all-connections.sql\u0060.\n\n## kill-all-connections.sql\n\nCreate a new file called \u0060kill-all-connections.sql\u0060 and add the following content:\n\u0060\u0060\u0060sql\n-- kill all connections to the database\nDECLARE @killCommand NVARCHAR(MAX) = \u0027\u0027;\n\nSELECT @killCommand = @killCommand \u002B \u0027KILL \u0027 \u002B CAST(spid AS VARCHAR) \u002B \u0027;\u0027\nFROM sys.sysprocesses\nWHERE dbid \u003E 4;\n\nEXEC sp_executesql @killCommand;\n\u0060\u0060\u0060\n\nAs its name suggests, this script kills all connections to the database. This is necessary to ensure that we can rename the database without any active connections.\n\n## Final Directory Structure and Dockerfile Changes\nIf everything went to plan, your directory should look like this:\n\u0060\u0060\u0060\n.\n\u251C\u2500\u2500 Dockerfile\n\u251C\u2500\u2500 entrypoint.sh\n\u251C\u2500\u2500 initialize-database-and-jobs.sh\n\u251C\u2500\u2500 download-latest.sh\n\u251C\u2500\u2500 enable-authentication.sql\n\u251C\u2500\u2500 reimport-database.sh\n\u251C\u2500\u2500 kill-all-connections.sql\n\u0060\u0060\u0060\n\nNow before we can build the Docker container, we need to make a few final changes to our \u0060Dockerfile\u0060.\nWe need to copy all the files from our local machine to the container and set the correct permissions on the scripts.\nAdd the following lines to your \u0060Dockerfile\u0060 directly after the \u0060USER root\u0060 line:\n\n\u0060\u0060\u0060Dockerfile\nRUN mkdir /home/mssql \u0026\u0026 chown mssql /home/mssql \u0026\u0026 \\\n    chmod \u002Bx /sql/initialize-database-and-jobs.sh \u0026\u0026 \\\n    chmod \u002Bx /sql/entrypoint.sh \u0026\u0026 \\\n    chmod \u002Bx /sql/download-latest.sh\n\u0060\u0060\u0060\n\nThe \u0060mkdir\u0060 command creates a new directory in the container and the \u0060chown\u0060 command changes the owner of the directory to the \u0060mssql\u0060 user.\nWe then set the correct permissions on the scripts using the \u0060chmod\u0060 command. Save the \u0060Dockerfile\u0060 and close it.\n\n# Building the Docker Container\nNow that we have all the necessary files, we can build the Docker container. Open a terminal and navigate to the directory where your \u0060Dockerfile\u0060 is located.\nRun the following command to build the container:\n\n\u0060\u0060\u0060bash\ndocker build -t azure-local-database-refresh .\n\u0060\u0060\u0060\n\nThis command will build the container using the \u0060Dockerfile\u0060 in the current directory and tag the container with the name \u0060azure-local-database-refresh\u0060.\nThe build process may take a few minutes to complete. Once it\u0027s done, you can run the container using the following command:\n\n\u0060\u0060\u0060bash\ndocker run -e ACCOUNT_NAME=\u003CstorageAccountName\u003E -e ACCOUNT_KEY=\u003CstorageAccountKey\u003E -e CONTAINER_NAME=\u003CcontainerName\u003E -e DATABASE_NAME=MyDatabase -p 1433:1433 -it  azure-local-database-refresh\n\u0060\u0060\u0060\n\nReplace \u0060\u003CstorageAccountName\u003E\u0060, \u0060\u003CstorageAccountKey\u003E\u0060, and \u0060\u003CcontainerName\u003E\u0060 with the appropriate values for your Azure Blob Storage account. The \u0060-p\u0060 flag maps port 1433 of the container to port 1433 of your local machine.\nThis allows you to connect to the SQL Server instance running in the container from your local machine. The \u0060-it\u0060 flag runs the container in interactive mode, which allows you to see the output of the container in your terminal.\n\nYou should see the output of the container in your terminal. If everything is working correctly, you should see messages indicating that the bacpac file has been downloaded and imported into the SQL Server instance. The import\nprocess can take a several minutes to complete, depending on the size of the database. Once the import process is complete, you should see a message indicating that the CRON job has been set up successfully.\n\n# Testing the Container\nNow that we have our container up and running, we can test it by connecting to the SQL Server instance and verifying that the database has been restored.\nOpen a new terminal and run the following command to connect to the SQL Server instance running in the container:\n\n\u0060\u0060\u0060bash\ndocker exec -it \u003CcontainerId\u003E /opt/mssql-tools/bin/sqlcmd -S localhost -U sa -P yourStrong(!)Password\n\u0060\u0060\u0060\n\nReplace \u0060\u003CcontainerId\u003E\u0060 with the ID of your container. This command will open an interactive SQL Server prompt. Run the following command to verify that the database has been restored:\n\n\u0060\u0060\u0060sql\nSELECT name FROM sys.databases;\n\u0060\u0060\u0060\n\nIf everything went to plan you should see the name of your database in the output. You can also run queries against the database to verify that the data has been restored correctly.\n\n\n# Conclusion\nIn this article, we created a Docker container that automates the process of downloading the latest backup of a production database from Azure Blob Storage and restoring it to a local SQL Server instance.\nWe used a combination of bash scripts and Docker to create a portable and easy-to-use solution that can be run on any machine that has Docker installed. This solution can be used to provide teams with access\nto the latest production data for development, testing, reporting, data analysis, or other internal uses. The container can be run on a schedule using a CRON job to ensure that the data is always up-to-date. This solution can be\neasily extended to support other databases and cloud storage providers.\n\nThe code for this solution is available on [GitHub](https://github.com/JerrettDavis/az-bacpac-blob-mssql-importer).\nThis docker container is also available on [Docker Hub](https://hub.docker.com/r/jdhproductions/az-bacpac-blob-mssql-importer).",
    "contentHtml": "\u003Ch1 id=\u0022background\u0022\u003EBackground\u003C/h1\u003E\n\u003Cp\u003EIn cloud-hosted applications, it is common to restrict access to production databases. This is a good practice, but it can\nmake it difficult for various teams to access the data they need for development, testing, reporting, and data analysis.\nOne way to solve this problem is to create a daily process that copies the production database to a location that is accessible\nto the teams that need it. In this article, we will create a docker container that will download the latest backup of a\nproduction database from Azure Blob Storage and restore it to a local SQL Server instance.\u003C/p\u003E\n\u003Ch1 id=\u0022prerequisites\u0022\u003EPrerequisites\u003C/h1\u003E\n\u003Cp\u003EThis article assumes that you have already set up an automation to back up your production database to Azure Blob Storage.\nIf you have not done this, you can follow the instructions in this article: \u003Ca href=\u0022https://techcommunity.microsoft.com/t5/azure-database-support-blog/automate-exporting-of-azure-sql-database-as-bacpac-to-blog/ba-p/2409213\u0022\u003EAutomating Nightly Database Backups to Azure Blob Storage\u003C/a\u003E.\nThe script in the article is reaching end of life, and an updated version can be found \u003Ca href=\u0022https://github.com/josegomera/AzureAutomation/tree/master/scripts/sqlDatabase\u0022\u003Ehere\u003C/a\u003E.\u003C/p\u003E\n\u003Cp\u003EDocker must be installed and configured to used Linux containers on your local machine. If you do not\nhave Docker installed, you can download it from \u003Ca href=\u0022https://www.docker.com/products/docker-desktop\u0022\u003EDocker\u0027s website\u003C/a\u003E.\u003C/p\u003E\n\u003Ch1 id=\u0022setting-up-the-docker-container\u0022\u003ESetting up the Docker Container\u003C/h1\u003E\n\u003Cp\u003EThe first step is to create a Dockerfile that will be used to build the container. The Dockerfile will contain the instructions\non how to configure the container and what commands to run when the container is started. For our environment, we will\nrequire Microsoft SQL Server to be installed in the container. We will use the official Microsoft SQL Server Docker image as a base.\u003C/p\u003E\n\u003Ch2 id=\u0022base-dockerfile\u0022\u003EBase Dockerfile\u003C/h2\u003E\n\u003Cp\u003ECreate a new directory on your local machine and create a file called \u003Ccode\u003EDockerfile\u003C/code\u003E in the directory. Add the following content to the file:\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-Dockerfile\u0022\u003EFROM mcr.microsoft.com/mssql/server:2022-CU11-ubuntu-22.04\n\n# Set environment variables for the container\nARG ACCOUNT_NAME\nENV ACCOUNT_NAME=$ACCOUNT_NAME\nARG ACCOUNT_KEY\nENV ACCOUNT_KEY=$ACCOUNT_KEY\nARG CONTAINER_NAME\nENV CONTAINER_NAME=$CONTAINER_NAME\n\nARG CRON_SCHEDULE=\u0026quot;0 4 * * *\u0026quot;\nENV CRON_SCHEDULE=$CRON_SCHEDULE\n\nARG DATABASE_NAME=MyDatabase\nENV DATABASE_NAME=$DATABASE_NAME\nENV MSSQL_SA_PASSWORD=yourStrong(!)Password\nENV ACCEPT_EULA=Y\nENV MSSQL_PID=Developer\n\n# Create a working directory for our tools and scripts and copy all the files from the host machine to the container\nCOPY . /sql\nWORKDIR /sql\n\u003C/code\u003E\u003C/pre\u003E\n\u003Ch2 id=\u0022install-azure-cli\u0022\u003EInstall Azure CLI\u003C/h2\u003E\n\u003Cp\u003ETo find and download the latest backup of the production database, we will need to install the Azure CLI in the container.\nWe\u0027ll also need wget, cron, unzip, and a few other utilities to help us automate the process. Microsoft does offer a script\nto install the Azure CLI, but in my testing it did not work as expected in the Docker container. Instead, we will use\nthe \u003Ca href=\u0022https://learn.microsoft.com/en-us/cli/azure/install-azure-cli-linux?pivots=apt#option-2-step-by-step-installation-instructions\u0022\u003Emanual installation instructions\u003C/a\u003E.\u003C/p\u003E\n\u003Cp\u003EUpdate the \u003Ccode\u003EDockerfile\u003C/code\u003E to include the following lines after the \u003Ccode\u003EWORKDIR\u003C/code\u003E line:\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-Dockerfile\u0022\u003E# Must be root to install packages\nUSER root\n\n# Install Dependencies\nRUN apt-get update \u0026amp;\u0026amp; \\\n    apt-get install -y --no-install-recommends \\\n    unzip cron wget apt-transport-https \\\n    software-properties-common ca-certificates curl \\\n    apt-transport-https lsb-release gnupg \u0026amp;\u0026amp; \\\n    rm -rf /var/lib/apt/lists/*\n\n# Install az cli\nRUN mkdir -p /etc/apt/keyrings \u0026amp;\u0026amp; \\\n    curl -sLS https://packages.microsoft.com/keys/microsoft.asc | \\\n        gpg --dearmor | \\\n        tee /etc/apt/keyrings/microsoft.gpg \u0026gt; /dev/null \u0026amp;\u0026amp; \\\n    chmod go\u002Br /etc/apt/keyrings/microsoft.gpg \u0026amp;\u0026amp; \\\n    AZ_DIST=$(lsb_release -cs) \u0026amp;\u0026amp; \\\n    echo \u0026quot;deb [arch=amd64 signed-by=/etc/apt/keyrings/microsoft.gpg] https://packages.microsoft.com/repos/azure-cli/ $AZ_DIST main\u0026quot; | \\\n    tee /etc/apt/sources.list.d/azure-cli.list \u0026amp;\u0026amp; \\\n    apt-get update \u0026amp;\u0026amp; \\\n    apt-get install azure-cli \u0026amp;\u0026amp; \\\n    rm -rf /var/lib/apt/lists/*\n\u003C/code\u003E\u003C/pre\u003E\n\u003Ch2 id=\u0022install-sqlpackage\u0022\u003EInstall SqlPackage\u003C/h2\u003E\n\u003Cp\u003ETo import the backup bacpac file that we\u0027re going to download from Azure Blob Storage, we will need to install the \u003Ccode\u003Esqlpackage\u003C/code\u003E utility.\nThis utility is used to import and export bacpac files to and from SQL Server. The utility has \u003Ca href=\u0022https://learn.microsoft.com/en-us/sql/tools/sqlpackage/sqlpackage-download?view=sql-server-ver16#supported-operating-systems\u0022\u003Eevergreen links\u003C/a\u003E\navailable, so we can use the link to download the latest version of the utility.\u003C/p\u003E\n\u003Cp\u003EAdd the following lines to the end of the \u003Ccode\u003EDockerfile\u003C/code\u003E:\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-Dockerfile\u0022\u003E# Install SQLPackage for Linux and make it executable\nRUN wget -q -O sqlpackage.zip https://aka.ms/sqlpackage-linux \\\n    \u0026amp;\u0026amp; unzip -qq sqlpackage.zip -d /sql/sqlpackage \\\n    \u0026amp;\u0026amp; chmod \u002Bx /sql/sqlpackage/sqlpackage \\\n    \u0026amp;\u0026amp; rm sqlpackage.zip\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EThe above lines download the latest version of the \u003Ccode\u003Esqlpackage\u003C/code\u003E utility from Microsoft\u0027s website, unzip it, make it executable, and then remove the zip file.\u003C/p\u003E\n\u003Ch2 id=\u0022entrypoint\u0022\u003EEntrypoint\u003C/h2\u003E\n\u003Cp\u003EFinally, we need to add our entrypoint script to the container. This script will be run when the container starts, and it\nwill perform all the necessary steps to download the latest backup from Azure Blob Storage and restore it to the local SQL Server instance.\u003C/p\u003E\n\u003Cp\u003EAdd the following lines to the end of the \u003Ccode\u003EDockerfile\u003C/code\u003E:\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-Dockerfile\u0022\u003E# Switch back to mssql user\nUSER mssql\n\nEXPOSE 1433\n\nCMD /bin/bash ./entrypoint.sh\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EWe need to switch back to the \u003Ccode\u003Emssql\u003C/code\u003E user to run the SQL Server process, so we make use of the \u003Ccode\u003EUSER\u003C/code\u003E command to do this.\nThe \u003Ccode\u003EEXPOSE\u003C/code\u003E command tells Docker that the container listens on the specified network ports at runtime.\nThe \u003Ccode\u003ECMD\u003C/code\u003E command specifies the command that will be run when the container starts. In this case, we are running the \u003Ccode\u003Eentrypoint.sh\u003C/code\u003E script.\u003C/p\u003E\n\u003Cp\u003EWe will make some final changes to our \u003Ccode\u003EDockerfile\u003C/code\u003E later, but for now, save the file and close it.\u003C/p\u003E\n\u003Ch1 id=\u0022the-scripts\u0022\u003EThe Scripts\u003C/h1\u003E\n\u003Cp\u003EAs we alluded to at the end of our \u003Ccode\u003EDockerfile\u003C/code\u003E, we need to create an \u003Ccode\u003Eentrypoint.sh\u003C/code\u003E script that will be run when the container starts.\nSince this container is based on the official Microsoft SQL Server Docker image, we need to ensure the original entrypoint is also run\nalongside our custom entrypoint script. To do this, we need to create an additional script that we will call in our \u003Ccode\u003Eentrypoint.sh\u003C/code\u003E script.\u003C/p\u003E\n\u003Ch2 id=\u0022entrypoint.sh\u0022\u003Eentrypoint.sh\u003C/h2\u003E\n\u003Cp\u003ECreate two new files in the same directory as your \u003Ccode\u003EDockerfile\u003C/code\u003E called \u003Ccode\u003Eentrypoint.sh\u003C/code\u003E and \u003Ccode\u003Einitialize-database-and-jobs.sh\u003C/code\u003E.\nAdd the following content to \u003Ccode\u003Eentrypoint.sh\u003C/code\u003E:\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-bash\u0022\u003E#!/bin/bash\n/sql/initialize-database-and-jobs.sh \u0026amp; /opt/mssql/bin/sqlservr\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EYou\u0027ll note that we are running the \u003Ccode\u003Einitialize-database-and-jobs.sh\u003C/code\u003E script in the background and then starting the SQL Server process.\nThis \u003Ccode\u003E\u0026amp;\u003C/code\u003E syntax is necessary to ensure that the original entrypoint script is also run without the docker container exiting immediately after\nthe script completes.\u003C/p\u003E\n\u003Ch2 id=\u0022initialize-database-and-jobs.sh\u0022\u003Einitialize-database-and-jobs.sh\u003C/h2\u003E\n\u003Cp\u003EAdd the following content to \u003Ccode\u003Einitialize-database-and-jobs.sh\u003C/code\u003E:\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-bash\u0022\u003E#!/bin/bash\n\n# wait 30 seconds for SQL Server to start up\necho \u0026quot;Waiting for SQL Server to start\u0026quot;\nsleep 30s\n\n# Download the bacpac file from the Azure Blob Storage\necho \u0026quot;Downloading bacpac file from Azure Blob Storage\u0026quot;\nbash /sql/download-latest.sh $ACCOUNT_NAME $ACCOUNT_KEY $CONTAINER_NAME /sql/backup.bacpac\nbackupJob=$?\n\nif [ \u0026quot;$backupJob\u0026quot; -eq 0 ]\nthen\n    echo \u0026quot;Successfully downloaded bacpac file from Azure Blob Storage!\u0026quot;\n    echo \u0026quot;Enabling SQL Server authentication...\u0026quot;\n    /opt/mssql-tools/bin/sqlcmd -S localhost -U sa -P $MSSQL_SA_PASSWORD -d master -i /sql/enable-authentication.sql\n    echo \u0026quot;SQL Server authentication enabled. Waiting for 10 seconds before importing the bacpac file...\u0026quot;\n    sleep 10s\n\n    # Import the bacpac file into the SQL Server\n    /sql/sqlpackage/sqlpackage /a:import /sf:/sql/backup.bacpac /tsn:localhost,1433 /tdn:$DATABASE_NAME /tu:sa /tp:$MSSQL_SA_PASSWORD /ttsc:True\n\n    # Set up 4am CRON job to re-import the database\n    echo \u0026quot;$CRON_SCHEDULE /bin/bash /sql/reimport-database.sh\u0026quot; | crontab -\n    echo \u0026quot;CRON job set up successfully\u0026quot;\n    exit 0\nelse\n    echo \u0026quot;Failed to download bacpac file from Azure Blob Storage\u0026quot;\n    exit 1\nfi\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EThis short script does quite a few operations. First, it\u0027s recommended to wait for SQL Server to start up before attempting to connect to it.\nWe set up a 30-second timer to wait for SQL Server to start. We then download the latest backup from Azure Blob Storage using the \u003Ccode\u003Edownload-latest.sh\u003C/code\u003E script.\nIf the download is successful, we use the built-in \u003Ccode\u003Esqlcmd\u003C/code\u003E utility to enable SQL Server authentication. We then wait for 10 seconds to ensure that the\nSQL Server has stabilized. We then use the \u003Ccode\u003Esqlpackage\u003C/code\u003E utility to import the bacpac file into the SQL Server. Finally, we set up a CRON job to run the\n\u003Ccode\u003Ereimport-database.sh\u003C/code\u003E script at a frequency specified by the \u003Ccode\u003ECRON_SCHEDULE\u003C/code\u003E environment variable. We then exit the script with a success code.\u003C/p\u003E\n\u003Cp\u003EWe need to create the \u003Ccode\u003Edownload-latest.sh\u003C/code\u003E and \u003Ccode\u003Ereimport-database.sh\u003C/code\u003E scripts that are called in the \u003Ccode\u003Einitialize-database-and-jobs.sh\u003C/code\u003E script.\u003C/p\u003E\n\u003Ch2 id=\u0022download-latest.sh\u0022\u003Edownload-latest.sh\u003C/h2\u003E\n\u003Cp\u003ECreate a new file called \u003Ccode\u003Edownload-latest.sh\u003C/code\u003E and add the following content:\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-bash\u0022\u003E#!/bin/bash\n\n# Description: This script downloads the latest backup from an Azure Storage Account\n# Usage: bash DownloadLatest.sh \u0026lt;storageAccountName\u0026gt; \u0026lt;storageAccountKey\u0026gt; \u0026lt;containerName\u0026gt; \u0026lt;localPath\u0026gt;\n\naccountName=$1\naccountKey=$2\ncontainerName=$3\nlocalPath=${4:-\u0026quot;./backup.bacpac\u0026quot;}\n\n# Get the name of the latest blob\nfirstBlob=$(az storage blob list \\\n    --account-key $accountKey \\\n    --account-name $accountName \\\n    -c $containerName \\\n    --query \u0026quot;[?properties.lastModified!=null \u0026amp;\u0026amp; ends_with(name, \u0027.bacpac\u0027)]|sort_by(@, \u0026amp;properties.lastModified)[-1].name\u0026quot; \\\n    -o tsv)\n\n# Check if $firstBlob is not null (i.e., there are blobs found)\nif [ -n \u0026quot;$firstBlob\u0026quot; ]; then\n    az storage blob download --account-key $accountKey --account-name $accountName -c $containerName --name $firstBlob --file $localPath --output none\n    exit 0\nelse\n    exit 1\nfi\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EThe Azure CLI lets us write queries to filter the results of the \u003Ccode\u003Eaz storage blob list\u003C/code\u003E command. The queries are written in\n\u003Ca href=\u0022https://jmespath.org/\u0022\u003EJMESPath\u003C/a\u003E, which is a query language for JSON. In this case, we are filtering the results to only include blobs that end with the\n\u003Ccode\u003E.bacpac\u003C/code\u003E extension and then selecting the first one as ordered by the \u003Ccode\u003ElastModified\u003C/code\u003E property. If there are no blobs found, the script exits with a failure code.\nIf we find a blob, we download it to the local path specified by the \u003Ccode\u003ElocalPath\u003C/code\u003E variable.\u003C/p\u003E\n\u003Ch2 id=\u0022enable-authentication.sql\u0022\u003Eenable-authentication.sql\u003C/h2\u003E\n\u003Cp\u003ECreate a new file called \u003Ccode\u003Eenable-authentication.sql\u003C/code\u003E and add the following content:\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-sql\u0022\u003Esp_configure \u0027contained database authentication\u0027, 1;\nGO\nRECONFIGURE;\nGO\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EThis script enables contained database authentication in the SQL Server instance. This is necessary to allow the \u003Ccode\u003Esa\u003C/code\u003E user to authenticate to the database\nwhen importing the bacpac file.\u003C/p\u003E\n\u003Ch2 id=\u0022reimport-database.sh\u0022\u003Ereimport-database.sh\u003C/h2\u003E\n\u003Cp\u003ECreate a new file called \u003Ccode\u003Ereimport-database.sh\u003C/code\u003E and add the following content:\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-bash\u0022\u003E#!/bin/bash\n\necho \u0026quot;Downloading bacpac file from Azure Blob Storage\u0026quot;\nbash /sql/download-latest.sh $ACCOUNT_NAME $ACCOUNT_KEY $CONTAINER_NAME /sql/backup.bacpac\nbackupJob=$?\nif [ \u0026quot;$backupJob\u0026quot; -eq 0 ]\nthen\n    echo \u0026quot;Successfully downloaded bacpac file from Azure Blob Storage!\u0026quot;\n    echo \u0026quot;Kill all connections to the database\u0026quot;\n    /opt/mssql-tools/bin/sqlcmd -S localhost -U sa -P $MSSQL_SA_PASSWORD -d master -i /sql/kill-all-connections.sql\n    databaseName=$DATABASE_NAME\n    existingDatabaseName=\u0026quot;${databaseName}_$(date \u002B%s)\u0026quot;\n    echo \u0026quot;Renaming existing database to $existingDatabaseName\u0026quot;\n    /opt/mssql-tools/bin/sqlcmd -S localhost -U sa -P $MSSQL_SA_PASSWORD -d master -Q \u0026quot;ALTER DATABASE $databaseName MODIFY NAME = $existingDatabaseName;\u0026quot;\n    echo \u0026quot;Renamed existing database to $existingDatabaseName\u0026quot;\n    echo \u0026quot;Importing bacpac file into the SQL Server\u0026quot;\n    /sql/sqlpackage/sqlpackage /a:import /sf:/sql/backup.bacpac /tsn:localhost,1433 /tdn:$DATABASE_NAME /tu:sa /tp:$MSSQL_SA_PASSWORD /ttsc:True\nelse\n    echo \u0026quot;Failed to download bacpac file from Azure Blob Storage\u0026quot;\n    exit 1\nfi\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EYou\u0027ll notice that this script is very similar to the \u003Ccode\u003Einitialize-database-and-jobs.sh\u003C/code\u003E script. The main difference is that we are renaming the existing database\nbefore importing the new bacpac file. This is necessary because the \u003Ccode\u003Esqlpackage\u003C/code\u003E utility does not support overwriting an existing database.\nWe also need to kill all connections to the database before renaming it. We do this by running a SQL script that we will create called \u003Ccode\u003Ekill-all-connections.sql\u003C/code\u003E.\u003C/p\u003E\n\u003Ch2 id=\u0022kill-all-connections.sql\u0022\u003Ekill-all-connections.sql\u003C/h2\u003E\n\u003Cp\u003ECreate a new file called \u003Ccode\u003Ekill-all-connections.sql\u003C/code\u003E and add the following content:\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-sql\u0022\u003E-- kill all connections to the database\nDECLARE @killCommand NVARCHAR(MAX) = \u0027\u0027;\n\nSELECT @killCommand = @killCommand \u002B \u0027KILL \u0027 \u002B CAST(spid AS VARCHAR) \u002B \u0027;\u0027\nFROM sys.sysprocesses\nWHERE dbid \u0026gt; 4;\n\nEXEC sp_executesql @killCommand;\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EAs its name suggests, this script kills all connections to the database. This is necessary to ensure that we can rename the database without any active connections.\u003C/p\u003E\n\u003Ch2 id=\u0022final-directory-structure-and-dockerfile-changes\u0022\u003EFinal Directory Structure and Dockerfile Changes\u003C/h2\u003E\n\u003Cp\u003EIf everything went to plan, your directory should look like this:\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode\u003E.\n\u251C\u2500\u2500 Dockerfile\n\u251C\u2500\u2500 entrypoint.sh\n\u251C\u2500\u2500 initialize-database-and-jobs.sh\n\u251C\u2500\u2500 download-latest.sh\n\u251C\u2500\u2500 enable-authentication.sql\n\u251C\u2500\u2500 reimport-database.sh\n\u251C\u2500\u2500 kill-all-connections.sql\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003ENow before we can build the Docker container, we need to make a few final changes to our \u003Ccode\u003EDockerfile\u003C/code\u003E.\nWe need to copy all the files from our local machine to the container and set the correct permissions on the scripts.\nAdd the following lines to your \u003Ccode\u003EDockerfile\u003C/code\u003E directly after the \u003Ccode\u003EUSER root\u003C/code\u003E line:\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-Dockerfile\u0022\u003ERUN mkdir /home/mssql \u0026amp;\u0026amp; chown mssql /home/mssql \u0026amp;\u0026amp; \\\n    chmod \u002Bx /sql/initialize-database-and-jobs.sh \u0026amp;\u0026amp; \\\n    chmod \u002Bx /sql/entrypoint.sh \u0026amp;\u0026amp; \\\n    chmod \u002Bx /sql/download-latest.sh\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EThe \u003Ccode\u003Emkdir\u003C/code\u003E command creates a new directory in the container and the \u003Ccode\u003Echown\u003C/code\u003E command changes the owner of the directory to the \u003Ccode\u003Emssql\u003C/code\u003E user.\nWe then set the correct permissions on the scripts using the \u003Ccode\u003Echmod\u003C/code\u003E command. Save the \u003Ccode\u003EDockerfile\u003C/code\u003E and close it.\u003C/p\u003E\n\u003Ch1 id=\u0022building-the-docker-container\u0022\u003EBuilding the Docker Container\u003C/h1\u003E\n\u003Cp\u003ENow that we have all the necessary files, we can build the Docker container. Open a terminal and navigate to the directory where your \u003Ccode\u003EDockerfile\u003C/code\u003E is located.\nRun the following command to build the container:\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-bash\u0022\u003Edocker build -t azure-local-database-refresh .\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EThis command will build the container using the \u003Ccode\u003EDockerfile\u003C/code\u003E in the current directory and tag the container with the name \u003Ccode\u003Eazure-local-database-refresh\u003C/code\u003E.\nThe build process may take a few minutes to complete. Once it\u0027s done, you can run the container using the following command:\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-bash\u0022\u003Edocker run -e ACCOUNT_NAME=\u0026lt;storageAccountName\u0026gt; -e ACCOUNT_KEY=\u0026lt;storageAccountKey\u0026gt; -e CONTAINER_NAME=\u0026lt;containerName\u0026gt; -e DATABASE_NAME=MyDatabase -p 1433:1433 -it  azure-local-database-refresh\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EReplace \u003Ccode\u003E\u0026lt;storageAccountName\u0026gt;\u003C/code\u003E, \u003Ccode\u003E\u0026lt;storageAccountKey\u0026gt;\u003C/code\u003E, and \u003Ccode\u003E\u0026lt;containerName\u0026gt;\u003C/code\u003E with the appropriate values for your Azure Blob Storage account. The \u003Ccode\u003E-p\u003C/code\u003E flag maps port 1433 of the container to port 1433 of your local machine.\nThis allows you to connect to the SQL Server instance running in the container from your local machine. The \u003Ccode\u003E-it\u003C/code\u003E flag runs the container in interactive mode, which allows you to see the output of the container in your terminal.\u003C/p\u003E\n\u003Cp\u003EYou should see the output of the container in your terminal. If everything is working correctly, you should see messages indicating that the bacpac file has been downloaded and imported into the SQL Server instance. The import\nprocess can take a several minutes to complete, depending on the size of the database. Once the import process is complete, you should see a message indicating that the CRON job has been set up successfully.\u003C/p\u003E\n\u003Ch1 id=\u0022testing-the-container\u0022\u003ETesting the Container\u003C/h1\u003E\n\u003Cp\u003ENow that we have our container up and running, we can test it by connecting to the SQL Server instance and verifying that the database has been restored.\nOpen a new terminal and run the following command to connect to the SQL Server instance running in the container:\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-bash\u0022\u003Edocker exec -it \u0026lt;containerId\u0026gt; /opt/mssql-tools/bin/sqlcmd -S localhost -U sa -P yourStrong(!)Password\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EReplace \u003Ccode\u003E\u0026lt;containerId\u0026gt;\u003C/code\u003E with the ID of your container. This command will open an interactive SQL Server prompt. Run the following command to verify that the database has been restored:\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-sql\u0022\u003ESELECT name FROM sys.databases;\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EIf everything went to plan you should see the name of your database in the output. You can also run queries against the database to verify that the data has been restored correctly.\u003C/p\u003E\n\u003Ch1 id=\u0022conclusion\u0022\u003EConclusion\u003C/h1\u003E\n\u003Cp\u003EIn this article, we created a Docker container that automates the process of downloading the latest backup of a production database from Azure Blob Storage and restoring it to a local SQL Server instance.\nWe used a combination of bash scripts and Docker to create a portable and easy-to-use solution that can be run on any machine that has Docker installed. This solution can be used to provide teams with access\nto the latest production data for development, testing, reporting, data analysis, or other internal uses. The container can be run on a schedule using a CRON job to ensure that the data is always up-to-date. This solution can be\neasily extended to support other databases and cloud storage providers.\u003C/p\u003E\n\u003Cp\u003EThe code for this solution is available on \u003Ca href=\u0022https://github.com/JerrettDavis/az-bacpac-blob-mssql-importer\u0022\u003EGitHub\u003C/a\u003E.\nThis docker container is also available on \u003Ca href=\u0022https://hub.docker.com/r/jdhproductions/az-bacpac-blob-mssql-importer\u0022\u003EDocker Hub\u003C/a\u003E.\u003C/p\u003E\n",
    "stub": "Learn how to automate the retrieval and restoration of production database backups from Azure Blob Storage to a local SQL Server using Docker. This concise guide provides step-by-step instructions and scripts, enabling easy setup for development, testing, and analysis purposes.",
    "wordCount": 1703,
    "useToc": true,
    "source": "syndicated",
    "canonicalUrl": "https://jerrettdavis.com/blog/automating-daily-database-restores"
  },
  {
    "id": "365-days-of-code-week-1",
    "title": "365 Days of Code - Week 1",
    "date": "2022-01-02T00:00:00",
    "description": "A recap of the first week participating in the \u0022365 Days of Code\u0022 challenge, where the author commits and pushes code related to a personal project daily. The post discusses the motivation behind the challenge, the author\u0027s goals, and the results of the first week, including the migration of the personal website from WordPress to a Static Site built with Next.js and React, and the creation of a command line weather client.",
    "featured": "/images/posts/365-days-of-code-week-1/featured.png",
    "tags": [
      "coding-challenge",
      "coding",
      "programming"
    ],
    "categories": [
      "Programming",
      "Programming/Fun"
    ],
    "series": null,
    "seriesOrder": null,
    "content": "In many hobbyist communities, there\u0027s often a rally at the beginning of the year to participate in some sort of \u0060365 Days of X\u0060. For example, in photography communities they often advocate for photographers to participate in \u0060365 Photo Challenge\u0060, where participants are asked to take a unique, interesting photo once a day, every day, for an entire year. Now on its surface, this seems like a rather mundane task, but participants of these challenges will attest that they\u0027re anything but!\n\nNow extended code challenges aren\u0027t anything new. Every year, thousands of developers participate in [Hacktoberfest](https://hacktoberfest.digitalocean.com/) and [Advent of Code](https://adventofcode.com/), and Googling \u0022365 days of code\u0022 returns [result](https://medium.com/@ovidijusparsiunas/the-365-days-of-code-challenge-42a309917b2e) after [result](https://365daysofcode.com/). So suffice to say, my idea isn\u0027t original, nor is it intended to be.\n\n# Why?\n\nI\u0027ve participated in 365 challenges before, and aside from them being wonderful practice for honing a skill, they are also a relatively painless way to continually propel you forward. Since you are expected to put down new, interesting work every single day, you cannot afford to let analysis paralysis set in. If you get stuck, you *must* find a way to continue forward.\n\nLike many developers, I often lose interest in many projects during the planning phase. It can be absolutely hellish to approach a large project with little idea of how it\u0027s going to evolve over time. Rather than simply getting started, we get stuck in our heads considering all the \u0022what-ifs\u0022. This challenge is the ultimate trial-by-fire.\n\n# Goals\n\nUnlike many other 365 challenges, this one is not focusing on a specific project. Instead, I have only 1 rule: \u0022I must commit and push code related to a personal project once a day\u0022. Now to keep with the spirit of the challenge, I will do my absolute best to ensure each commit is substantial.\n\nNow while I don\u0027t have a specific project in mind, I do have a few general goals:\n\n- I would like to finally get my personal website up and running on something other than WordPress\n- I would like to become familiar with React and Redux\n- I would like to learn a functional language\n- Finally become consistent with my blogging by publishing a weekly retrospective each Sunday evening\n\n# This Week\u0027s Results\n\nSince this is the first week of the year, it\u0027s a *bit* short at a whopping 2 days long. Regardless, I\u0027ve managed to get off to a start by starting 2 new projects.\n\n## [Personal Website](https://github.com/JerrettDavis/personal-site)\n\nMy first project this week was a big one! I managed to get my personal website migrated off of a self-hosted WordPress instance and onto a Static Site. The new site is built using [Next.js](https://nextjs.org), and by extension, [React](https://reactjs.org). For the time being, it\u0027s being hosted on [Vercel](https://vercel.com).\n\nAs it is now, it\u0027s rather basic, but it does contain a blog, an about me section, links to my socials, and a dark theme toggle!\n\n## [weather-cli](https://github.com/JerrettDavis/weather-cli)\n\nI rarely write commandline applications, but I saw an idea for a fun one: A command line weather client. While there are obviously many out there, it serves as a great jumping off point for translating the application into other languages.\n\nCurrently, it simply uses [OpenWeatherMap](https://openweathermap.org) as the provider, but it\u0027s implemented in such a way that additional providers can be added. Implementing more providers and extended forecasts are my next steps for this application.\n\n# Conclusion\n\nWhile this week may have been short, it was still super fruitful! I can\u0027t wait to see what the rest of this year brings.",
    "contentHtml": "\u003Cp\u003EIn many hobbyist communities, there\u0027s often a rally at the beginning of the year to participate in some sort of \u003Ccode\u003E365 Days of X\u003C/code\u003E. For example, in photography communities they often advocate for photographers to participate in \u003Ccode\u003E365 Photo Challenge\u003C/code\u003E, where participants are asked to take a unique, interesting photo once a day, every day, for an entire year. Now on its surface, this seems like a rather mundane task, but participants of these challenges will attest that they\u0027re anything but!\u003C/p\u003E\n\u003Cp\u003ENow extended code challenges aren\u0027t anything new. Every year, thousands of developers participate in \u003Ca href=\u0022https://hacktoberfest.digitalocean.com/\u0022\u003EHacktoberfest\u003C/a\u003E and \u003Ca href=\u0022https://adventofcode.com/\u0022\u003EAdvent of Code\u003C/a\u003E, and Googling \u0026quot;365 days of code\u0026quot; returns \u003Ca href=\u0022https://medium.com/@ovidijusparsiunas/the-365-days-of-code-challenge-42a309917b2e\u0022\u003Eresult\u003C/a\u003E after \u003Ca href=\u0022https://365daysofcode.com/\u0022\u003Eresult\u003C/a\u003E. So suffice to say, my idea isn\u0027t original, nor is it intended to be.\u003C/p\u003E\n\u003Ch1 id=\u0022why\u0022\u003EWhy?\u003C/h1\u003E\n\u003Cp\u003EI\u0027ve participated in 365 challenges before, and aside from them being wonderful practice for honing a skill, they are also a relatively painless way to continually propel you forward. Since you are expected to put down new, interesting work every single day, you cannot afford to let analysis paralysis set in. If you get stuck, you \u003Cem\u003Emust\u003C/em\u003E find a way to continue forward.\u003C/p\u003E\n\u003Cp\u003ELike many developers, I often lose interest in many projects during the planning phase. It can be absolutely hellish to approach a large project with little idea of how it\u0027s going to evolve over time. Rather than simply getting started, we get stuck in our heads considering all the \u0026quot;what-ifs\u0026quot;. This challenge is the ultimate trial-by-fire.\u003C/p\u003E\n\u003Ch1 id=\u0022goals\u0022\u003EGoals\u003C/h1\u003E\n\u003Cp\u003EUnlike many other 365 challenges, this one is not focusing on a specific project. Instead, I have only 1 rule: \u0026quot;I must commit and push code related to a personal project once a day\u0026quot;. Now to keep with the spirit of the challenge, I will do my absolute best to ensure each commit is substantial.\u003C/p\u003E\n\u003Cp\u003ENow while I don\u0027t have a specific project in mind, I do have a few general goals:\u003C/p\u003E\n\u003Cul\u003E\n\u003Cli\u003EI would like to finally get my personal website up and running on something other than WordPress\u003C/li\u003E\n\u003Cli\u003EI would like to become familiar with React and Redux\u003C/li\u003E\n\u003Cli\u003EI would like to learn a functional language\u003C/li\u003E\n\u003Cli\u003EFinally become consistent with my blogging by publishing a weekly retrospective each Sunday evening\u003C/li\u003E\n\u003C/ul\u003E\n\u003Ch1 id=\u0022this-weeks-results\u0022\u003EThis Week\u0027s Results\u003C/h1\u003E\n\u003Cp\u003ESince this is the first week of the year, it\u0027s a \u003Cem\u003Ebit\u003C/em\u003E short at a whopping 2 days long. Regardless, I\u0027ve managed to get off to a start by starting 2 new projects.\u003C/p\u003E\n\u003Ch2 id=\u0022personal-website\u0022\u003E\u003Ca href=\u0022https://github.com/JerrettDavis/personal-site\u0022\u003EPersonal Website\u003C/a\u003E\u003C/h2\u003E\n\u003Cp\u003EMy first project this week was a big one! I managed to get my personal website migrated off of a self-hosted WordPress instance and onto a Static Site. The new site is built using \u003Ca href=\u0022https://nextjs.org\u0022\u003ENext.js\u003C/a\u003E, and by extension, \u003Ca href=\u0022https://reactjs.org\u0022\u003EReact\u003C/a\u003E. For the time being, it\u0027s being hosted on \u003Ca href=\u0022https://vercel.com\u0022\u003EVercel\u003C/a\u003E.\u003C/p\u003E\n\u003Cp\u003EAs it is now, it\u0027s rather basic, but it does contain a blog, an about me section, links to my socials, and a dark theme toggle!\u003C/p\u003E\n\u003Ch2 id=\u0022weather-cli\u0022\u003E\u003Ca href=\u0022https://github.com/JerrettDavis/weather-cli\u0022\u003Eweather-cli\u003C/a\u003E\u003C/h2\u003E\n\u003Cp\u003EI rarely write commandline applications, but I saw an idea for a fun one: A command line weather client. While there are obviously many out there, it serves as a great jumping off point for translating the application into other languages.\u003C/p\u003E\n\u003Cp\u003ECurrently, it simply uses \u003Ca href=\u0022https://openweathermap.org\u0022\u003EOpenWeatherMap\u003C/a\u003E as the provider, but it\u0027s implemented in such a way that additional providers can be added. Implementing more providers and extended forecasts are my next steps for this application.\u003C/p\u003E\n\u003Ch1 id=\u0022conclusion\u0022\u003EConclusion\u003C/h1\u003E\n\u003Cp\u003EWhile this week may have been short, it was still super fruitful! I can\u0027t wait to see what the rest of this year brings.\u003C/p\u003E\n",
    "stub": "A recap of the first week participating in the \u0022365 Days of Code\u0022 challenge, where the author commits and pushes code related to a personal project daily. The post discusses the motivation behind the challenge, the author\u0027s goals, and the results of the first week, including the migration of the personal website from WordPress to a Static Site built with Next.js and React, and the creation of a command line weather client.",
    "wordCount": 578,
    "useToc": false,
    "source": "syndicated",
    "canonicalUrl": "https://jerrettdavis.com/blog/365-days-of-code-week-1"
  },
  {
    "id": "ef-core-public-id",
    "title": "Simple Public ID Generation in EF Core",
    "date": "2021-12-27T00:00:00",
    "description": "A guide detailing the implementation of a simple public ID generation system in Entity Framework Core (EF Core). The post discusses the background motivation, problems encountered, and the implementation steps, including hiding primary keys, generating public identifiers, and simplifying operations on public entities.",
    "featured": null,
    "tags": [
      "csharp",
      "dotnet",
      "ef-core",
      "entity-framework",
      "entity-framework-core",
      "programming",
      "coding"
    ],
    "categories": [
      "Programming",
      "Programming/Architecture"
    ],
    "series": null,
    "seriesOrder": null,
    "content": "# Background\n\nI\u0027ve had the fortune of working in many codebases leveraging all sorts of different technologies. However, regardless of the stack, one of the many issues I\u0027ve found shared among them is the exposure of sensitive internal implementation details. More specifically, one of the most common culprits has been the pervasiveness of database keys finding their way in front of users. Whether that\u0027s through URLs, JSON payloads, or websockets makes no difference; the presence of implementation details in userland is a problem.\n\nIn lieu of turning this into a discussion about the issues surrounding database keys being leaked, below are some good articles about the root of the problem:\n\n- [Tom Harrison - UUID or GUID as Primary Keys? Be Careful!](https://tomharrisonjr.com/uuid-or-guid-as-primary-keys-be-careful-7b2aa3dcb439)\n- [Petre Popescu - Exposing sequential IDs is bad! Here is how to avoid it.](https://petrepopescu.tech/2021/01/exposing-sequential-ids-is-bad-here-is-how-to-avoid-it/)\n\n# Problems\n\nLike most developers, I hate writing more code than necessary. While investigating rewriting our system to hide internal IDs, it quickly became apparent I\u0027d need something as unobtrusive as possible. My goal was to have a near turn-key solution that required next-to-zero developer intervention. With that in mind, I came up with some core problems to address:\n\n- We can\u0027t expose database keys\n- We need to generate an immutable public identifier for all sensitive public entities\n- Public identifiers must be customizable for different situations\n- We need to avoid code repetition\n- We need to supplement existing EF Linq extensions to ease fetching public entities\n\n# Implementation\n\n## Hiding Primary Keys\n\nOur first step was to completely remove publicly accessible primary keys from our models directly. While this may not be a good fit for every project (or even every entity within a project), it made sense in the vast majority of ours. To accomplish this, we converted each previously public \u0060Id\u0060 to a private, readonly \u0060_id\u0060. Like so:\n\n\u0060\u0060\u0060csharp\n// old\npublic int Id { get; set; }\n\n// new\nprivate readonly int _id;\n\u0060\u0060\u0060\n\nThe above change also required a slight update to our entity configuration code:\n\n\u0060\u0060\u0060csharp\npublic class MyEntityConfiguration : IEntityTypeConfiguration\u003CMyEntity\u003E\n{\n    public void Configure(EntityTypeBuilder\u003CMyEntity\u003E builder)\n    {\n        // We now have to explicitly tell EF to map this private property\n        // This alone will create migrations with an identity field named \u0027_id\u0027\n        builder.HasKey(\u0022_id\u0022);\n\n        // The following is optional, but it helps add a bit of explicitness\n        // And it renames the property. Some may actually like the property being \u0027_id\u0027\n        builder.Property(\u0022_id\u0022)\n            .HasColumnName(\u0022Id\u0022)\n            .ValueGeneratedOnAdd();\n    }\n}\n\u0060\u0060\u0060\nIt is worth noting that while we did not elect to do so, you could define an empty interface that you place on all classes that hide their primary key. This could enable you to apply the property configurations en masse. A similar approach will be demonstrated later for configuring public entities.\n\n## Generating a Public Identifier for all Public Entities\n\nAfter we removed the publicly accessible primary keys from our models, we still needed  a way of accessing those entities. To accomplish this, we defined a new interface: \u0060IPublicEntity\u0060. This simple interface, while only defining a single property, underpins the entirety of the automated ID generation logic.\n\n\u0060\u0060\u0060csharp\npublic interface IPublicEntity\n{\n    string PublicId { get; }\n}\n\u0060\u0060\u0060\n\nOnce we have the \u0060PublicId\u0060 on our entities, we have to tell EF to generate that value for us. We can do that with the help of a [\u0060ValueGenerator\u0060](https://docs.microsoft.com/en-us/dotnet/api/microsoft.entityframeworkcore.valuegeneration.valuegenerator?view=efcore-5.0) and the \u0060PropertyBuilder\u0060 extension [\u0060HasValueGenerator\u0060](https://docs.microsoft.com/en-us/dotnet/api/microsoft.entityframeworkcore.metadata.builders.propertybuilder.hasvaluegenerator?view=efcore-5.0#Microsoft_EntityFrameworkCore_Metadata_Builders_PropertyBuilder_HasValueGenerator_System_Type_). The \u0060ValueGenerator\u0060 abstract class, as shown implemented below, consists of a method to generate values (\u0060Next\u0060) and a property indicating if the values generated should be replaced by the database (\u0060GeneratesTemporaryValues\u0060).\n\n\u0060\u0060\u0060csharp\npublic class PublicIdValueGenerator : ValueGenerator\u003Cstring\u003E\n{\n    public override string Next(EntityEntry entry)\n    {\n        if (entry == null)\n            throw new ArgumentNullException(nameof(entry));\n            \n        return Guid.NewGuid().ToString();\n    }\n\n    public override bool GeneratesTemporaryValues =\u003E false;\n}\n\u0060\u0060\u0060\nThe \u0060PublicIdValueGenerator\u0060 also has access to the underlying IoC container, so if you wanted to fetch a service to generate the IDs, you could do that like so:\n\n\u0060\u0060\u0060csharp\n// old \nreturn Guid.NewGuid().ToString();\n\n// new\nvar gen = entry.Context.GetService\u003CIUniqueIdGenerator\u003E();\nreturn gen.CreateId();\n\u0060\u0060\u0060\nOnce we have our generator implemented, we need a way of telling our entities to use it. One approach is to define a class implementing \u0060IEntityTypeConfiguration\u003CTEntity\u003E\u0060 like below.\n\n\u0060\u0060\u0060csharp\npublic class MyEntityConfiguration : IEntityTypeConfiguration\u003CMyEntity\u003E\n{\n    public void Configure(EntityTypeBuilder\u003CMyEntity\u003E builder)\n    {\n        builder.Property(e =\u003E e.PublicId)\n            .HasValueGenerator\u003CPublicIdValueGenerator\u003E();\n    }\n}\n\u0060\u0060\u0060\n\nAlternatively, as a less tedious approach, you can configure all the classes implementing \u0060IPublicEntity\u0060 en masse from the EF database context.\n\n\u0060\u0060\u0060csharp\nprotected override void OnModelCreating(ModelBuilder modelBuilder)\n{\n    // Finds all the classes implementing IEntityTypeConfiguration in the assembly\n    // and applies them to the context.\n    modelBuilder.ApplyConfigurationsFromAssembly(GetType().Assembly);\n    \n    // Get a list of all entities that implement IPublicEntity\n    var publicEntities = modelBuilder.Model.GetEntityTypes()\n        .Where(i =\u003E i.ClrType.IsAssignableTo(typeof(IPublicEntity)));\n    foreach (var item in publicEntities)\n    {\n        // Add value generator to each entity\n        modelBuilder.Entity(item.ClrType)\n            .Property(nameof(IPublicEntity.PublicId)) // Will be \u0027PublicId\u0027\n            .HasValueGenerator\u003CPublicIdValueGenerator\u003E();\n    }\n    \n    base.OnModelCreating(modelBuilder);\n}\n\u0060\u0060\u0060\nUsing the latter approach, creating new public entities becomes as simple as implementing the \u0060IPublicEntity\u0060 interface on any class you wish to make public.\n\n## Simplify Operations on Public Entities\n\nSince we\u0027re moving away from passing around our public IDs, no longer having access to Linq\u0027s \u0060Find\u0060 and \u0060FindAsync\u0060 methods can prove a bit irksome. To help ease matters a bit, we can write an extension method to mimic \u0060Find\u0060\u0027s functionality, but tailored for \u0060IPublicEntities\u0060 instead. To do this, we need to create an extension for the underlying \u0060IQueryable\u0060s that underpin much of EF Core\u0027s functionality, and maybe a couple more for good measure.\n\n\u0060\u0060\u0060csharp\npublic static class QueryableExtensions\n{\n    public static Task\u003Cbool\u003E PublicEntityExistsAsync\u003CTEntity\u003E(\n        this IQueryable\u003CTEntity\u003E queryable,\n        string publicId, \n        CancellationToken cancellationToken = default) \n        where TEntity : class, IPublicEntity =\u003E\n        queryable.AnyAsync(e =\u003E e.PublicId == publicId, \n            cancellationToken);\n    \n    public static Task\u003CTEntity?\u003E FindPublicEntityAsync\u003CTEntity\u003E(\n        this IQueryable\u003CTEntity\u003E queryable,\n        string publicId,\n        CancellationToken cancellationToken = default)\n        where TEntity : class, IPublicEntity =\u003E\n        queryable.FirstOrDefaultAsync(e =\u003E e.PublicId == publicId,\n            cancellationToken);\n\n    public static Task\u003CTEntity\u003E SinglePublicEntityAsync\u003CTEntity\u003E(\n        this IQueryable\u003CTEntity\u003E queryable,\n        string publicId,\n        CancellationToken cancellationToken = default)\n        where TEntity : class, IPublicEntity =\u003E\n        queryable.SingleAsync(e =\u003E e.PublicId == publicId,\n            cancellationToken);\n}\n\u0060\u0060\u0060\nFor most intents and purposes, the \u0060SinglePublicEntityAsync\u0060 and \u0060FindPublicEntityAsync\u0060 methods above mimic the Linq methods they\u0027re named after. The \u0060FindPublicEntityAsync\u0060 method is actually backed by \u0060FirstOrDefaultAsync\u0060 and as such, it lacks some of \u0060FindAsync\u0060\u0027s caching abilities, but it *does* have \u0060FirstOrDefaultAsync\u0060\u0027s ability to easily load related data!\n\n# Conclusion\n\nImplementing universal public IDs in a scalable and reusable way was *far* easier than I initially anticipated when I began researching the matter. Thankfully, this allows us to implement a rather vital feature with minimal effort on the developer\u0027s behalf. You can find a runnable example of the code on my [GitHub Page](https://github.com/JerrettDavis/EfCorePublicIdDemo).\n\nIf you have any questions, feel free to reach out!",
    "contentHtml": "\u003Ch1 id=\u0022background\u0022\u003EBackground\u003C/h1\u003E\n\u003Cp\u003EI\u0027ve had the fortune of working in many codebases leveraging all sorts of different technologies. However, regardless of the stack, one of the many issues I\u0027ve found shared among them is the exposure of sensitive internal implementation details. More specifically, one of the most common culprits has been the pervasiveness of database keys finding their way in front of users. Whether that\u0027s through URLs, JSON payloads, or websockets makes no difference; the presence of implementation details in userland is a problem.\u003C/p\u003E\n\u003Cp\u003EIn lieu of turning this into a discussion about the issues surrounding database keys being leaked, below are some good articles about the root of the problem:\u003C/p\u003E\n\u003Cul\u003E\n\u003Cli\u003E\u003Ca href=\u0022https://tomharrisonjr.com/uuid-or-guid-as-primary-keys-be-careful-7b2aa3dcb439\u0022\u003ETom Harrison - UUID or GUID as Primary Keys? Be Careful!\u003C/a\u003E\u003C/li\u003E\n\u003Cli\u003E\u003Ca href=\u0022https://petrepopescu.tech/2021/01/exposing-sequential-ids-is-bad-here-is-how-to-avoid-it/\u0022\u003EPetre Popescu - Exposing sequential IDs is bad! Here is how to avoid it.\u003C/a\u003E\u003C/li\u003E\n\u003C/ul\u003E\n\u003Ch1 id=\u0022problems\u0022\u003EProblems\u003C/h1\u003E\n\u003Cp\u003ELike most developers, I hate writing more code than necessary. While investigating rewriting our system to hide internal IDs, it quickly became apparent I\u0027d need something as unobtrusive as possible. My goal was to have a near turn-key solution that required next-to-zero developer intervention. With that in mind, I came up with some core problems to address:\u003C/p\u003E\n\u003Cul\u003E\n\u003Cli\u003EWe can\u0027t expose database keys\u003C/li\u003E\n\u003Cli\u003EWe need to generate an immutable public identifier for all sensitive public entities\u003C/li\u003E\n\u003Cli\u003EPublic identifiers must be customizable for different situations\u003C/li\u003E\n\u003Cli\u003EWe need to avoid code repetition\u003C/li\u003E\n\u003Cli\u003EWe need to supplement existing EF Linq extensions to ease fetching public entities\u003C/li\u003E\n\u003C/ul\u003E\n\u003Ch1 id=\u0022implementation\u0022\u003EImplementation\u003C/h1\u003E\n\u003Ch2 id=\u0022hiding-primary-keys\u0022\u003EHiding Primary Keys\u003C/h2\u003E\n\u003Cp\u003EOur first step was to completely remove publicly accessible primary keys from our models directly. While this may not be a good fit for every project (or even every entity within a project), it made sense in the vast majority of ours. To accomplish this, we converted each previously public \u003Ccode\u003EId\u003C/code\u003E to a private, readonly \u003Ccode\u003E_id\u003C/code\u003E. Like so:\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-csharp\u0022\u003E// old\npublic int Id { get; set; }\n\n// new\nprivate readonly int _id;\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EThe above change also required a slight update to our entity configuration code:\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-csharp\u0022\u003Epublic class MyEntityConfiguration : IEntityTypeConfiguration\n{\n    public void Configure(EntityTypeBuilder builder)\n    {\n        // We now have to explicitly tell EF to map this private property\n        // This alone will create migrations with an identity field named \u0027_id\u0027\n        builder.HasKey(\u0026quot;_id\u0026quot;);\n\n        // The following is optional, but it helps add a bit of explicitness\n        // And it renames the property. Some may actually like the property being \u0027_id\u0027\n        builder.Property(\u0026quot;_id\u0026quot;)\n            .HasColumnName(\u0026quot;Id\u0026quot;)\n            .ValueGeneratedOnAdd();\n    }\n}\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EIt is worth noting that while we did not elect to do so, you could define an empty interface that you place on all classes that hide their primary key. This could enable you to apply the property configurations en masse. A similar approach will be demonstrated later for configuring public entities.\u003C/p\u003E\n\u003Ch2 id=\u0022generating-a-public-identifier-for-all-public-entities\u0022\u003EGenerating a Public Identifier for all Public Entities\u003C/h2\u003E\n\u003Cp\u003EAfter we removed the publicly accessible primary keys from our models, we still needed  a way of accessing those entities. To accomplish this, we defined a new interface: \u003Ccode\u003EIPublicEntity\u003C/code\u003E. This simple interface, while only defining a single property, underpins the entirety of the automated ID generation logic.\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-csharp\u0022\u003Epublic interface IPublicEntity\n{\n    string PublicId { get; }\n}\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EOnce we have the \u003Ccode\u003EPublicId\u003C/code\u003E on our entities, we have to tell EF to generate that value for us. We can do that with the help of a \u003Ca href=\u0022https://docs.microsoft.com/en-us/dotnet/api/microsoft.entityframeworkcore.valuegeneration.valuegenerator?view=efcore-5.0\u0022\u003E\u003Ccode\u003EValueGenerator\u003C/code\u003E\u003C/a\u003E and the \u003Ccode\u003EPropertyBuilder\u003C/code\u003E extension \u003Ca href=\u0022https://docs.microsoft.com/en-us/dotnet/api/microsoft.entityframeworkcore.metadata.builders.propertybuilder.hasvaluegenerator?view=efcore-5.0#Microsoft_EntityFrameworkCore_Metadata_Builders_PropertyBuilder_HasValueGenerator_System_Type_\u0022\u003E\u003Ccode\u003EHasValueGenerator\u003C/code\u003E\u003C/a\u003E. The \u003Ccode\u003EValueGenerator\u003C/code\u003E abstract class, as shown implemented below, consists of a method to generate values (\u003Ccode\u003ENext\u003C/code\u003E) and a property indicating if the values generated should be replaced by the database (\u003Ccode\u003EGeneratesTemporaryValues\u003C/code\u003E).\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-csharp\u0022\u003Epublic class PublicIdValueGenerator : ValueGenerator\u0026lt;string\u0026gt;\n{\n    public override string Next(EntityEntry entry)\n    {\n        if (entry == null)\n            throw new ArgumentNullException(nameof(entry));\n            \n        return Guid.NewGuid().ToString();\n    }\n\n    public override bool GeneratesTemporaryValues =\u0026gt; false;\n}\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EThe \u003Ccode\u003EPublicIdValueGenerator\u003C/code\u003E also has access to the underlying IoC container, so if you wanted to fetch a service to generate the IDs, you could do that like so:\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-csharp\u0022\u003E// old \nreturn Guid.NewGuid().ToString();\n\n// new\nvar gen = entry.Context.GetService();\nreturn gen.CreateId();\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EOnce we have our generator implemented, we need a way of telling our entities to use it. One approach is to define a class implementing \u003Ccode\u003EIEntityTypeConfiguration\u003C/code\u003E like below.\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-csharp\u0022\u003Epublic class MyEntityConfiguration : IEntityTypeConfiguration\n{\n    public void Configure(EntityTypeBuilder builder)\n    {\n        builder.Property(e =\u0026gt; e.PublicId)\n            .HasValueGenerator();\n    }\n}\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EAlternatively, as a less tedious approach, you can configure all the classes implementing \u003Ccode\u003EIPublicEntity\u003C/code\u003E en masse from the EF database context.\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-csharp\u0022\u003Eprotected override void OnModelCreating(ModelBuilder modelBuilder)\n{\n    // Finds all the classes implementing IEntityTypeConfiguration in the assembly\n    // and applies them to the context.\n    modelBuilder.ApplyConfigurationsFromAssembly(GetType().Assembly);\n    \n    // Get a list of all entities that implement IPublicEntity\n    var publicEntities = modelBuilder.Model.GetEntityTypes()\n        .Where(i =\u0026gt; i.ClrType.IsAssignableTo(typeof(IPublicEntity)));\n    foreach (var item in publicEntities)\n    {\n        // Add value generator to each entity\n        modelBuilder.Entity(item.ClrType)\n            .Property(nameof(IPublicEntity.PublicId)) // Will be \u0027PublicId\u0027\n            .HasValueGenerator();\n    }\n    \n    base.OnModelCreating(modelBuilder);\n}\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EUsing the latter approach, creating new public entities becomes as simple as implementing the \u003Ccode\u003EIPublicEntity\u003C/code\u003E interface on any class you wish to make public.\u003C/p\u003E\n\u003Ch2 id=\u0022simplify-operations-on-public-entities\u0022\u003ESimplify Operations on Public Entities\u003C/h2\u003E\n\u003Cp\u003ESince we\u0027re moving away from passing around our public IDs, no longer having access to Linq\u0027s \u003Ccode\u003EFind\u003C/code\u003E and \u003Ccode\u003EFindAsync\u003C/code\u003E methods can prove a bit irksome. To help ease matters a bit, we can write an extension method to mimic \u003Ccode\u003EFind\u003C/code\u003E\u0027s functionality, but tailored for \u003Ccode\u003EIPublicEntities\u003C/code\u003E instead. To do this, we need to create an extension for the underlying \u003Ccode\u003EIQueryable\u003C/code\u003Es that underpin much of EF Core\u0027s functionality, and maybe a couple more for good measure.\u003C/p\u003E\n\u003Cpre\u003E\u003Ccode class=\u0022language-csharp\u0022\u003Epublic static class QueryableExtensions\n{\n    public static Task\u0026lt;bool\u0026gt; PublicEntityExistsAsync(\n        this IQueryable queryable,\n        string publicId, \n        CancellationToken cancellationToken = default) \n        where TEntity : class, IPublicEntity =\u0026gt;\n        queryable.AnyAsync(e =\u0026gt; e.PublicId == publicId, \n            cancellationToken);\n    \n    public static Task\u0026lt;TEntity?\u0026gt; FindPublicEntityAsync(\n        this IQueryable queryable,\n        string publicId,\n        CancellationToken cancellationToken = default)\n        where TEntity : class, IPublicEntity =\u0026gt;\n        queryable.FirstOrDefaultAsync(e =\u0026gt; e.PublicId == publicId,\n            cancellationToken);\n\n    public static Task SinglePublicEntityAsync(\n        this IQueryable queryable,\n        string publicId,\n        CancellationToken cancellationToken = default)\n        where TEntity : class, IPublicEntity =\u0026gt;\n        queryable.SingleAsync(e =\u0026gt; e.PublicId == publicId,\n            cancellationToken);\n}\n\u003C/code\u003E\u003C/pre\u003E\n\u003Cp\u003EFor most intents and purposes, the \u003Ccode\u003ESinglePublicEntityAsync\u003C/code\u003E and \u003Ccode\u003EFindPublicEntityAsync\u003C/code\u003E methods above mimic the Linq methods they\u0027re named after. The \u003Ccode\u003EFindPublicEntityAsync\u003C/code\u003E method is actually backed by \u003Ccode\u003EFirstOrDefaultAsync\u003C/code\u003E and as such, it lacks some of \u003Ccode\u003EFindAsync\u003C/code\u003E\u0027s caching abilities, but it \u003Cem\u003Edoes\u003C/em\u003E have \u003Ccode\u003EFirstOrDefaultAsync\u003C/code\u003E\u0027s ability to easily load related data!\u003C/p\u003E\n\u003Ch1 id=\u0022conclusion\u0022\u003EConclusion\u003C/h1\u003E\n\u003Cp\u003EImplementing universal public IDs in a scalable and reusable way was \u003Cem\u003Efar\u003C/em\u003E easier than I initially anticipated when I began researching the matter. Thankfully, this allows us to implement a rather vital feature with minimal effort on the developer\u0027s behalf. You can find a runnable example of the code on my \u003Ca href=\u0022https://github.com/JerrettDavis/EfCorePublicIdDemo\u0022\u003EGitHub Page\u003C/a\u003E.\u003C/p\u003E\n\u003Cp\u003EIf you have any questions, feel free to reach out!\u003C/p\u003E\n",
    "stub": "A guide detailing the implementation of a simple public ID generation system in Entity Framework Core (EF Core). The post discusses the background motivation, problems encountered, and the implementation steps, including hiding primary keys, generating public identifiers, and simplifying operations on public entities.",
    "wordCount": 767,
    "useToc": true,
    "source": "syndicated",
    "canonicalUrl": "https://jerrettdavis.com/blog/ef-core-public-id"
  }
]